\documentclass[a4paper]{article}
\input{../../../preamble.tex}

\title{Principles of Statistics \\ \large{As lectured (roughly) by Richard Nickl (Michaelmas 2019)}}
\date{}
\author{Typeset by Oliver Shenton}

\begin{document}
	
\maketitle

\section{Introduction}

Consider a random variable (r.v.) $X$ defined on some probability space: $ X: (\Omega, A,\P) \to \R$. ($\Omega$ is the set of outcomes, $A$ is the measurable events in $\Omega$, $\P$ is the probability measure on $A $), with distribution function $F\left( t \right) = \P\left( \omega \in  \Omega : X\left( \omega \right) \le  t\right), t \in \R$.

If $X$ is a discrete r.v. Then
\[
	F(t) = \sum_{x\le t} f(x) \textrm{, where f is the probability mass function (pmf)}
.\] 
and, if $X$ is a continuous r.v., then
\[
	F(t) = \int_{- \infty}^{t} f(x) dx \textrm{, where f is the probability density function (pdf)}
.\] 

We typically only write $F(t) = P\left( X \le  t\right)$, where $P$ is the \textit{law} of  $X$ (i.e. the image measure  $P = \P \cdot X^{-1} $)

\begin{defn}
	A statistical model for the law $P$ of $X$ is any collection $\{ f\left(\cdot, \theta \right): \theta \in \Theta \} $, or $\{ P_{\theta} : \theta \in \Theta\} $ of pdf/pmf's or probability distributions. The index set $\Theta$ is the parameter space.
\end{defn}

\begin{eg}
	\begin{enumerate}
		\item $N\left( \theta, 1 \right), \theta \in \Theta = \R $, or $\Theta = \left[ -1, 1 \right] $

		\item $N\left( \mu, \sigma ^2 \right), \left( \mu, \sigma ^2 \right) = \theta \in  \Theta = \R \times \left( 0, \infty \right)  $
	
		\item $Exp\left( \theta \right) , \theta \in \Theta = \left( 0, \infty \right)  $
	\end{enumerate}
\end{eg}

\begin{defn}
	A statistical model $\{ P_{\theta} : \theta \in \Theta \} $ is correctly specified (for the law $P$ of $X$) if $\exists \theta \in \Theta$ s.t. $P_{\theta} = P$. We often write $\theta_{0}$ for this specific 'true' value of $\theta$. We say that observations $X_{1}, \ldots, X_{n} \stackrel{iid}{\sim} $ arise from the model $\{ P_{0} : \theta \in \Theta\} $ in this case. We refer to n as the sample size.
\end{defn}

The tasks of statistical inference comprise at least:

1) Estimation: Construct an estimator $\hat{\theta}_{n} = \hat{\theta}\left( X_{1}, \ldots, X_{n} \right) \in  \Theta $ that is close with high probability to $\theta$ when $X_{1}, \ldots, X_{n} \stackrel{iid}{\sim} P_{\theta}, \forall \theta \in  \Theta$.

2) Hypothesis Testing: For $H_0 : \theta = \theta_{0} $ vs $H_1 : \theta \neq \theta_{0} $, we want a test (indicator) function $\psi_{n} = \psi \left( X_{1}, \ldots, X_{n} \right) $ s.t $\psi _{n} = 0$ with high probability when $H_{0}$ is true, and $\psi_{n} = 1$ otherwise.

3) Confidence Regions (Inference): Find regions (intervals) $C_{n} = C \left( X_{1}, \ldots, X_{n} \right) \subseteq   \Theta $ of confidence in that $P_{\theta} \left( \theta \in  C_{n} \right) \ge 1 - \alpha, \forall \theta \in  \Theta$. This quantifies the uncertainty in the inference on $\Theta$ by the size/diameter of $C_{n}$. Here, $0 < \alpha < 1$ is a pre-described significance level.

\subsection{Likelihood Principle}

\begin{eg}
        Consider a sample $X_{1}, \ldots, X_{n} \sim ^{iid} Poi\left( \theta \right) $ with (unknown) $\theta > 0$. If the actual observed values are  $X_{1} = x_{1}, \ldots, X_{n} = x_{n}$, then the probability of this particular occurrence of $x_{1}, \ldots, x_{n}$ as a function of $\theta$ is 
\begin{align*}
	f \left( x_{1}, \ldots, x_{n}, \theta \right) &= P_{\theta} \left( X_{1}=x_{1}, \ldots, X_{n} = x_{n} \right) \\
						    &= \prod_{i=1}^{n} P_{\theta} \left( X_{i} = x_{i} \right) \\
						    &= \prod_{i=1}^{n} e^{-\theta} \frac{\theta^{x_{i}}}{x_{i}!} \\
						    &= L_{n}\left( \theta \right), \textrm{ a random function of } \theta 
\end{align*} 
\end{eg}

\begin{idea}
	(C.F. Gauss, R. Fisher): 
	Maximise $L_{n}\left( \theta \right)$ over $\Theta$, and for continuous variables, replace pmf's by pdf's.
\end{idea}

In the example, we can equivalently maximise 
\[
	l_{n}\left( \theta \right) = \log\left( L_{n}\left( \theta \right)  \right) = -n\theta + \log\theta \sum_{i=1}^{n} X_{i}  - \sum_{i=1}^{n} \log\left( X_{i}! \right)
.\]
over $(0, \infty)$. Then  $l_{n}'\left( \theta \right) = -n + \frac{1}{\theta}\sum_{i=1}^{n} X_{i}$, so at maximum $\hat{\theta}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$

Also, $l_{n}''\left( \theta \right) = - \frac{1}{\theta ^2} \sum_{i=1}^{n} X_{i} < 0$ if not all $X_{i} = 0$ (in which case $\hat{\theta} = 0 = \frac{1}{n} \sum_{i=1}^{n} X_{i}$).

\newpage

\begin{defn}
Given a statistical model $\{f\left( \cdot , \theta \right): \theta \in \Theta \} $ of pdf/pmf's for the law $P$ of $X$, and given 'numerical' observations  $\left( x_{i}: i=1,\ldots,n \right) $ arising as iid copies $X_{i}\stackrel{iid}{\sim}P$, the \textit{likelihood function of the model} is defined as
	\begin{align*}
		L_{n} : \Theta \to \R \text{, } L_{n}\left( \theta \right) = \prod_{i=1}^{n} f\left( x_{i}, \theta \right)   
	.\end{align*}

Moreover, the \textit{log-likelihood} is
\begin{align*}
	l_{n}: \Theta \to \R \cup \{-\infty\} \text{, } l_{n}\left( \theta \right) = \sum_{i=1}^{n}\log f\left( x_{i}, \theta \right)
,\end{align*}
and the normalised log-likelihood is
\begin{align*}
	\overline{l}_{n}\left( \theta \right) = \frac{1}{n}\sum_{i=1}^{n}\log f\left( x_{i}, \theta \right) 
.\end{align*}
\end{defn}
We regard these functions as ('random' via the $X_{i}'s$) maps of $\theta$.

\begin{defn}
	A \textit{maximum likelihood estimator} (MLE) is any $\hat{\theta} = \hat{\theta}_{MLE} \left( X_{1}, \ldots, X_{n} \right) \in \Theta$ s.t. \[
		L_{n}( \hat{\theta}) = \max_{\theta \in \Theta} L_{n}\left( \theta \right) 
	.\] 
\end{defn}

\begin{eg}
	For $Poi(\theta), \theta \ge 0$, we have seen $\hat{\theta}_{MLE} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$
\end{eg}

\begin{eg}
	$N\left( \mu, \sigma^2 \right) $, where $\theta = \left( \mu, \sigma^2 \right) \in \Theta = \R \times (0, \infty)$, one can show that the MLE
	\begin{align*}
		\hat{\theta}_{MLE} &= \begin{pmatrix} \hat{\mu}_{MLE} \\ \hat{\sigma}^2\end{pmatrix} \\
		&= \begin{pmatrix} \overline{X}_{n} \\ \frac{1}{n} \sum_{i=1}^{n}\left( X_{i} - \overline{X}_{n} \right)  \end{pmatrix} 
	,\end{align*}
	is obtained from solving  $\nabla \ln(\hat{\theta}_{MLE}) = 0$
\end{eg}

\begin{remark}
	Calculation of marginal MLEs that optimise only one variable is not sufficient. Typically the MLE for $\theta \in \Theta \subseteq \R^{p}$ is found by solving the \textit{score equations}
	\[
		S_{n}( \hat{\theta}) = 0, \text{ where } S_{n} : \Theta \to  \R^{p} \text{ is the score function } S_{n} (\theta) = \nabla \ln(\theta)
	.\]
	Here, we use the implicit notation $S_{n}(\hat{\theta}) = \nabla \ln(\theta) |_{\theta = \hat{\theta}}$
\end{remark}

\begin{remark}
	The likelihood principle 'works' as soon as a joint pdf/pmf (family $\{f\left( \cdot , \theta \right) : \theta \in  \Theta\} $) of $X_{1}, \ldots, X_{n}$ can be specified, and does note rely on the iid assumption. For instance in the normal linear model, $N\left( \chi \beta, \sigma^2 I \right) $, where $\chi$ is an $n\times p$ matrix, $(\beta, \sigma^2) = \theta \in \R^{p} \times (0, \infty)$, the MLE coincides with the LS-estimator (not iid, but independent).
\end{remark}

\section{Information Geometry}

For a r.v. $X$ of law/distribution $P_{\theta}$ on $\chi \subseteq \R^{d}$, and let $g: \chi \to  \R$ be given. We will write
\begin{align*}
	\E_{\theta} g(X) &= \E_{P_{\theta}} g(X) \\
	&= \int_{\chi} g(x) dP_{\theta}(x)
,\end{align*}
which in the continuous case equals $\int_{\chi} g(x) f(x, \theta) dx$, and in the discrete case is $\sum_{x \in \chi}g(x) f(x, \theta)$.

\begin{observation}[1]
	Consider a model $\{f(\cdot , \theta) : \theta \in \Theta\}$ for $X$ of law $P$ on $\chi$, and assume $\E_{P} |\log f(x, \theta)| < \infty$. Then $\overline{l}_{n}(\theta) = \frac{1}{n}\sum_{i=1}^{n}\log f(X_{i}, \theta)$ is a sample approximation of
	\begin{align*}
		l(\theta) = \E_{P} \log f(X, \theta), \theta \in \Theta
	.\end{align*}
	If the model is correctly specified with any true value $\theta_{0}$ s.t. $P=P_{\theta_{0}}$, then we can rewrite
	\begin{align*}
		l(\theta) &= \E_{P_{\theta_{0}}} \log f(X, \theta) \\
		&= \int_{\chi} \left(\log f(x, \theta) \right) f(x, \theta_{0}) dx
	.\end{align*}

	Next, we write 
	\begin{align*}
		l(\theta) - l(\theta_{0}) &= \E_{\theta_{0}}\left[ \log \frac{f(X, \theta)}{f(X, \theta_{0})} \right] \text{, which by Jensen's inequality applied to log} \\
		&\le \log \E_{\theta_{0}}\left[ \frac{f(X, \theta)}{f(X, \theta_{0})} \right] \\
		&= \log \int_{\chi} \frac{f(x, \theta)}{f(X, \theta_{0})} f(X, \theta_{0}) dx = 0,\text{ } \forall \theta \in  \Theta
	.\end{align*}

	Thus $l(\theta) \le l(\theta_{0}) \text{ }\forall \theta \in \Theta$, and approximately maximising $l(\theta)$ appears sensible.

	Note next that by the strict version of Jensen's inequality,  $l(\theta) = l(\theta_{0})$ can only occur when $\frac{f(X, \theta)}{f(X, \theta_{0})} = \text{const (in $X$) }$, which since $\int_{\chi} f(x, \theta) dx = 1$ can only happen when $f(\cdot , \theta) \stackrel{a.s}{=} f(\cdot , \theta_{0})$.
\end{observation}

The quantity 
\begin{align*}
	0 \le  -\left( l(\theta) - l(\theta_{0}) \right) &= \E_{\theta_{0}} \log \frac{f(X, \theta_{0})}{f(X, \theta)} \\
	&\equiv KL\left( P_{\theta_{0}}, P_{\theta} \right) 
\end{align*}
is called the Kullback-Leibler divergence (entropy-distance), which builds the basis of statistical information theory. In particular, the differential geometry of the map $\theta \to KL\left( P_{\theta_{0}}, P_{\theta} \right)$ determines what 'optimal' inference in a statistical model could be.

Let us say a statistical model $\{f(\cdot , \theta) : \theta \in \Theta\} $ is regular if $\frac{d}{d \theta} \frac{d^2}{d \theta d \theta ^{T}} ( = \nabla_{\theta}, \nabla_{\theta} \nabla_{\theta}^{T} )$ of $f(x, \theta)$ can be interchanged with $\int (\cdot ) dx$ integration.

\begin{observation}[2]
	In a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta\}$, we have $\forall \theta \in\intr(\Theta)$ (the interior in $\R^{p}$) that
	\begin{align*}
		0 &= \frac{d}{d \theta} 1 \\
		&= \frac{d}{d \theta} \int_{\chi} f(x, \theta) dx \\
		&= \int_{\chi} \frac{d}{d\theta} f(x,\theta) dx \\
		&= \int_{\chi} \frac{d}{d \theta} [\log f(x, \theta)] f(x, \theta) dx \\
		&= \E_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \right]
	\end{align*}
	In other words, the score vector will be $\E_{\theta}$ centred $\forall \theta \in\intr(\Theta)$
\end{observation}

\begin{defn}
	Let $\Theta \subseteq \R^{p}, \theta \in \intr(\Theta)$. Then the $p\times p$ matrix
	\[
		I(\theta) = \E_{\theta}\left[ \frac{d}{d \theta} \log f(X, \theta) \frac{d}{d \theta} \log f(x, \theta)^{T} \right] 
		\text{ , (if it exists) }
	\]
	is called the \textit{Fisher information (matrix)} of the model $\{f(\cdot , \theta) : \theta \in \Theta\} \text{ at } \theta$.
\end{defn}

\begin{prop}
	In a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta\}$, we have $\forall \theta \in\intr (\Theta), \Theta \subseteq \R^{p}, p\ge 1 $,
	\[
		I(\theta) = - \E_{\theta} \left[ \frac{d^2}{d \theta d \theta^{T}} \log f(X, \theta) \right] 
	\] 
\end{prop}

\begin{proof}
	As earlier, we write
	\begin{align*}
		0 &= \frac{d}{d \theta d \theta^{T}} 1 \\
		&= \frac{d^2}{d \theta d \theta^{T}} \int_{\chi} f(x, \theta) dx \\
		&= \int_{\chi} \frac{d^2}{d \theta d \theta^{T}} f(x, \theta) dx \tag{*}
	\end{align*}

	Moreover, using the chain/product rule, we have
	\begin{align*}
		\frac{d^2}{d\theta d\theta^{T}}\log f(X, \theta) &= \frac{d}{d \theta^{T}} \left[ \frac{1}{f(X, \theta)} \frac{d}{d\theta} f(X,\theta) \right] \\
		&= \frac{1}{f(X,\theta)} \frac{d^2}{d\theta d\theta^{T}} f(X, \theta) - \frac{1}{f^2(X, \theta)} \frac{d}{d \theta} f(X, \theta) \frac{d}{d \theta} f(X, \theta)^{T}
	\end{align*}
	Then, taking $\E_{\theta}$ expectation, we see
	 \begin{align*}
		 \E_{\theta} \left[ \frac{d^2}{d\theta d\theta^{T}} \log f(X, \theta)\right] &= \int_{\chi} \frac{d^2}{d\theta d\theta^{T}} f(X,\theta) \frac{f(X, \theta)}{f(X,\theta)} dx - \E_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \frac{d}{d \theta} \log f(X, \theta)^{T}  \right] 
	\end{align*}
	Hence by $(*)$, (3.1) holds.
\end{proof}

\begin{remark}
\item[1)] When $p=1$, the above expressions simplify and we have
	\begin{align*}
		I(\theta) &= \E_{\theta} \left( \left[ \frac{d}{d \theta} \log f(X, \theta) \right]^2  \right)  \\
		&= Var_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \right]  \\
		&= -\E_{\theta \left[ \frac{d^2}{d \theta^2} \log f(X, \theta) \right] }
	\end{align*}

\item[2)] If $X = (X_1, \ldots, X_n)$ consists of iid copies of $X$ so that its pdf/pmf is given by  \[f(x_1, \ldots, x_n, \theta) = \prod_{i=1}^{n} f(x_i, \theta),\]
	then the Fisher information \textit{tensorises}, that is 
	\begin{align*}
		I_n(\theta) &= \E_{\theta}\left[ \frac{d}{d \theta} \log f(X_1, \ldots, x_n, \theta) \frac{d}{d\theta} \log f(X_1, \ldots, X_n, \theta)^{T} \right] \\
		&= \sum_{i,j = 1}^{ n} \E_{\theta} \left[ \frac{d}{d \theta} f(X_{i}, \theta) \frac{d}{d \theta} \log f(X_j, \theta)^{T} \right]  \\
		&= \sum_{i=1}^{n} \E_{\theta} \left[ \frac{d}{d \theta} \log f(X_i, \theta) \frac{d}{d\theta} f(X_i, \theta)^{T} \right] + \sum_{i\neq j} \E_{\theta} \left[ \frac{d}{d\theta} \log f(X_i, \theta) \right] \E_{\theta}\left[ \frac{d}{d\theta} \log f(X_j, \theta) \right]  \\
		&= nI(\theta) + 0
	\end{align*}
	Where $I(\theta)$ is the Fisher information 'per observation', i.e for $\{f(x, \theta) : \theta \in \Theta, x \in \R\} $.
\end{remark}

\newpage

\begin{prop}
	(Cramer-Rao lower bound/inequality)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim}$ from a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta, \Theta \subseteq \R\}$ and suppose $\tilde{\theta} = \tilde{\theta}(X_1, \ldots, X_n)$ is any unbiased estimator of $\theta$ (i.e $\E_{\theta} \tilde{\theta} = \theta \; \forall \theta \in \Theta)$. Then $\forall \theta \in \intr(\Theta)$,
	\[
		Var_{\theta} \tilde{\theta} \ge \frac{1}{nI(\theta)} \;  \forall n \in  \N
	\] 
\end{prop}

\begin{proof}
	Assume wlog  that $Var_{\theta} \tilde{\theta} < \infty$, and consider first $n=1$. recall the Cauchy-Schwarz inequality to the affect that $\cov^2(Y, Z) \le \var Y\text{ } \var Z$. For $Y= \tilde{\theta}$ and for $Z = \frac{d}{d\theta} \log f(X, \theta)$. Then $\E_{\theta} Z = 0$ by observation 2, and by the preceding remark, $\E_{theta} Z = Var_{\theta} Z = I(\theta)$. Thus by Cauchy-Schwarz inequality,
	\begin{align*}
		\var(\tilde{\theta}) &\ge \frac{\cov^2(Y, Z)}{I(\theta)} \text{, since} \\
		\cov(Y,Z) &= \E_{\theta}(YZ) \text{, as } E_{\theta} Z = 0 \\
		&= \int_{\chi} \tilde{\theta} (x) \left( \frac{d}{d\theta} \log f(x,\theta) \right) f(X, \theta) dx \\
		&= \int_{\chi} \tilde{\theta}(x) \frac{d}{d\theta} f(x,\theta) dx \\
		&= \frac{d}{d\theta} \int_{\chi} \tilde{\theta}(x) f(x, \theta) dx \\
		&= \frac{d}{d\theta} \E_{\theta} \tilde{\theta} \\
		&= \frac{d}{d \theta} \theta \\
		&= 1
	\end{align*}

	For general n, replace $Z$ by $\frac{d}{d\theta} \log \prod_{i=1}^{n} f(X_{i}, \theta)$, and use that 
\[
	\E_{\theta} g(X_1, \ldots, X_n) = \int_{\chi} g(x_1, \ldots, x_n) \prod_{i=1}^{n} f(x_1, \ldots, x_n, \theta ) dx_{1} \ldots dx_{n}
,\]
and use that Fisher information tensorises. 
\end{proof}

Let us recall also

\begin{corol}
	If $\tilde{\theta}$ is not necessarily unbiased, the proof still gives
	\[
		Var_{\theta} (\tilde{\theta}) \ge \frac{(\frac{d}{d\theta} \E_{\theta} \tilde{\theta} )^2} {nI(\theta)} \text{ } \forall \theta \in \Theta, \Theta \subseteq \R
	\] This is the CR-inequality for biased estimators. 
\end{corol}

A multi-dimensional version of the CRLB can be obtained from considering estimation of general differentiable functions $\Phi : \Theta \to \R, \Theta \in \R^{p}$. Then one shows that for any unbiased estimator $\tilde{\Phi} = \tilde{\Phi}(X_1, \ldots, X_n)$, where $X_i \stackrel{iid}{\sim} \{f(\cdot , \theta) : \theta \in \Theta\} $, we have
\[
	Var_{\theta} \tilde{\Phi} \ge \frac{1}{n} \frac{\partial\Phi}{\partial\theta}^{T}(\theta)I(\theta)^{-1}\frac{\partial\Phi}{\partial\theta}(\theta) \text{,  } \forall \theta \in\intr(\Theta)
\] 
Indeed, for p=1, the proof is the same, but replacing $\frac{d}{d\theta} \E_{\theta} \tilde{\theta} = \frac{d}{d\theta} \theta = 1$ by $\frac{d}{d\theta} \E_{\theta} \tilde{\Phi} = \frac{d}{d\theta} \Phi(\theta)$, and for $p>1$ it only needs notational adjustment.

In particular, setting $\Phi(\theta) = \alpha ^{T} \theta$ for any $\alpha \in  \R^{p}$, we see that for any unbiased estimator $\tilde{\theta}$ of $\theta$ in $\R^{p}$, we also have
\[
	Var_{\theta} (\alpha ^{T}\tilde{\theta}) \ge \frac{1}{n} \alpha ^{T}I(\theta)^{-1}\alpha \text{     } \forall \alpha \in \R^{p},
\] 

so that $\cov_{\theta} \tilde{\theta} - \frac{1}{n} I(\theta)^{-1}$ is positive semi-definite, hence using the order structure on symmetric $p\times p$ matrices,
\[
	\cov_{\theta} \tilde{\theta} > \frac{1}{n} I(\theta)^{-1}, \forall \theta \in\intr(\Theta)
\] 

\begin{note}
	Here, > is in the sense of our order structure: for symmetric $n\times n$ matrices $A$ and $B$,  $A>B$ if $A-B$ is positive semi-definite.
\end{note}

\begin{eg}
	Consider $X \sim N(\theta, \Sigma)$, where $\theta = \begin{pmatrix} \theta_1 \\ \theta_2 \end{pmatrix} \in \R^{2}, \Sigma$ is positive definite $[n=1] $.

	Case 1: Suppose one wants to estimate $\theta_1$, and  $\theta_2$ is known. Then (see example sheet), one finds the Fisher information $I_{n}(\theta)$ of this one-dimensional statistical model $\{f(\cdot , \theta_1), \Theta_1 \in  \R\} $, with CRLB  $I_1(\theta)^{-1}$.

	Case 2: Now suppose $\theta_2$ is unknown. Then one can compute the $2\times 2$ information matrix $I_2(\theta)$, and the CRLB for estimating $\theta_1$ is, with $\Phi(\theta) = \theta_1$, 
	\[
		\frac{\partial\Phi}{\partial\theta}^{T} I(\theta)^{-1} \frac{\partial\Phi}{\partial\theta}
	\] 
	One can see $CRLB(1) < CRLB(2) $ unless  $\Sigma$ is diagonal
\end{eg}

\section{Asymptotic Theory for MLEs}

We will investigate the large sample performance of estimators $\tilde{\theta}(X_1, \ldots, X_n)$, specifically the MLE $\hat{\theta}_{MLE}$, as $n\to \infty$. The main goal will be to prove
\begin{align*}
	\hat{\theta}_{MLE} \underset{n\to\infty}{\approx} N\left(\theta, \frac{1}{n} I(\theta)^{-1}\right), \forall \theta \in  \Theta
,\end{align*} 
 in a sense to be made precise later.

\newpage

\subsection*{Stochastic Convergence: Concepts and Facts}

\begin{defn}
	Let $(X_n: n \in \N), X$ be random vectors in $\R^{k}$, defined on some probability space $(\Omega, \mathcal{A}, \P )$

		1) We say $X_n\to X$ almost surely, $X_n \stackrel{a.s}{\to} X \text{ as } n\to \infty$, if
		\begin{align*}
			\P\left(\omega \in \Omega : \|X_n (\omega) - X(\omega)\| \to 0 \text{ as } n\to \infty \right) &= 1 \\
			\text{ i.e. }P\left(\|X_n - X\| \to  0 \text{ as } n\to \infty\right) &=1
		\end{align*}

		2) We say $X_n \to  X$ in probability, $X_n \to ^{P} X$ as $n\to \infty$, if $\forall \epsilon > 0$,
		\begin{align*}
			P\left( \|X_n - X\| > \epsilon\right) \to 0 \text{ as } n \to \infty
		\end{align*}
		
\end{defn}

\begin{remark}
	The choice of norm on $\R^{k}$ is irrelevant. Also, one shows (see example sheet) that $X_n \stackrel{a.s}{\to}^{P} X$ as $n\to \infty$ is equivalent to $X_{n_{j}} \stackrel{a.s}{\to } ^{P} X_j $ as $n\to \infty \; \forall j = 1, \ldots, k$.
\end{remark}

\begin{defn}
	We say $X_n \to X$ in distribution (or in law), writing $X_n \to ^{d} X$ as $n \to \infty$, if
	\begin{align*}
		P\left(X_n \le t\right) \to P\left( X \le t \right) \forall t \in \R^{k}
	,\end{align*}
	for which $t \mapsto P\left( X \le t \right) $ is continuous.
\end{defn}

Recall, $P( Z\le t) := P\left( Z_1 \le t_1, \ldots, Z_k \le t_k \right).$

The following facts on stochastic convergence will be frequently used, and can be proved with measure theory.

\begin{prop}
	1) $X_n \underset{n\to \infty}{\overset{a.s}{\to}} X \implies X_n \underset{n\to \infty}{\to^{P}} X \implies X_n \underset{n\to \infty}{\to^{d}} X$, but any converse is false in general.

	2) (Continuous Mapping Theorem) If $X_n, X$ take values in $\chi \subseteq \R^{k} $, and $g:\chi \to \R^{d}$ is continuous, then $X_n \underset{n\to \infty}{\to} X \text{ a.s/P/d } \implies g(X_n) \underset{n \to  \infty}{\to} g(X) \text{ a.s/P/d respectively} $

	3) (Slutsky's Lemma) Suppose $X_n \underset{n\to \infty}{\to^{d}} X, Y_n \underset{n\to \infty}{\to^{d}} c$,  $c$ constant (non-stochastic), then
	 \begin{align*}
		 Y_n &\to ^{P} c, n\to \infty  \\
		 X_n + Y_n &\to^{d} X + c, n\to \infty  \\
		 X_nY_n &\to^{d} cX \\
		 \frac{X_n}{Y_n} &\to ^{d} \frac{X}{c} \text{, provided $c \neq 0$, as $n\to \infty$ } \\
		 \text{If $(A_n)_{ij}$ are random matrices s.t. $(A_n)_{ij} \to^{P} A_{ij}$, then} \\
		 A_n X_n &\to^{d} AX \text{ as } n\to \infty
	 \end{align*}
	 4) If $X_n \to^{d} X \text{ as } n \to \infty$, then $X_n$ is stochastically bounded $\left[ = O_P(1) \right]$, that is $\forall \epsilon > 0, \exists M_{\epsilon}$ s.t. for all sufficiently large  $n, P\left( \|X_n\| > M_{\epsilon} \right) < \epsilon$ 

\end{prop}

\subsection{Law of Large Numbers (LLN) and Central Limit Theorem (CLT)}

Consider $X_1,\ldots,X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$. This sequence can be realised as the coordinate projections of the infinite product probability space $\left( \Omega, \mathcal{A}, \P \right) = \left( \R^{\N}, \mathcal{B}^{\N}, P^{\N} \right)$, where $P^{\N} = \otimes_{i=1}^{\infty} \P$.

Under this product space, we can make some simultaneous statements about the stochastic behaviour of our $X_i$.

\begin{thm}
	(Weak Law of Large Numbers)

	If $\var(X) < \infty$, we have that 
	\begin{align*}
		P\left( \left| \frac{1}{n}\sum_{i=1}^{n} (X_{i} - \E(X)) \right| > \epsilon \right) &\le \frac{\var\left( \overline{X} - \E(X) \right)}{n\epsilon^2} \text{ by Chebyshev} \\ 
		&= \frac{\var(X)}{n\epsilon^2} \text{ as $X_i$ iid}\\
		&\to 0 \text{ as  } n \to \infty
	\end{align*}
\end{thm}	

\begin{thm}
	(Strong Law of Large Numbers)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$ such that $\E\left(\|X\|  \right)  < \infty$. Then $\overline{X} \overset{a.s}{\to}^{P} \E(X) \text{ as } n \to \infty$
\end{thm}

This is harder to prove than the Weak version, so we don't.

The stochastic fluctuations of $\overline{X}$ about $\E(X)$ are of order $\frac{1}{\sqrt{n} }$ and as long as $\var(X) < \infty$, \textit{always} look normally distributed.

\begin{thm}
	(Univariate Central Limit Theorem)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R$, with $\var(X) = \sigma^2 < \infty$. Then

	\[
		\sqrt{n} (\overline{X} - \E(X)) \underset{n \to  \infty}{\to^{d}} N(0, \sigma^2)
	\] 
\end{thm}

To give a multivariate version, we recall that $X \in \R^{k}$ is multivariate normal if $\forall t \in \R^{k}$, $t^{T}X$ is univariate normal, and write $X\sim N_{\mu}(\mu, \Sigma)$, where $\mu$ is our mean  $\E(X)$, and  $\Sigma $ is our covariance matrix $\var(X)$.

In fact, X is uniquely characterised as the random variable on  $\R^{k}$ such that $t ^{T}X \sim N(t ^{T}\mu, t ^{T}\Sigma t)$ for all $t \in \R^{k}$.

If $\Sigma$ is invertible, then X has pdf
\[
	f(x) = \frac{1}{\sqrt{(2\pi)^{n}|\Delta \Sigma|} } \exp\left( - \frac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x - \mu) \right) 
.\] 

If $A \in \R^{d\times k}$ and $b \in \R^{d}$, then
\[
	AX + b \sim N_{d}\left( A\mu + b, A\Sigma A^{T} \right) 
\] 

Furthermore, if $A_{n} \to^{P} A$ are random matrices, and $X_n \to ^{d} N_{k}\left( \mu, \Sigma \right) $, then $A_n X_n \to ^{d} N_{d}\left( A\mu, A\Sigma A^{T} \right) $.

\begin{thm}
	(Multivariate Central Limit Theorem)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$ with $\var(X) = \Sigma$ positive definite. Then

	 \[
		 \sqrt{n} \left( \overline{X} - \E(X) \right) \underset{n \to \infty}{\to ^{d}} N_{k}(0, \Sigma)
	\] 
\end{thm}

From this, and the SLLN, we can bound in probability the deviations.

\begin{defn}
	For a sequences $Y_1, \ldots, Y_n$ and $c_1, \ldots, c_n \in \R\setminus\{0\}$, we define 
	 \[
		 Y_n = O_{P}(c_n)
	\]
	if
	\[
		\forall \epsilon > 0, \exists M,N > 0 \text{ s.t } P\left( \left| \frac{Y_n}{c_n} \right| > M \right) < \epsilon \text{  } \forall n > N
	\] (By Prokhorov's Theorem) 
\end{defn}

\begin{corol}
	\[
		\overline{X} - \E(X) = O_P \left(\frac{1}{\sqrt{n} }\right)
	\] 
\end{corol}

\begin{eg} (Confidence Intervals)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R$ with mean  $\mu_{0}$ and variance $\sigma^2$.

	Define  $\mathcal{C}_n = \{\mu \in \R : |\overline{X} - \mu| \le \frac{\sigma z_{\alpha}}{\sqrt{n} }\} $, where $P\left( |Z| \le z_{\alpha} \right) = 1 - \alpha $ for $Z\sim N(0,1)$ as our 'confidence region'

	\begin{align*}
		P\left( \mu \in \mathcal{C}_n \right) &= P\left( |\overline{X} - \mu_0| \le \frac{\sigma z_{\alpha}}{\sqrt{n} }  \right) \\
		&= P\left( |\overline{X} - \E(X)| < \frac{\sigma z_{\alpha}}{\sqrt{n} } \right) \\
		&= P\left( \sqrt{n} \left| \frac{1}{n} \sum_{i=1}^{n} \frac{X_i - \E(X_i)}{\sigma} \right| \le z_{\alpha} \right) \\
		&\underset{n \to \infty}{\to } P\left( |Z| \le z_{\alpha} \right) \text{ by CLT} \\
		&= 1- \alpha
	\end{align*}

	where we have used the continuous mapping theorem, and because $z_{\alpha}$ is a continuity point of the distribution of $Z$. Therefore $\mathcal{C}_n$ is an asymptotic confidence region with confidence level (or coverage) $1-\alpha$. (Alternatively of size/significance level  $\alpha$).

	When  $\sigma$ is unknown, we can replace it in $\mathcal{C}_n$ by $S_n$, where  $S_{n}^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \overline{X})^2$, and the same conclusion follows, using the asymptotic distribution of the t-distribution,
	\[
		t_n := \frac{\sqrt{n}\left (\overline{X} - \E(X)\right)}{S_n} \underset{n \to \infty}{\to ^{d}} N(0,1)
	\] 
\end{eg}

\subsection{Consistency of MLEs}

\begin{defn}
	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X$ form a statistical model $\{P_{\theta} : \theta \in \Theta\, \Theta \subseteq \R^{p}\}$. Then we say that an estimator $\tilde{\theta}_{n} = \tilde{\theta}(x_1, \ldots, x_n)$ is consistent (for that model) if $\tilde{\theta}_n \underset{n\to \infty}{\to} \theta$ in $(P_\theta^{\N})$-probability, $\forall \theta \in \theta$.
\end{defn}

\begin{assumption}
	Suppose a statistical model $\{f (\cdot , \theta) : \theta \in \Theta \}, \Theta \in \R^{p}$ of pdf/pmf on $\chi \subseteq \R^{d}$ satisfies the following conditions:

\begin{enumerate}
	\item $f(x,0) > 0 \; \forall (x, \theta) \in  (\chi, \Theta)$
	\item $\int_{\chi} f(x,\theta) dx = 1 \; \forall \theta \in \Theta$
	\item The map $\theta \mapsto f(x, \theta)$ is continuous $\forall x \in  \chi$
	\item  $\Theta \in \R^{p}$ is compact
	\item $\theta = \theta' \iff f(\cdot , \theta) = f(\cdot , \theta') \; \forall \theta, \theta' \in \Theta $
	\item $\E_{\theta} \sup_{\theta \in \Theta} |\log f(x, \theta)| < \infty$
\end{enumerate}

\end{assumption}

\begin{remark}
	\begin{enumerate}
		\item The above conditions justify the application of Jensen's inequality in Observation (1) of section 3 (Information Geometry) - in particular the map 
			\[\theta \mapsto l(\theta) = \E_{\theta_0} \log f(X, \theta)\] is maximised uniquely at $\theta_0 \in \Theta$.
		\item Using the dominated convergence theorem, one can integrate the limit
			\[
				\lim_{\eta \to  0} |\log f(X, \theta + \eta) - \log f(X, \theta)| = 0
			\] wrt $P_\theta$, and conclude that also the map $\theta \mapsto f(\theta)$ is continuous under assumption A.
	\end{enumerate}
\end{remark}

\begin{thm}
	Suppose that the statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies assumption A. Then an MLE exists, and any MLE is consistent.
\end{thm}

\begin{proof}
	The map $\overline{l}_n(\theta) = \frac{1}{n} \sum_{i=1}^{n} \log f(X_i, \theta)$ is continuous on the compact set $\Theta \in \R^{p}$, so by the Heine-Borel theorem, $\overline{l}_n$ obtains a maximum on $\Theta$, hence a MLE  $\hat{\theta}_n $ exists.

	Now, let  $\hat{\theta}_n$ be any MLE, and for a true (arbitrary) value $\theta_0 \in \Theta$ we now prove that $\hat{\theta}_n \underset{n\to \infty}{\to ^{P}} \theta_0$ (in $P_{\theta_0}^{\N}$ probability). The idea is that maximisers $\hat{\theta}_n$ of $\overline{l}_n$ over $\Theta$ should converge to the unique maximiser  $\theta_0$ of $l$ over $\Theta$, since  $\overline{l}_n(\theta) \underset{n\to \infty}{\to ^{p}} l(\theta)$ by the LLN, for all $\theta \in  \Theta$ pointwise.

	This is generally false unless one has uniform convergence.
	\[
		(\ast) \sup_{\theta \in \Theta} |\overline{l}_n(\theta) - l(\theta)| \underset{n\to \infty}{\to ^{P}} 0 \text{ (see Ex sheet for counterexample) }
	\] 

	We show later that $(\ast)$ indeed holds under the maintained hypothesis.

	Define for any $\epsilon > 0$ 
	\[
		\Theta_{\epsilon} = \{\theta \in  \Theta : \|\theta - \theta_0\| \ge \epsilon\} 
	\] 
	which is again a compact subset of $\R^{p}$ intersection of closed and compact). Thus the function $l(\theta)$ attains its bounds on  $\Theta_{\epsilon}$, so
	\[
		c(\epsilon) = \sup_{\theta \in \Theta_{\epsilon}} l(\theta) = l(\overline{\theta}_{\epsilon} \in \Theta_{\epsilon}) < l(\theta_0) \text{ since l is maximised uniquely at } \theta_0
	\] 
	Thus we can choose $\delta(\epsilon)$ small enough such that 
	 \[
	 c(\epsilon) + \delta(\epsilon) < l(\theta_0) - \delta(\epsilon) \tag{\dag}
	\]

	Now, note that  
	\[
		\sup_{\theta \in  \Theta_{\epsilon}} \overline{l}_n = \sup_{\theta \in \Theta_{\epsilon}} l(\theta) + (\overline{l}_n(\theta) - l(\theta) \le \sup_{\theta \in \Theta_{\epsilon}} l(\theta) + \sup_{\theta \in \Theta_{\epsilon}} |\overline{l}_n (\theta) - l(\theta)|
\]

	Next, define events (subsets of $\R^{\N}$ supporting $(X_1, X_2,\ldots)$) 
	\[
		A_n(\epsilon) = \left\{\sup_{\theta \in \Theta_{\epsilon}} |\overline{l}_n(\theta) - l(\theta) | \le  \delta(\epsilon)\right\} 
	\] 

	On these events, we have 

	\[
		\sup_{\theta \in \Theta_{\epsilon}} \overline{l}_n (\theta) \le  c(\epsilon) + \delta(\epsilon) < l(\theta_0) - \delta(\epsilon) \le \overline{l}_n(\theta_{0}) \text{, since on $A_n(\epsilon)$ we also have } |l(\theta_0) - \overline{l}_n(\theta)| < \delta(\epsilon)
	\] 

	Thus if we assume $\hat{\theta}_n \in \Theta_{\epsilon}$, then by what precedes

	\[
		\overline{l}_n(\hat{\theta}_n) \le  \sup_{\theta \in \Theta_{\epsilon}} \overline{l}_n(\theta) < l(\theta_0) \text{ on } A_n(\epsilon)
	\]

	This is a contradiction to $\hat{\theta}_n$ being a maximiser. Therefore on $A_n(\epsilon)$ we must have  $\hat{\theta}_n \in  \Theta_{\epsilon}^{c}$, or in other words

	\[
		A_n(\epsilon) \subseteq \{\| \hat{\theta}_n - \theta_0\| < \epsilon\} 
	.\] 
	Now, by $(\ast)$,  $P\left( A_n(\epsilon) \right) \to 1$. We conclude that
	 \begin{align*}
		&P\left( \| \hat{\theta}_n - \theta_0 \| < \epsilon  \right) \underset{n\to \infty}{\to } 1 \text{ or } \\
		&P\left( \| \hat{\theta}_n - \theta_0\| < \epsilon \right) \underset{n\to \infty}{\to } 0
	\end{align*}

	Since $\epsilon$ is non arbitrary,  $\hat{\theta}_n \underset{n\to \infty}{\to^{P} } \theta_0$, and the proof is complete modulo the verification of $(\ast)$.
\end{proof}

\begin{remark}
	The previous proof works as well if $(\Theta, d)$ is any compact metric space, and if continuity in assumption A is for the metric d.
\end{remark}

To verify ($\ast$), we now make the following (non-examinable) digression:

For a (meas.) $\chi \in \R^{d}$ and (meas.) $h: \chi \to \R$, and let $X_1, \ldots, X_n \stackrel{iid}{\sim} X$ in $\chi$ with law $P$. Then the $h(X_i)$'s are also iid, and if $\E |h(X)| < \infty$ ($\E = \E_p$), then by the SLLN,
\[
	\frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \underset{n \to \infty}{\to^{a.s}} 0 
\] 

Next, let $h_1, \ldots, h_N$ be a finite collection of such functions. Then
\[
	P\left( \left|\frac{1}{n} \sum_{i=1}^{n} h_j (X_i) - \E h_j (X)  \right| \underset{n \to \infty}{\to} 0 \right) \equiv P\left( A_j \right) = 1.
\] 
Moreover,
\[
	P\left( \max_{j=1,\ldots,N} |\frac{1}{n} \sum_{i=1}^{n} h_j (X_i) - \E h_j (X)  | \underset{n \to \infty}{\to} 0 \right) = P\left( \bigcap_{j=1}^{N} A_j  \right) = 1,
\] since
\[
	P\left( \left(\bigcap_{j=1}^{N}A_j\right)^{c}  \right) = P\left( \bigcup_{j=1}^{N} A_j^{c}  \right) \le \sum_{j=1}^{N}P\left( A_j^{c} \right) = 0.
\] 

To transfer to an infinite collection of h's, let us say that a family of brackets $[\underline{h}_j, \overline{h}_j], \underline{h}_j, \overline{h}_j : \chi \to \R, j=1,\ldots,N$, covers a class $\mathcal{H}$ of maps on $\chi$ if  $\forall h \in  \mathcal{H}, \exists j$ s.t $\underline{h}_j \le h(x) \le \overline{h}_j \forall x \in \chi$.

\begin{prop}
	Suppose that $\forall \epsilon > 0$ there exist brackets $[\underline{h}_j, \overline{h}_j], j=1,\ldots,N(\epsilon)$ covering $\mathcal{H}$ and such that
	\begin{enumerate}
		\item $\E |\underline{h}_j(x) | < \infty, \E|\overline{h}_j(x)| < \infty$
		\item $\E |\overline{h}_j(x) - \underline{h}_j(x)| < \epsilon$
	\end{enumerate}
	Then
	\[
		\sup_{h \in \mathcal{H}} \left| \frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \right| \underset{n \to \infty}{\to^{a.s} } 0
	\] 
\end{prop}

\begin{proof}[Non-examinable]
	Let $\epsilon = \frac{1}{m}$, where $m \in \N$ is arbitrary. Then take $N(\frac{\epsilon}{3})$-many brackets covering $\mathcal{H}$, and note that by the preceding argument, we have
	\[
		P\left( \max_{j=1,\ldots,N(\frac{\epsilon}{3})} \left| \frac{1}{n}\sum_{i=1}^{n} \underline{h}_j (X_i) - \E \underline{h}_j (X) \right| \le  \frac{\epsilon}{3}, \forall n \ge n_o (\epsilon) \right) = P\left( A_{\epsilon} \right) = 1
	,\] and equivalently for $\overline{h}_j$

	Now, pick $h \in  \mathcal{H}$ arbitrary, and write for the respective bracket $[\underline{h}_j, \overline{h}_j] \ni h$ 
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^{n}h(X_i) - \E h(X) &\le \frac{1}{n}\sum_{i=1}^{n} \overline{h}_j(X_i) - \E \overline{h}_j(X) + \E \overline{h}_j (X) - \E h(X) \\
		&\le \frac{\epsilon}{3} + \E |\overline{h}_j(X) - \underline{h}_j (X) |  \\
		&\le \frac{2\epsilon}{3}
	\end{align*}
	Likewise, we get
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^{n}h(X_i) - \E h(X) &\ge \frac{1}{n}\sum_{i=1}^{n} \underline{h}_j(X_i) - \E \underline{h}_j(X) + \E \underline{h}_j (X) - \E h(X) \\
		&\ge - \frac{2\epsilon}{3}
	\end{align*}

	Therefore on the event $A = \bigcap_{m} A_m $ we have
	\[
		\left| \frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \right| < \frac{2\epsilon}{3} < \epsilon
	,\] and since
	\[
	P\left( A^{c} \right) \le \sum_{m=1}^{\infty} P\left( A_m^{c} \right) = 0
	,\] the proof is complete.
\end{proof}

From this proposition, we deduce

\begin{prop}
	Let $\chi \subseteq \R^{d}, \Theta \in \R^{p}$ compact, and suppose $\theta \mapsto q(x, \theta)$ is continuous  $\forall x$ (and x measurable $\forall \theta$), and that  $\E \sup_{\theta \in \Theta} |q(X, \theta)| < \infty$. If $X_1, \ldots, X_n \stackrel{iid}{\sim} X$, then
	\[
		\sup_{\theta \in \Theta} \left| \frac{1}{n} \sum_{i=1}^{n} q(X_i, \theta) - \E q(X, \theta) \right| \underset{n \to  \infty}{\to ^{a.s}} 0
	\]
\end{prop}

\begin{remark}
	By choosing $q(X, \theta) = \log f(X,\theta)$, we verify $(\ast)$ in the proof of the last theorem.
	
\end{remark}

\begin{remark}
	The condition that the expectation of the supremum of $q$ over $\theta$ is finite can be seen to be necessary as $\E \|Z\|< \infty$ in the LLN for $Z_1, \ldots, Z_n$ iid in the space $\mathcal{C}(\Theta)$ of continuous functions on the compact space $\Theta$.
\end{remark}

\subsection{Asymptotic Distribution of MLEs}

\begin{defn}
	We say that an estimator $\tilde{\theta}_n$ is asymptotically efficient in a regular statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ if $\lim_{n\to \infty} n Var_{\theta} (\tilde{\theta)} = I(\theta)^{-1}, \forall \theta \in\intr\Theta$.
\end{defn}

\newpage

\begin{assumption}
	Consider a model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}, \Theta \in \R^{p}$ of pdf/pmfs on $\chi \subseteq \R^{d}$ such that $f(x, \theta) > 0$ for all  $x \in \chi$ and all $\theta \in \Theta$, and such that $\int_{\chi} f(x, \theta) dx = 1$ for every $\theta \in  \Theta$.

	Let $\theta_0$ be a fixed ('true') value, and assume

	\begin{enumerate}
		\item $\theta_0 \in\intr\Theta$
		\item There exists an open set $U$ satisfying $\theta_0 \in U \subseteq \Theta$ such that $f(x, \theta)$ is, for every $x\in \chi$, twice continuously differentiable wrt $\theta$ in $U$
		\item The  $p\times p$ matrix $\E_{\theta_0} \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}}  $ is non singular, and 
			\[
				\E_{\theta_0} \left\|\frac{\partial \log f(X, \theta_0)}{\partial\theta} \right\|^2 < \infty
			\]
		\item There exists a compact ball $K \subset U$ (with non-empty interior) centered at $\theta_0$ such that
			\[
			\E_{\theta_0} \left\| \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}} \right\| < \infty
			,\]
			\[
				\int_{\chi} \sup_{\theta \in K} \left\| \frac{\partial f(x, \theta)}{\partial\theta}\right\| dx < \infty \text{ and } \int_{\chi} \sup_{\theta \in  K} \left\| \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}} \right\| dx < \infty
			\]
			\item Suppose the MLE $\hat{\theta}_n$ in the model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ based on the sample $X_1, \ldots, X_n$ exists and is consistent, i.e. $\hat{\theta}_n \underset{n\to \infty}{\to ^{P_{\theta_{0}}}} \theta_0$.
	\end{enumerate}
\end{assumption}

\begin{thm}
	Suppose a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ is regular in the sense that it satisfies the conditions B on the handout. Then, if $\hat{\theta}_n$ is the MLE, based on $X_1, \ldots, X_n \stackrel{iid}{\sim}$ from the model, we have $\sqrt{n} (\hat{\theta}_n - \theta_0) \underset{n \to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$
\end{thm}

\begin{idea}[p=1]
	For $\hat{\theta}$, we must have that for $l_n(\theta) = \sum_{i=1}^{n} \log f(X_i, \theta)$
	\[
		0 = l_{n}' (\hat{\theta}) = l_{n}' (\theta_0) + l _{n}''(\overline{\theta}_n (\hat{\theta}_n - \theta_0) \text{ (MVT) }
	,\]
	so
	\[
		\sqrt{n} (\hat{\theta} - \theta_0) = \frac{\sqrt{n} l_{n}' (\theta_0)}{- \frac{1}{n} l_{n}''(\overline{\theta}_n)} = \frac{\sqrt{n} \sum_{i=1}^{n} \frac{d}{d\theta} \log f(X_i, \theta) - \E(\cdot , 1) }{- \frac{1}{n} \sum_{i=1}^{n} \frac{d^2}{d\theta ^2} \log f(X_i, \overline{\theta}_n)} \underset{n\to \infty}{\to^{d}}  \frac{N(0, I(\theta_0)}{I(\theta_0)} = N(0, I(\theta_0)^{-1})
	\] 
\end{idea}
		
\begin{lemma}
	Observations 2 and 3 from the Information Geometry section are valid.		
\end{lemma}
	
\begin{proof}
	Apply the dominated convergence theorem and Assumption B.
\end{proof}


\begin{proof}(8.2)
        Here, $\P \equiv P_{\theta_0}^{\N}, \E \equiv \E_{\theta_0}$.

	In proving convergence in distribution (say $Z_n \to ^{d} Z$), it suffices to restrict to any sequence $(E_n)$ of events (in $\R^{N}$ ) s.t. $P\left( E_N \right) \to  1$. Indeed,
	\[
		|P\left( Z_n \le  t \right) - P\left( Z_n \le t  \mid  E_n \right) | \le P\left( E_n^{c} \right) \to 0
	.\]

	By consistency, $\hat{\theta}_n \to ^{P} \theta_0$, hence the events $E_n = \{\hat{\theta}_n \in K\} $ have probability tending to 1, and we restrict to this event in what follows. Therefore we must have
	\begin{align*}
		0 &= \begin{pmatrix} \frac{\partial}{\partial\theta_1} \overline{l}_n (\hat{\theta}_n) \\ \vdots \\ \frac{\partial}{\partial\theta_p} \overline{l}_n (\hat{\theta}_n) \end{pmatrix} 
	\end{align*} 
	Where we recall $\overline{l}_n(\theta) = \frac{1}{n} \sum_{i=1}^{n} \log f(X_i, \theta)$.

	For any map $h: U \to \R$, we can apply the mean value theorem along the line segment \[\{t \hat{\theta}_n + (1-t) \hat{\theta}_0 : 0 < t < 1\} \] connecting $\hat{\theta}_n $ and $\theta_0$, and write
	\[
		h(\hat{\theta}_n) = h(\theta_0) + \frac{\partial h}{\partial \theta}^{T} \bigg\rvert_{\theta = \overline{\theta}} (\hat{\theta}_n - \theta_0) 
	\]
	here $\overline{\theta} = \overline{\theta}(h)$ is some mean value on that line segment.

	Second derivatives of $\overline{l}_n(\theta)$ are differentials of the map $u \mapsto \frac{\partial}{\partial\theta} \ln(\theta)  \mid _{\theta = u}$, and hence applying what precedes p-times to the vector entries $\frac{\partial}{\partial\theta_j} \overline{l}_n(\hat{\theta}_k)$, we obtain

	\begin{align*}
		0 &= \begin{pmatrix} \frac{\partial}{\partial\theta_j} \\ \vdots \end{pmatrix} \\
		&= \begin{pmatrix} \frac{\partial}{\partial\theta_j} \overline{l}_n(\theta_0) \\ \vdots \end{pmatrix} + \begin{pmatrix} & \vdots & \\ \cdots & \frac{\partial^2}{\partial\theta_i \partial\theta_j} \overline{l}_n(\overline{\theta}_{ij}) & \cdots \\ & \vdots &\end{pmatrix} (\hat{\theta}_n - \theta_0) \\
		&:= \overline{A}_n (\hat{\theta}_n - \theta_0)
	\end{align*}
	where $\overline{\theta}_{ij}$ is the $p\times 1$ vector arising from the $j^{th}$ application of the MVT.

	We will show
	\[
		\overline{A}_n \underset{n\to \infty}{\to ^{P}} -I(\theta_0) \tag{$\dagger$} 
	\]
	and in particular this implies convergence of $\|\overline{A}_n + I(\theta_0)\| \to ^{P} 0$ under the operator norm.

	Hence since $I(\theta_0)$ is non-singular, so is $\overline{A}_n$ on events of probability converging to $1$, and since we can rewrite
	\begin{align*}
		\sqrt{n}(\hat{\theta}_n - \theta_0) &= (-\overline{A}_n)^{-1} \sqrt{n} \frac{\partial}{\partial\theta} \overline{l}_n(\theta_0)   
	 \end{align*}
	 and the theorem follows from $(\dagger)$, Slutsky's Lemma, and since

	  \begin{align*}
		  \sqrt{n}\frac{\partial}{\partial\theta}\overline{l}_n(\theta) &= \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left( \frac{\partial}{\partial\theta} \log f(X_i, \theta_0) - \E \frac{\partial}{\partial\theta} \log f(X_i,\theta_0) \right) \\
		  & \underset{CLT}{\to ^{d}} N(0, I_{\theta_0}) \text{ as $n\to \infty$, applying Observation 2 to the second term above. } 
	 \end{align*}

	 To verify $(\dagger)$, it suffices (see Ex. sheet) to check convergence in probability of $\overline{A}_{n_{jk}}\to (-I_(\theta_0))_{jk}$.

	 Now, we write
	 \begin{align*}
		 \overline{A}_{n_{jk}} = &\frac{1}{n} \sum_{i=1}^{n}\frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) - \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) \\
		 &+ \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) - \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \theta_0) + (-I(\theta_0)_{jk}) \text{ (by Obs. 3) } 
	 \end{align*}
	 We denote this by $(1) + (2) - I(\theta_0)_{jk}$.

	 For $(1)$, notice that $\overline{\theta}_{(j)} \in K$, and hence with $q(x,\theta) = \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(x, \theta)$,

	 \[
		 |(1)| = |\frac{1}{n} \sum q(X_i, \overline{\theta}_j) - \E q(X, \overline{\theta}_{(j)})| \le \sup_{\theta \in K} |\frac{1}{n} \sum q(X_i, \overline{\theta}_j) - \E q(X, \overline{\theta}_{(j)})| \underset{n\to \infty}{\to ^{P}} 0 \text{ by the uniform LLN }
	 \] 

	 For $(2)$, we notice that  $\hat{\theta}_n \to ^{P} \theta_0 \implies \overline{\theta}_{(j)} \to ^{P} \theta_0 \text{ as } n\to \infty \;\forall j$, and since $\theta \mapsto \E q(X,\theta)$ is continuous, the continuous mapping theorem implies that 
	 \[
		 (2) = \E q(X, \overline{\theta}_j) - \E q(X, \theta_0) \underset{n\to \infty}{\to ^{P}} 0
	 ,\]
	 completing the proof of $(\dagger)$

\end{proof}

\begin{remark}
	\begin{enumerate}
		\item The assumption that $\theta \mapsto f(x,\theta)$ is $C^2$ can be relaxed to the existence of first derivatives (weak ones) by more involved proof methods (Le Cam Theory, see van der Vaart (1998)), including in particular the Laplace distribution (where one may show $I_n(\theta) = n)$. However, this cannot be weakened further, and for non-smooth parameterisation, the asymptotic theory for MLEs may be different as the example of $U(0, \theta), \theta \in [0,\infty)$ shows (see Ex. sheet).

		\item If the 'true' value $\theta_0$ lies at the boundary of $\Theta$, then the MLE is also not asymptotically normal (Ex. sheet $N(\theta, 1), \theta \in \Theta = [0, \infty) $)

		\item An asymptotic version of the Cramer-Rao lower bound can also be proved (see Le Cam theory), but it requires a restriction to 'regular' or 'uniformly consistent' (instead of unbiased) estimators to claim asymptotic efficiency. Some restriction on the class of estimators is indeed necessary, as the following example (due to Hodges) shows:
			Consider a statistical model $\{P_{\theta} : \theta \in \Theta \}, \Theta \subseteq \R, 0 \in  \Theta$, s.t $\sqrt{n}(\hat{\theta}_{MLE} - \theta) \underset{n \to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$ $ \forall \theta \in\intr\Theta$. (Recall that this implies that $\sqrt{n}(\hat{\theta} - \theta) $ is stochastically bounded, i.e, $\forall \epsilon > 0 \; \exists M_{\epsilon}$ s.t. $P\left( |\hat{\theta} - \theta| > \frac{M_{\epsilon}}{\sqrt{n} } \right) < \epsilon$, and in particular $\hat{\theta} \underset{n\to \infty}{\to ^{P}} \theta$)

			Define \[
				\tilde{\theta} = \tilde{\theta}_{Hodges} = \begin{cases}
					\hat{\theta} \text{ if } |\hat{\theta}| > n^{- \frac{1}{4}}
					\\
				0 \text{ if } |\hat{\theta}| < n^{- \frac{1}{4}}
			\end{cases}
		\] 

		Now for $\theta \neq 0$ and under $P_\theta$,

		 \begin{align*}
			 P\left( \tilde{\theta} \neq \hat{\theta} \right) &= P\left( |\hat{\theta}| \le n^{- \frac{1}{4}} \right) \\
			 &= P\left( |\hat{\theta} - \theta + \theta| \le n^{- \frac{1}{4}} \right) \\
			 &\le P\left( |\hat{\theta} - \theta| \ge \theta - n^{- \frac{1}{4}} \right) \\
			 &\le P\left( |\hat{\theta} - \theta| > \frac{1}{2}|\theta| \right) \text{ for large enough n } \\
			 &\underset{n\to \infty}{\to } 0 \text{ since  } \hat{\theta} \to ^{P} \theta, |\theta|\neq 0
		 \end{align*}

		 So for such $\theta$ we thus have $\sqrt{n} (\tilde{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$.

		 Next, if $\theta = 0$, we have under  $P_0$

		 \begin{align*}
		 	P\left( \tilde{\theta} \neq 0 \right) &= P\left( |\hat{\theta}| \ge n^{- \frac{1}{4}} \right) \\
			&= P\left( |\hat{\theta} - \theta| > n^{- \frac{1}{4}} \right) \\
			&= P\left( \sqrt{n}|\hat{\theta} - \theta| > n^{\frac{1}{4}}  \right)
		 \end{align*}

		 So for any $\epsilon$ > 0 and n s.t. $n^{\frac{1}{4}} > M_{\epsilon}$, we have by stochastic boundedness of $\sqrt{n}(\hat{\theta} - \theta) $ that the last probability is less that $\epsilon$. Hence we conclude that under  $P_0$, $\sqrt{n}(\tilde{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0,0)$, and so $\tilde{\theta}$ 'beats' the asymptotic efficiency bound $I(\theta)^{-1}$ at $\theta = 0$.
	\end{enumerate}
\end{remark}

\subsection{Plug-in MLEs and the Delta method}

Consider estimating a functional $\Phi : \Theta \to \R^{k}$, $\Theta \subseteq \R^{p}$ based on $X_i \stackrel{iid}{\sim} \{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, where $\hat{\theta}$ is the MLE for $\theta$. One can show that \textit{a} MLE in the model  $\{f\left( \cdot , \phi \right) : \phi = \Phi(\theta) \text{ for some } \theta \in \Theta \}$

The asymptotic normality and efficiency of  $\hat{\theta}$ then implies the same for $\Phi(\hat{\theta})$ as long as $\Phi$ is differentiable.

\begin{thm}(Delta-method)
	
	Suppose $\Phi : \Theta \to \R$ is continuously differentiable at $\theta \in \Theta$ with gradient vector $\frac{\partial\Phi}{\partial\theta}(\theta)$. Suppose further $\hat{\theta}_n$ are random vectors in $\Theta$ s.t. $\sqrt{n}(\hat{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} Z $, where $Z$ is some random vector in  $\R^{p}$. Then $\sqrt{n}(\Phi(\hat{\theta}) - \Phi(\theta)) \underset{n\to \infty}{\to ^{d}} \frac{\partial\Phi}{\partial\theta}(\theta)^{T} Z$
\end{thm}

\begin{proof}
	By the mean value theorem applied to $\Phi$ on the line segment $\{t \hat{\theta} + (1-t)\theta : 0<t<1\} $ we can write for mean value $\overline{\theta}$
	\begin{align*}
		\sqrt{n}(\Phi(\hat{\theta}_n) - \Phi(\theta)) = \frac{\partial\Phi}{\partial\theta}(\overline{\theta}_n)^{T}(\hat{\theta}_n - \theta)
	\end{align*}

	Since $\sqrt{n}(\hat{\theta} - \theta) \to ^{d} Z $, we have in particular $\hat{\theta} \underset{n\to \infty}{\to ^{P}} \theta$ (by stochastic boundedness), so also $\overline{\theta} \underset{n\to \infty}{\to ^{P}} \theta$, and hence by the continuous mapping theorem, we also have $\frac{\partial\Phi}{\partial\theta}(\overline{\theta}_n) \underset{n\to \infty}{\to ^{P}} \frac{\partial\Phi}{\partial\theta} (\theta)$. Hence by Slutsky's Lemma, $\sqrt{n}(\Phi(\hat{\theta}) - \Phi(\theta)) \underset{n\to \infty}{\to ^{d}} \frac{\partial\Phi}{\partial\theta}(\theta)^{T} Z$ 
	
\end{proof}

\begin{remark}
	If $\sqrt{n}( \hat{\theta}_{MLE} - \theta) \to ^{d} N(0, I(\theta)^{-1})$, then what precedes implies that the plug-in MLE satisfies $\sqrt{n} (\Phi(\hat{\theta}_{MLE}) - \Phi(\theta)) \to ^{d} N\left(0, \frac{\partial\Phi}{\partial\theta}(\theta)^{T} I(\theta)^{-1} \frac{\partial\Phi}{\partial\theta}(\theta)\right)$, in particular the asymptotic covariance attains the CRLB for estimating $\Phi(\theta)$
	
\end{remark}

\subsection{Asymptotic inference with the MLE}

Suppose we want to make inference on $\theta_i$, the $i^{th}$ component of $\theta \in \R^{p}$, from a regular statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$. Then $\theta_i = e_i ^{T} \theta$, and by the last theorem,
\begin{align*}
	\sqrt{n} (\hat{\theta}_i - \theta_i) = \sqrt{n} e_i ^{T} (\hat{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0, e_i ^{T} I(\theta)^{-1} e_i) = N(0, I(\theta)^{-1}_{ii})  
\end{align*}

This suggests an asymptotic confidence interval (CI) $C_n = \left\{v \in \R : |\hat{\theta}_i - v| \le  \frac{(I(\theta)^{-1}_{ii})^{\frac{1}{2}}}{\sqrt{n} } z_{\alpha}\right\} $

Indeed by the continuous mapping theorem,

\begin{align*}
	P\left( \theta_j \in C_n \right) &= P\left( \sqrt{n} (I(\theta)^{-1})^{- \frac{1}{2}}_{jj}  |\hat{\theta}_{n,j} - \theta_j| \le  z_{\alpha}\right) \\
	&\underset{n\to \infty}{\to } P\left( |Z| \le z_{\alpha} \right) \\
	&= 1- \alpha
\end{align*}

So $C_n$ is a confidence interval of asymptotic level $1-\alpha$.

In practice,  $I(\theta)$ may still depend on  $\theta$ and hence needs to be replaced by a consistent estimate $\hat{i}_{n} \underset{n\to \infty}{\to ^{P}}  I(\theta)$ (in which case, by Slutsky's lemma, the new CI again has asymptotic coverage level $1-\alpha$).

 \begin{defn}
	The $p\times p$ matrix
	\[
		i_n(\theta) = \frac{1}{n}\sum_{i=1}^{n} \frac{\partial }{\partial \theta} \log f(X_i, \theta) \frac{\partial }{\partial \theta} \log f(X_i, \theta)^{T} 
	\] 
	is called the observed Fisher information (at $\theta$).
	We then define $\hat{i}_n = i_n(\hat{\theta}_{MLE})$, an estimator of $I(\theta_0)$.
\end{defn}

\begin{prop}
	Suppose the statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies Assumption B. Then $\hat{i}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0) $.
\end{prop}

\begin{proof}
	Just as when proving $\overline{A}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0)$ in the proof of asymptotic normality of $\hat{\theta}_{MLE}$,  replacing $\frac{\partial^2 }{\partial \theta \partial\theta^{T}}, \overline{\theta}_{ij} $ by $\frac{\partial }{\partial \theta} $ and $\hat{\theta}_{MLE}$ respectively.
\end{proof}

\begin{remark}
	The continuous mapping theorem and invertability of $I(\theta_0)$ then also imply that $\hat{i}_n^{-1} \underset{n\to \infty}{\to ^{P}} I(\theta_0)^{-1}$, since $A \mapsto A^{-1} $ is continuous on $\{A : det A \neq 0\} $.  
\end{remark}

Alternatively, one uses $j_n(\theta) = - \frac{1}{n} \sum_{i=1}^{n} \frac{\partial ^2}{\partial \theta\partial\theta^{T}} \log f(X_i, \theta) $, and estimates $I_n(\theta_0)$ by \[\hat{j}_n = j_n(\hat{\theta}_{MLE}),\] which as before (and by Observation 3 from information geometry) satisfies again $\hat{j}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0)$.

To make inference on the entire parameter $\theta \in  \Theta \subseteq \R^{p}$, one can use the Wald-Statistic
\begin{align*}
	W_n(\theta) = n (\hat{\theta}_n - \theta)^{T} \hat{i}_n (\hat{\theta}_n - \theta), \theta \in \Theta
,\end{align*}
with $\hat{i}_n$ possibly replaced by $i_n(\theta)$. One shows (Ex sheet), that under $P_{\theta}$ then
 \[
	 W_n(\theta) \underset{n\to \infty}{\to ^{d}} \chi^2_{p} \text{ (Chi-squared with p degrees of freedom)}
,\] 
and this entails that the confidence ellipsoid $C_n = \{\theta \in \R^{p} : W_n(\theta) \le \xi_{\alpha}\} $ has asymptotic coverage $\lim_{n\to \infty} P_{\theta}(\theta \in C_n) = 1- \alpha$ if $\xi_{\alpha}$ are the $1- \alpha$ quantiles of the  $\chi^2_{p}$ distribution.

Consider next a hypothesis testing problem
\begin{align*}
	H_0 : \theta \in \Theta_0 \subset \Theta \text{ vs } H_1 : \theta \in \Theta\setminus\Theta_0
\end{align*}
We wish to construct a test $\Psi_n = \Psi(X_1, \ldots, X_n)$ which takes value $0$ to indicate $H_0$ is true, and takes value $1$ otherwise (to indicate $H_1$ is true). The type-I error of any such test is, for $\theta \in \Theta_0$,

\begin{align*}
	P_{\theta}(\text{reject } H_0) = E_{\theta} \Psi_n
,\end{align*}
and the type-II error, for $\theta \in \Theta\setminus\Theta_0$ is
\begin{align*}
	P_{\theta}(\text{accept } H_0) = E_{\theta}(1-\Psi_n)
\end{align*}

\begin{defn}
A general purpose test can be constructed from the Likelihood ratio test statistic
\begin{align*}
	\Lambda_n(\Theta,\Theta_0) &= 2\log \frac{\prod_{i_=1}^{n} f(X_i, \hat{\theta}_{MLE})}{\prod_{i=1}^{n}f(X_i, \hat{\theta}_{MLE})} \\
	&= 2\log \frac{\sup_{\theta \in \Theta} \prod_{i_=1}^{n} f(X_i, \hat{\theta})}{\sup_{\theta \in \Theta_0} \prod_{i_=1}^{n} f(X_i, \hat{\theta})}
\end{align*}
\end{defn}

\begin{thm}[Wilks']
	In a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfying Assumption B, and for $\Theta_0 = \{\theta_0\} $, for $\theta_0 \in \Theta$, we have under $P_{\theta_0}$,
	\[
		\Lambda_{n}(\Theta, \Theta_0) \underset{n\to \infty}{\to ^{d}} \chi^2_{p},p = \dim\Theta
	\] 
	
\end{thm}

\begin{remark}
	\begin{enumerate}
		\item One can show more generally that for $\dim(\Theta_0) = p_0 > 0$ but $p_0 < p$, we have
	\[
		\Lambda_{n}(\Theta, \Theta_0) \underset{n\to \infty}{\to ^{d}} \chi^2_{p-p_0} \text{ under  } P_{\theta}, \theta \in \Theta_0
	\]
\item We can construct a test $\Psi_{n} = 1_{\{ \Lambda_n (\Theta, \Theta_0) > \xi_{\alpha}\} }$ for $H_0$, where type-I errors are controlled at asymptotic level $\alpha$ if $\xi_{\alpha}$ are the $\alpha$-quantiles of  $\chi^2_{p-p_0}$-distribution.
	\end{enumerate}
\end{remark}

\begin{proof}
	We restrict to events $\hat{\theta}_n \in\intr \Theta$ (of probability approaching $1$). Then since $\hat{\theta}_{n,0} = \theta_0$, we can write 
	\begin{align*}
		\Lambda_n(\Theta,\Theta_0) &= 2l_n(\hat{\theta}_n) - 2\log(\theta_0) \\
		&= (-2l_n(\theta_0)) -(-2l_n(\hat{\theta}_n)) \\
		&= -2 \frac{\partial }{\partial \theta} l_n(\hat{\theta}_n) - \frac{2}{2} (\theta_0 - \hat{\theta}_n)^{T} \frac{\partial ^2}{\partial \theta\partial\theta} l_n(\overline{\theta}_n)(\theta_0 - \hat{\theta}_n) \text{ (Taylor expansion)} \\ 
		&= 0 - (\theta_0 - \hat{\theta}_n)^{T} \frac{\partial ^2}{\partial \theta\partial\theta} l_n(\overline{\theta}_n)(\theta_0 - \hat{\theta}_n) \text{ as $\hat{\theta}_n \in\intr \Theta$, as the gradient at maximiser must vanish}
	,\end{align*}
	where $\overline{\theta}_n$ are mean values lying on the line segment connecting $\hat{\theta}_n, \theta_0$. The second order term can be written as
\[
	\sqrt{n}(\hat{\theta}_n - \theta_0)(j_n(\theta) - I(\theta_0))\sqrt{n}(\hat{\theta}_n - \theta_0) + \sqrt{n}(\hat{\theta}_n - \theta_0)(I(\theta_0))\sqrt{n}(\hat{\theta}_n - \theta_0) \underset{n\to \infty}{\to ^{d}} Z^{T}I(\theta_0)Z
		\]
by liberal usage of Slutsky's lemma, the previous proposition, and the continuous mapping theorem for the map $x \mapsto x^{T}I(\theta_0)x$ from $\R^{p} \to \R$. Moreover, by standard linear algebra, $Z^{T}I_{\theta_0}Z = \sum_{i=1}^{p} W_{i}^2$, $W_i \stackrel{iid}{\sim} N$, so $\sim \chi^2_{p}$ 
\end{proof}

\section{Bayesian Inference}

For a given statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}, \Theta \subseteq \R^{p}$, we will now regard $\theta$ as drawn at random from some \textit{prior} distribution $\pi$ on $\Theta$. This may

\begin{enumerate}[label=\roman*)]
	\item Model an intrinsically random state of nature $\Theta$.
	\item Model subjective beliefs about the state of nature $\Theta$.
	\item Serve as a way to generate statistical decision rules/estimators in our inference problem.
\end{enumerate}

\begin{eg}
	Consider a countable set of (scientific) hypotheses $H_i, i \in \Theta$, about the state of nature, each of prior probability $\pi_i, \sum_{i \in \Theta} \pi = 1$, and such that
	\[
		P\left( X=x \mid H_i \right) = f_i(x)
	,\]
	where $x$ is a random outcome that can be measured. Then by the Bayes rule for conditional probabilities,
	\[
		P\left( H_i  \mid  X = x \right) = \frac{f_i(x) \pi_i}{\sum_{j\in \Theta} f_j(x) \pi_j}
	\]

	To check whether $H_i$ is more likely than $H_j$ given  $X=x$, we compare
	 \[
		 \frac{P\left( H_i  \mid X=x \right)}{P\left( H_j  \mid X=x \right)} = \frac{f_i(x)\pi_i}{f_j(x)\pi_j}
	\]

	If all $\pi_i$ agree ($\Theta$ is finite), then this just reduces to the likelihood ratio test. In a general setting, $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, we wish to model the observation $X \mid \theta \sim f(\cdot , \theta)$ and $\theta \sim \pi$ on $\Theta$, where $\pi$ is the prior distribution. The \textit{posterior distribution} is then the conditional distribution of $\theta  \mid X$. To make this rigorous, consider a sample space $\chi \subseteq \R^{d}$ supporting $\{f\left( \cdot , \theta \right) : \theta \in \Theta \subseteq \R^{p}\}$, and on the product space $\chi \times \Theta \left( \subseteq \R^{d} \times \R^{p} \right) $ consider a probability distribution $Q$ with pmf $f$
	 \[
		 dQ(x,\theta) = f(x,\theta)\pi(\theta)dxd\theta
	 .\]

	 By the usual rules for conditional densities, if $(X,\Theta) \sim Q$, then
	 \[
		 X | \Theta \sim \frac{f(x,\theta)\pi(\theta)}{\int_{\chi}f(x,\theta)\pi(\theta)dx} = f(x,\theta)
	 \]

	 Likewise,
	 \[
		 \theta  | X \sim \frac{f(x,\theta)\pi(\theta)}{\int_{\Theta} f(x,\theta) \pi(\theta) d\theta} = \pi(\theta \mid X)
	 \]

	 is the pdf/pmf of the posterior distribution. If $X_1, \ldots, X_n$ are i.i.d copies of $X \mid \theta$ then the same argument gives that the posterior distribution is given by  
	 \[
		 \theta|(X_1, \ldots, X_n) \sim \frac{\prod_{i=1}^{n}f(X_i, \theta) \pi(\theta)}{\int_{\Theta} \prod_{i=1}^{n} f(X_i, \theta) \pi(\theta) \pi(\theta) d\theta} = \pi(\theta|X_1, \ldots, X_n)
	 \] 
	 
\end{eg}

\begin{eg}
	Consider a $N(\theta, 1)$ model with prior $\pi\sim N(0,1)$ on $\Theta = \R $. Given $X_1, \ldots, X_n$ i.i.d copies of $X|\theta$, we see that
	\begin{align*}
		\pi(\theta |X_1, \ldots, X_n) &\overset{\text{in $\theta$}}{\propto} e^{\frac{1}{2}\sum_{i=1}^{n}(X_i - \theta)^2}e^{-\frac{\theta^2}{2}} \\
		&= e^{-\frac{1}{2} \sum_{i=1}^{n}X_i^2 + \sum_{i=1}^{n}X_i \theta - \frac{n\theta^2}{2} - \frac{\theta^2}{2}} \\
		&\propto e^{n\overline{X}\theta - \frac{n+1}{2}\theta^2} \\
		&\propto e^{-\frac{1}{2} \left( \frac{n}{\sqrt{n+1}} \overline{X} - \sqrt{n+1} \theta  \right)^2 } \\
		= e^{-\frac{n+1}{2}\left(\frac{n}{n+1}\overline{X} - \theta  \right)^2 }
	 \end{align*}
	 And so $\pi(\theta|X_1, \ldots, X_n) \sim N(\frac{1}{n+1}\sum_{i=1}^{n}X_i, \frac{1}{n+1})$.
\end{eg}

One shows more generally that for normal prior and normal 'sampling' models $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, the posterior distribution is again a normal distribution. This is an example of a \textit{conjugate prior} where the posterior distribution after sampling belongs to the same family of probability distributions.

\begin{eg} We have some other examples of conjugate priors.

	\begin{enumerate}[label=\roman*)]
		\item Beta prior $+$ Binomial sampling $\to $ Beta posterior
		\item Gamma prior $+$ Poisson sampling $\to $ Gamma posterior
	\end{enumerate}
\end{eg}

Even when $\pi$ is not a proper probability distribution, the expression
\[
	\pi(\theta | X_1, \ldots, X_n) = \frac{\prod_{i=1}^{n}f(X_i, \theta) \pi(\theta)}{ \int_{\Theta} \prod_{i=1}^{n}f(X_i, \theta) \pi(\theta) d\theta}
\]
may still be well defined, in which case we speak of a posterior distribution arising from an 'improper' prior. Specifically, the family of 'Jeffrey's' priors which are such that $\pi(\theta) \propto \sqrt{\det(I(\theta))} $ often fall into this class. For instance, for the $N(\theta, \sigma^2)$ model (with $\sigma^2$ known), one sees that the Jeffrey's prior is proportionally constant, and one shows that the 'improper' posterior is equal to a $N(\overline{X}_n, \frac{\sigma^2}{n}) = N(\hat{\theta}_{MLE}, \frac{\sigma^2}{n})$ distribution (see ex. sheet). Note however (see ex. sheet) that uniform priors do not necessarily return 'Bayes estimators' that coincide with MLEs, as the $\text{Bin}(n,p)$ model with $p\sim U(0,1)$ prior shows.

\subsection{Statistical Inference with Posterior Distributions}

The posterior distribution $\pi(\cdot, X_1, \ldots, X_n)$ is a (random) probability distribution on $\Theta$, and hence can be used in principle to construct inference procedures for $\theta$.
\begin{enumerate}[label=\roman*)]
	\item Estimation - One may use the posterior mean $\E^{\pi} [\theta | X_1, \ldots, X_n]$ as an estimator $\overline{\theta}_n = \overline{\theta}(X_1, \ldots, X_n)$ for $\theta$, or alternatively (when appropriately  defined) the posterior mode or median.
	\item Uncertainty quantification - Any subset $C_n \subseteq \Theta$ for which $\pi(C_n|X_1, \ldots, X_n) = 1-\alpha$ is a level $1-\alpha$ \textit{credible} set (but it has, a fortiori, no interpretation in terms of coverage probabilities $P_{\theta} (\theta  \in C_n))$ 
	\item Hypothesis testing - Given $\Theta_0, \Theta_1 \subseteq \Theta$, we can compute Bayes-factors
		\[
			\frac{\pi(\Theta | X_1, \ldots, X_n)}{\pi(\Theta_1|X_1, \ldots, X_n)} = \frac{\int_{\Theta_0} \prod_{i=1}^{n} f(X_i, \theta)\pi(\theta)d\theta}{\int_{\Theta_1} \prod_{i=1}^{n}f(X_i, \theta) \pi(\theta) d\theta} = \frac{P\left( X_1, \ldots, X_n | \Theta_0 \right)}{P\left( X_1, \ldots, X_n| \Theta_1 \right)}
		\]
		So we may 'test' for (choose to prefer) $H_0$ based on $\phi_n = 1_{\{\text{Bayes factor} <1\}}$
\end{enumerate}

\subsection{Frequentist Analysis of Bayes Methods}

Bayesian inference procedures $\overline{\theta}(X_1, \ldots, X_n), C(X_1, \ldots, X_n), \Psi(X_1, \ldots, X_n)$ can be analysed as statistical algorithms in their own right under the \textit{frequentist} sampling assumption that $X_i \stackrel{iid}{\sim} \{f(\cdot, \theta_0), \theta_0 \in \Theta\}$.

\begin{eg}
	$X_1, \ldots, X_n \stackrel{iid}{\sim}$ copies of $X|\theta \sim N(\theta, 1)$ with $\theta \sim N(0, 1)$ prior. Then the posterior is
	 \[
		 \theta | X_1, \ldots, X_n \sim N\left( \frac{1}{n+1} \sum_{i=1}^{n}X_i, \frac{1}{n+1} \right) 
	\]
\end{eg}

One shows easily that $\overline{\theta}_n = \frac{1}{n+1} \sum_{i=1}^{n} X_i \to^{a.s} \theta_0 $ under $P_{\theta_0}^{\N}$, and also that $\sqrt{n}(\overline{\theta}_n - \theta) \to ^{d} N(0, I(\theta_0)^{-1})$ under $P_{\theta_0}^{\N}$. To corroborate Bayesian credible sets, however, more is required, as these are based not on the 'limit distribution' $N(0, I(\theta_0)^{-1})$, but on $\pi(\cdot , X_1, \ldots, X_n)$.

\begin{thm}(Bernstein-von Misses)
	
	Suppose a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies assumption B, and let the prior have a continuous and positive density $\pi$ near $\theta_0$. Denote by $\pi_n = \pi(\cdot , X_1, \ldots, X_n)$, and let $\hat{\phi}_n$ be the pdf of a $N(\hat{\theta}_{MLE}, \frac{1}{n}I(\theta_0)^{-1})$. Then
	\[
		\|\pi_n - \hat{\phi}_n\|_{L^{1}} = \int_{\R} \left|\pi_n - \hat{\phi}_n(\theta)\right|d\theta \underset{n\to \infty}{\to ^{a.s}} 0
	\] 
\end{thm}

\begin{proof}
	The general proof requires LeCam theory, so we only prove $X|\theta \sim N(\theta, 1)$ with  $\theta \sim N(0,1)$, in which case $I(\theta) = 1$ and $\hat{\theta}_{MLE} = \overline{X}_{n}$. Recall $\pi_n$ is the pdf of a $N(\overline{\theta}$ distribution where $ \overline{\theta} = \frac{n}{n+1}$, and so
	\[
		\sqrt{n} (\hat{\theta}_{MLE} - \theta) = - \sqrt{n} \frac{1}{n+1} (\overline{X}_n - \theta_0 + \theta_0) \underset{n\to \infty}{\to ^{P}} 0 \text{ by SLLN.}
	\] 
	Since $\int_{\R} \pi_n - \hat{\phi}_n(\theta) d\theta = 1-1 = 0$, we have 
	\begin{align*}
		\int_{\R} |\pi_n (\theta) - \hat{\phi}_n(\theta) d\theta &= 2\int_{\R} \left( \pi_n(\theta) - \hat{\phi}(\theta) \right)^{+} d\theta  \\
		&= 2\int_{\R} \left( 1 - \frac{\pi_n(\theta)}{\hat{\phi}(\theta)} \right) ^{+} \hat{\phi}(\theta) d\theta \\
		&= 2\int_{\R} \left( 1 - \frac{\sqrt{\frac{n+1}{2\pi}} \exp\left(-\frac{n+1}{2}(\theta - \hat{\theta} + \hat{\theta} - \hat{\theta})^2\right) }{\sqrt{\frac{n}{2\pi}} \exp(-\frac{n}{2}(\theta - \overline{\theta})^2) } \right)^{+} \sqrt{\frac{n}{2\pi}} \exp\left( -\frac{n}{2}(\theta - \hat{\theta})^2 \right) d\theta  \\
		&= 2\int_{\R} \left( 1 - \sqrt{\frac{n+1}{n}} \frac{\exp\left( - \frac{n+1}{2n} (v + \sqrt{n} (\hat{\theta} - \theta))^2) \right) }{\exp(-\frac{1}{2}v^2) } \right)^{+} \frac{1}{\sqrt{2\pi} } e^{-\frac{v^2}{2}} dv
	\end{align*}
	Where we substituted $v = \sqrt{n} (\theta - \hat{\theta})$. Fixing  $\omega \in \Omega_0 \subseteq \Omega$ such that $P\left( \Omega_0 \right) = 1$, so $\sqrt{n} (\hat{\theta}(\omega) - \overline{\theta}(\omega) \underset{n\to \infty}{\to } 0 $ (as scalars), note that for $Z\ge 0$, $(1-Z)^{+} \in [0,1]$, so that by integrability of $e^{-\frac{v^2}{2}}$ on $\R$, an application of the dominated convergence theorem implies that the whole last integral tends to $0$ $\forall \omega \in \Omega_0$. Since $P\left( \Omega_0 \right) = 1$, the limit holds almost surely.
\end{proof}

The last theorem remains true when $\hat{\theta}_{MLE}$ is replaced by any estimator $\overline{\theta}_n$ s.t.
\[
	\sqrt{n} (\hat{\theta}_{MLE} - \overline{\theta}_n) \underset{n\to \infty}{\to } 0 \;P_{\theta_0}^{\N}\text{-a.s}
,\] typically permitting the alternative centring at $\overline{\theta} = \E^{\pi} \theta(X_1, \ldots, X_n)$. One important consequence of the BvM-theorem is that certain posterior \textit{credible sets} are in fact proper (asymptotic) frequentist confidence sets.

For instance, consider $C_n = \{\theta : |\theta - \hat{\theta}_{MLE}| \le \frac{R_n}{\sqrt{n} }\} $, where $R_n$ are random quantile constants chosen s.t $\prod (C_n | X_1,\ldots,X_n) = 1- \alpha, 0 < \alpha < 1$. Recall $\hat{\phi}_n$ was the pdf of $N(\hat{\theta}_{MLE}, \frac{1}{n}I(\theta_0)^{-1})$distribution, and defined further $\phi_0$ to be the pdf of $Z\sim N(0, I(\theta)^{-1})$. We can define $\Phi(t) = P\left( |Z| \le t \right) = \int_{-t}^{t} \phi_0 (v) dv$ , which is strictly increasing in t, and also continuously differentiable, hence admits a continuous inverse $\Phi ^{-1} : [0,1] \to \R$.

Now, we can write $\Phi (R_n) = \int_{-R_n}^{R_n} \phi_0(v)dv$, and substituting $v = \sqrt{n} (\theta - \hat{\theta})$ so that $-R_n \le v \le R_n$ becomes the set $C_n$ and since $v ~ N(0, I(\theta_0)^{-1}) \iff \sqrt{n} (\theta - \hat{\theta}) \sim N(\hat{\theta}, \frac{1}{n}I(\theta_0)^{-1})$, the last integral gives

\begin{align*}
	\Phi(R_n) &= \int_{C_n} \hat{\phi}_n (\theta) d\theta \\
	&= \int_{C_n} (\hat{\phi}_n(\theta) - \pi_n(\theta)) d\theta + \int_{C_n} \pi_n(\theta) \\
	&= \pi (C_n |X_1, \ldots, X_n) \text{ by BvM theorem} \\
	= 1 - \alpha \; \forall n
\end{align*} 

So by the BvM theorem we know $\Phi(R_n) \underset{n\to \infty}{\to } 1 - \alpha$ a.s under $P_{\theta_0}$, hence applying the continuous mapping theorem to $\Phi ^{-1}$, we deduce

\[
	R_n = \Phi ^{-1}(\Phi(R_n)) \underset{n\to \infty}{\to ^{a.s}} \Phi ^{-1}(1-\alpha) \text{ under } P_{\theta_0}.
\] 
In particular, by Slutsky's lemma and asymptotic normality of $\hat{\theta}_{MLE}$, we have 

\[
	\frac{\Phi ^{-1} (1- \alpha)}{R_n}\sqrt{n} (\hat{\theta}_{MLE} - \theta_0) \underset{n\to \infty}{\to ^{d}} N(0, I(\theta_0)^{-1}) \text{ under } P_{\theta_0}
\] 

Now,
\begin{align*}
	P_{\theta_0}^{\N} (\theta_0 \in C_n) &= P_{\theta_0}^{\N} \left( |\theta_0 - \hat{\theta}_{MLE} | \le \frac{R_n}{\sqrt{n} }\right) \\
	&= P_{\theta_0}^{\N} \left( \frac{\Phi ^{-1} (1-\alpha)}{R_n} \sqrt{n} |\hat{\theta}_{MLE} - \theta_0| \le \Phi ^{-1}(1-\alpha)  \right)  \\
	&\underset{n\to \infty}{\to} P\left( |Z| \le \Phi ^{-1} (1- \alpha) \right) \text{ by $(\dagger)$ and continuous mapping theorem on $|\cdot |$ }\\
	&= \Phi (\Phi ^{-1} (1-\alpha)) \\
	&= 1 - \alpha
\end{align*}

Replacing $|\cdot |$ by $\|\cdot \|$, the argument extends to parameter spaces $\Theta \subseteq \R^{p}$.

We conclude that credible sets computed for priors $\pi$ with positive continuous density functions on $\Theta$ give rise to asymptotically exact level $1- \alpha$ (frequentist) confidence regions. (A version for discrete priors can be proved as well)

\section{Decision Theory}

Consider a (single) observation $X$ from some statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ of pdf/pmfs on $\chi \subseteq \R^{d}$. In a decision we consider decision maps $\delta : \chi \to \mathcal{A}$, where $\mathcal{A}$ is some 'action space'

\begin{eg}
	\begin{enumerate}[label=\roman*)]
		\item $\mathcal{A} = \{0,1\} $, a binary decision problem, where $\delta(X) \in \{0,1\} $ will be a test function, or more generally a finite decision problem $\mathcal{A} = \{1,\ldots,M\} $
		\item $\mathcal{A} = \Theta$, and decision rules are estimators $\hat{\Theta}(X) \in  \Theta$
		\item $\mathcal{A} = \{ \text{All (meas) subsets of } \Theta\} $, set-valued estimation, decision rule $\delta(X) = C(X)$ are confidence regions.
	\end{enumerate}
\end{eg}

For general decision problems, we consider \textit{loss functions}
\[
	L: \mathcal{A} \times  \Theta \to [0, \infty)
\] 
measuring the error $L(\delta(X), \theta)$ incurred by  $\delta(X)$ for observation $X$ and parameter value $\theta$.

\begin{eg}

	\begin{enumerate}[label=\roman*)]
		\item (Testing) $L(a,\theta) = 1_{\{a \neq  a(\theta)\} }$, where $a(\theta)$ is the index in $\mathcal{A}$ corresponding to $\theta$.
		\item (Estimation) $\Theta \subset \R$, absolute loss $L(a,\theta) = |a-\theta|$, squared loss $L(a,\theta) = (a-\theta)^2$
	\end{enumerate}
\end{eg}

\begin{defn}
	The risk of a decision rule $\delta(X)$ from $X \sim P_{\theta}$, for loss $L$, is defined as 
	\[
		R(\delta, \theta) = \E_{\theta} L(\delta(X),\theta) = \int_{\chi} L(\delta(x),\theta) f(x,\theta)dx
	\] 
\end{defn}

\begin{eg}
	For squared loss, in estimation problems with estimator $\overline{\theta}(X) = \delta(X)$,
	\[
		R(\delta,\theta) = \E_{\theta} (\overline{\theta} - \theta)^2
	\]
	I.e our risk is the mean squared error (MSE) (quadratic risk)
\end{eg}

\begin{defn}
	Given a prior $\pi$ on $\Theta$, the $\pi$-Bayes risk in a decision rule $\delta$ is 
	\[
		R_{\pi}(\delta) = \int_{\Theta} R(\delta, \theta) \pi(\theta) d\theta
	\]
	A $\pi$-Bayes decision rule $\delta_{\pi}$ is any decision rule for which $R_{\pi}(\delta)$ is minimised in $\delta$
\end{defn}

\newpage

We can rewrite, interchanging the order of integration,

\begin{align*}
	R_{\pi}(\delta) &= \int_{\chi} \int_{\Theta} L(\delta(x), \theta) \frac{f(x, \theta) \pi(\theta)}{\int_{\Theta} f(x, v) \pi(v) dv} \left( \int_{\Theta} f(x, v) \pi(v) dv \right) d\theta dx \\
	&:= \int_{\chi} \int_{\Theta} L(\delta(x), \theta) \pi(\theta |x) m(x) d\theta dx \text{ where } m\ge 0 \\
	&= \int_{\chi} \E^{\pi}\left[ L(\delta(x), \theta)  | x \right] m(x) dx \\
\end{align*}
where $\E^{\pi}\left[ L(\delta(x), \theta)  | x \right]$ is the posterior risk.

We conclude that any decision rule $\overline{\delta}(X)$ which minimises the posterior risk in the sense that $\forall X, \delta$,
\[
	\E^{\pi}\left[ L(\overline{\delta}(X), \theta)  | X \right] \le \E^{\pi}\left[ L(\delta(X), \theta)  | X \right]
,\]
then this inequality can be $m(x)dx$-integrated to deduce that $\overline{\delta}(X)$ is a Bayes rule $\delta_{\pi}$ minimising the $\pi$-Bayes risk.

\begin{remark}
	One shows (ex. sheet) that for squared loss, the unique $\pi$-Bayes rule $\delta_{\pi}(X)$ equals the posterior mean $\E^{\pi}[\theta | X]$, and this is the unique Bayes rule. For absolute loss, the $\pi$-Bayes rule will be the posterior median $(p=1)$
\end{remark}

\begin{prop}
	In an estimation problem, suppose a decision rule $\delta(X)$ is unbiased for $\Theta$, i.e $\E_{\theta} \delta(X) = \theta \text{ } \forall \theta \in \Theta$. Assume further that $\delta$ is a $\pi$-Bayes rule for some prior $\pi$ on $\Theta$ with squared loss. Then, where $Q$ has density on $\chi \times \Theta$ given by $dQ(x, \theta) = f(x, \theta) \pi(\theta) dx d\theta$,
	\begin{align*}
		\E_Q [\delta(X) - \theta]^2 = \int_{\chi} \int_{\Theta} (\delta(x) - \theta)^2 f(x, \theta) \pi(\theta) d\theta dx = 0
	\end{align*}
	[It is sometimes said that $\delta(X) =  \theta$ almost surely under Q]
\end{prop}

\begin{proof}
	Recall the 'tower property' of iterated expectations

	\begin{align*}
		\E[Z(X,\theta)] &= \E [\E^{\pi} [|(X,\theta)|X]] \\
				&\overset{\text{or}}{=} \E [\E_{\theta} [Z(X,\theta)]]
	\end{align*}
	
	Moreover, by the previous remark, $\delta(X) = \delta_{\pi}(X) = \E^{\pi}[\theta|X]$ (by uniqueness). Thus
	\begin{align*}
		\E [\delta(X) \theta] = \E [ \E^{\pi} [\theta |X] \delta(X) ] = \E [\delta ^2 (X)]
	\end{align*}
	and likewise
	\[
		\E [\delta(X) \theta] = \E [ E_{\theta} \delta (X) \theta ] = E[\theta ^2]
	\] 
	Now, 
	\begin{align*}
	\E[\delta(X) - \theta]^2 &= \E[\delta^2(X)] - 2 E[\theta \delta(X)] + \E[\theta ^2] \\
				 &= 0
	\end{align*}
\end{proof}

From what precedes, unbiased estimators are typically \textit{not} $\pi$-Bayes rules for any prior $\pi$.

\begin{eg}
	$\overline{X}_n = \hat{\theta}_{MLE}$ in $N(\theta, 1), \theta \in \Theta = \R$ is \textit{not} a Bayes rule for any prior in quadratic risk.
\end{eg}

\begin{eg}
	$\frac{X}{n}$ in a $Bin(n,\theta), \theta \in \Theta =[0,1]$ is a $\pi$-Bayes rule for quadratic risk only for degenerate pairs.
\end{eg}

\subsection{Minimax Risk}

\begin{defn}
	A decision rule $\delta(X)$ in a decision problem is called \textit{minimax} if it attains the \textit{minimax risk} (for loss $L$)
	\[
		\inf_{\delta(X)} \sup_{\theta \in \Theta} R(\delta, \theta)
	\]
	where $R_{m}(\delta, \Theta) = \sup_{\theta \in \Theta}$ is the maximal/worst case risk.
\end{defn}

Clearly, the Bayes risk for any prior is dominated by the minimax risk, since
\[
	R_{\pi}(\delta) = \int_{\Theta} R(\delta,\theta) \pi(\theta) d\theta \le R_{m}(\delta, \Theta) \int_{\Theta} \pi(\theta) d\theta = R_{m}(\delta, \Theta)
\] 

A prior $\lambda$ on $\Theta$ is called least favourable (for a decision problem) if
\[
	R_{\lambda}(\delta_{\lambda}) \ge R_{\lambda'}(\delta_{\lambda'}) \text{ } \forall \lambda' \text{ priors on } \Theta
\] 

\begin{prop}
	In a decision problem, suppose for some prior $\pi$ on $\Theta$, we have that 
	\begin{align*}
		R_{m}(\delta_{\pi}) = \sup_{\theta \in  \Theta} R(\delta_{\pi}, \theta)
	\end{align*}
	(the $\pi$-Bayes risk of $\delta_{\pi}$ coincides with the worst case risk of $\delta_{\pi}$).
	Then,

	\begin{enumerate}[label=\roman*)]
		\item $\delta_{\pi}$ is minimax.

		\item If $\delta_{\pi}$ is the unique Bayes rule, then $\delta_{\pi}$ is the unique minimax.

		\item $\pi$ is least favourable.
		
	\end{enumerate}
\end{prop}

\begin{corol}
	If $\delta_{\pi}$ has constant risk in $\theta$, then it is minimax, and if $\delta_{\pi}$ is unique, then it is unique minimax.
\end{corol}

\begin{proof}
	\begin{enumerate}[label=\roman*)]
		\item Let $\delta$ be any decision rule with maximal risk 
			\begin{align*}
				sup_{\theta\in \Theta} R(\delta,\theta) &\ge  \int_{\Theta} R(\delta,\theta)\pi(\theta)d\theta \\
				&= R\pi(\delta) \\
				&\ge R\pi(\delta_{\pi}) \\
				&= \sup_{\theta \in \Theta} R(\delta_{\pi},\theta) \text{ by assumption}
			\end{align*}

			So, taking inf over all $\delta$ we see
			\[
				\inf_{\delta} \sup_{\theta \in  \Theta} R(\delta,\theta) \ge  \sup_{\theta \in  \Theta} R(\delta_{\pi},\theta)
			,\]
			and hence $\delta_{\pi}$ attains the minimax risk.

		\item Moreover, the second preceding inequality is strict when $\delta_{\pi}$ is the unique $\pi$-Bayes rule, and if $\delta \neq \delta_{\pi}$ so that for such $\delta$ we have
			\[
				\sup_{\theta \in \Theta} R(\delta, \theta) > \sup_{\theta \in \Theta} R(\delta_{\pi},\theta)
			,\]
			so $\delta_{\pi}$ is the unique minimax.
		\item Let $\pi'$ be any prior on $\Theta$. Then
			\begin{align*}
				R_{\pi'}(\delta_{\pi'}) &\le R_{\pi'}(\delta_{\pi}) \\
				&= \int_{\Theta} R(\delta_{\pi}, \theta) \pi'(\theta)d\theta \\
				&\le \sup_{\theta \in \Theta} R(\delta_{\pi},\theta) \\
				&= R_{\pi}(\delta_{\pi}) \text{ by assumption}
			\end{align*}
			Hence $\pi$ maximises the $\pi$-risk of the $\pi$-Bayes rule among all $\pi$ 's, i.e is \textit{least favourable}.
	\end{enumerate}

	The corollary also follows, since for $\delta_{\pi}$ with risk constant in $\theta$ we must have equality.
\end{proof}

In such situations, the (unique) minimax decision rule is characterised by a (unique) $\pi$-Bayes rule corresponding to a least favourable prior $\pi$.

\begin{eg}
	In a $\text{Bin}(n,\theta) $ model with $\theta \in \Theta = [0,1]$ and quadratic risk, consider priors $\pi_{a, b}$ arising from the $\text{Beta}(a,b)$ distribution. In this case, the unique $\pi_{a, b}$-Bayes rule is given by the posterior mean $\delta_{a, b}(X) = \E^{\pi_{a,b}} [\theta |X]$, available in closed form as the mean of an 'updated' Beta distribution (ex. sheet).

	One then solves in a,b the equation
	\begin{align*}
		R_{\pi_{a,b}}(\delta_{\pi_{a,b}}, \theta) = \text{ const}
	,\end{align*}
	to obtain a unique Bayes rule $\delta_{\pi_{\overline{a},\overline{b}}}$ at constant quadratic risk. By what precedes, this gives the unique minimax rule for a $\text{Bin}(n, \theta)$ model, which is seen to be distinct from the MLE, and moreover biased. One shows further that as $n\to \infty$, the minimax risk of $\delta_{\pi_{\overline{a},\overline{b}}}$ aligns with the risk of the MLE.
\end{eg}

\begin{remark}
	In a $N(\theta, I_{p})$ model, $\theta \in \Theta = \R^{p}$, we will show later that $\overline{X}_n = \hat{\theta}_{MLE}$ is minimax, however.
\end{remark}

\subsection{Admissibility}

\begin{defn}
	In a decision problem, a decision rule $\delta$ is called \textit{inadmissible} if $\exists \delta' $ s.t $R(\delta', \theta) \le \R(\delta, \theta) \text{ } \forall \theta \in \Theta$, and $R(\delta', \theta) < R(\delta, \theta) \text{ for some } \theta$. We say that $\delta'$ dominates $\delta$.

	$\delta$ is called admissible if no such $\delta'$ exists.
\end{defn}

\begin{prop}
	\begin{enumerate}[label=\roman*)]
		\item Every unique Bayes rule is admissible

		\item If $\delta$ is admissible and has risk constant in $\theta$, then it is minimax.
	\end{enumerate}
\end{prop}

\begin{proof}
	See example sheet.
\end{proof}

\begin{remark}
	The unique minimax rule from the previous $\text{Bin}(n, \theta)$ model is thus also admissible.
\end{remark}

\begin{thm}
	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} N(\theta, \sigma^2)$, where  $\sigma^2$ is known, $\theta \in \Theta = \R$. Then $\hat{\theta}_{MLE} = \overline{X}_n$ is admissible and minimax for quadratic risk.
\end{thm}

\begin{remark}
	Admissibility extends to $p=2$, and minimaxity to any $p \in \N$, but this will not be proved.
\end{remark}

However, for $p\ge 3$ we have\ldots

\begin{thm}
	If $X \sim N_p(\theta, I_p), \theta \in \Theta = \R^{p}, p\ge 3$, then $\hat{\theta}_{MLE} = \overline{X}_n$ is inadmissible for quadratic risk. 	
\end{thm}

\begin{proof}(12.9)
	We can set $\sigma^2=1$ as the proof will show. Also, the risk of $\overline{X}_n$ is given by

	\[
		R(\overline{X}_n, \theta) = \E_{\theta} (\overline{X}_n - \theta)^2 =\frac{1}{n} \text{ which is constant in } \theta
	,\]

	hence by the previous proposition,  it suffices to prove that $\overline{X}_n$ is admissible.

	So, let $\delta$ be any other decision rule. Then
	\begin{align*}
		R(\delta,\theta) &= \E_{\theta} [(\delta - \theta)^2] \\
		&= \E_{\theta} [(\delta - \E_{\theta} \delta)^2] + (\E_{\theta} \delta - \theta)^2 + \underbrace{\E_{\theta}[\delta -\E_{\theta}\delta ]}_{=0} \cdot (\E_{\theta}\delta - \theta)\\
		&= \var_{\theta}(\delta) + B^2(\theta)
	\end{align*}
	for $B(\theta) = \E_{\theta}\delta - \theta$

	Recalling the Cramer-Rao inequality for biased estimators, we know that
	\[
	\var_{\theta}\delta(X) \ge  \frac{(\frac{d}{d\theta} \E_{\theta}\delta)^2}{nI(\theta)} = \frac{(1+B'(\theta))^2}{n}
	,\]
	by definition of $B$ and $I(\theta)$. Hence, if  $\delta $ dominates $\overline{X}_n$, then necessarily
	\[
		\frac{1}{n} = R(\overline{X}_n, \theta) \ge R(\delta, \theta) \ge B^2(\theta) + \frac{(1+B'(\theta))^2}{n} \text{ } \forall \theta\in \R
	\]

	We deduce that $|B(\theta)| \le \frac{1}{\sqrt{n} }$, in particular $B$ is bounded on $\R$. Moreover, we also have
	 \[
		 (1+B'(\theta))^2 = 1 +2B'(\theta) + (B'(\theta))^2 \le 1 
	,\]
	so $B'(\theta) \le 0 \text{ } \forall \theta \in \R$.

	$\forall \epsilon >0, i \in \N$, there must exist $\theta_i$ large enough such that $B'(\theta_i) \ge - \epsilon$, as otherwise $\forall |\theta| \ge \theta_i $ we would have $B'(\theta) \le - \epsilon$, so that by the MVT $B$ is unbounded. In other words, for these sequences $B'(\theta_i) \to 0$.

	Now, evaluating these limits in the above inequality, we see that $\lim_{i\to \infty}\left[ B^2(\theta_i) + \frac{(1+B'(\theta_i))^2}{n} \right] \le \frac{1}{n} $ gives as that $\lim_{i\to \infty} B^2(\theta_i) = 0$. Therefore by monotonicity $B(-\infty) = B(\theta) = B(\infty) = 0$, and so the bias vanishes identically.

	So,
	\[
		R(\delta, \theta) \ge \frac{1}{n} = R(\overline{X}_n, \theta) \text{ } \forall \theta \in \Theta
	\] 
\end{proof}

\begin{remark}
	\begin{enumerate}[label=\roman*)]
		\item One shows (example sheet) that $\overline{X}_n$ is not a $\pi$-Bayes rule $\delta_{\pi}$ for any prior $\pi$ on $\Theta = \R$, hence there exists and admissible minimax decision rule which is not $\pi$-Bayes for any $\pi$. One may show that $\overline{X}_n$ is a 'limiting Bayes' in the sense that is is the limit as $\nu \to \infty$ of the Bayes rule for a $N(0, \nu^2)$ prior on $\Theta$.
		\item The unboundedness of $\Theta$ in the last proof is crucial. When  $\Theta = [0,\infty)$, then $\overline{X}_n$ is inadmissible (it is still minimax, however), and when $\Theta = [a,b]$, then $\overline{X}_n$ is also no longer minimax (see example sheet).
		\item Minimaxity of $\overline{X}_n$ on $\R$ extends to $X_i \stackrel{iid}{\sim} N_p(\theta, I), \theta\in \Theta = \R^{p}$ for $p \in \N$.
		
	\end{enumerate}
\end{remark}

To prove theorem $12.10$, we first need a new estimator.

\begin{defn} (James-Stein Estimator)
	Define the \textit{James-Stein Estimator}, for $X \sim N_p(\theta, I)$ by

	 \[
		 \delta^{JS} := \begin{pmatrix} \delta_1^{JS} \\ \vdots \\ \delta_p^{JS} \end{pmatrix}, \delta_j^{JS} = \left(1 - \frac{p-2}{\|X\|_2^2}\right) X (p\ge 3)
	\] 
\end{defn}

We now show that the risk of $\delta^{JS}$ dominates the quadratic risk
\[
	R(\hat{\theta}_{MLE}, \theta) = \E_{\theta} \|X - \theta\|^2 = \E_{\theta} \sum_{j=1}^{p}(X_j - \theta_j)^2 = p
.\] 

\newpage

\begin{lemma} (Stein)

	Let $X \sim N(\theta, 1)$ and let $g: \R \to \R$ be differentiable and such that $\E_{\theta} g(X) < \infty$. Then $\forall \theta \in \R$,
	\[
		\E_{\theta} [g(X)(X-\theta)] = \E_{\theta} g'(X)
	\] 
	
\end{lemma}

\begin{proof}
	\begin{align*}
		\E_{\theta} [g(X)(X-\theta)] &= \frac{1}{\sqrt{2\pi} } \int_{\R} g(x)(x-\theta) e^{-\frac{1}{2} (x-\theta)^2} dx\\
			&= -\frac{1}{\sqrt{2\pi} } \int_{\R} g(x) \frac{d}{dx} e^{- \frac{1}{2} (x-\theta)^2} dx \\
			&= \left[ -\frac{1}{\sqrt{2\pi} } g(x) e^{- \frac{1}{2} (x-\theta)^2} \right]_{-\infty}^{\infty} + \frac{1}{\sqrt{2\pi} } \int_{\R} g'(x) e^{- \frac{1}{2} (x-\theta)^2}\\
			&= \E_{\theta} g'(X) \text{, as $g$ has finite expectation and so vanishes at infinity} 
	\end{align*}
\end{proof}

\begin{proof} (12.10)

\begin{align*}
	R(\delta^{JS}, \theta) &= \E_{\theta} \|\delta^{JS} - \theta\|^2 \\
	&= \E_{\theta} \|X - \theta - \frac{p-2}{\|X\|^2}X\|^2 \\
	&= \E_{\theta} \|X-\theta\|^2 + (p-2)^2\E_{\theta}\frac{\|X\|^2}{\|X\|^{4}} - 2(p-2) \E_{\theta} (X-\theta)^{T}\frac{X}{\|X\|^2}
\end{align*}

Now, the last expectation can be written as
\[
	\E_{\theta} \sum_{j=1}^{p} \E_{j} \left[ \frac{(X_j - \theta_j)}{\|X\|^2} \right] 
,\]
where $\E_j = \E [\cdot | X_{(-j)}]$, with $X_{(-j)} = \{X_i : -i \neq j\} $. The $j ^{th}$ expectation can be written as $\E_j [(X_j - \theta_j) g(X_j)]$, where $g(y) = \frac{y}{y^2+1}$, $a = \sum_{i\neq j} X_i^2$, which (since $P\left( a=0 \right) =0)$ is a bounded, differentiable function with 
\[
	g'(y) = \frac{y^2+a-2y^2}{(y^2+a)^2}
,\] which is also bounded on $\R$.

Hence we can apply Stein's lemma to this expectation, giving
\begin{align*}
	\E_j g'(X_j) &= \E_j \frac{X_j^2 + \sum_{i\neq j}X_i^2 - 2X_j^2}{\|X\|^4} \\
\end{align*}

So going back to our original statement,

\begin{align*}
	R(\delta^{JS}, \theta) &= \E_{\theta}\|X-\theta\|^2 + (p-2)^2\E_{\theta}\frac{1}{\|X\|^2} - 2(p-2) \E_{\theta} \sum_{j=1}^{p} \frac{ \|X\|^2 -2X_{j}^2}{\|X\|^{4}} \\
	&= \E_{\theta}\|X-\theta\|^2 + (p-2)^2\E_{\theta}\frac{1}{\|X\|^2} - 2(p-2) \E_{\theta} [\frac{p}{\|X\|^2} -\frac{2}{\|X\|^2} \\
	&= \underbrace{\E_{\theta}\|X-\theta\|^2}_{=p} - (p-2)^2\E_{\theta}\frac{1}{\|X\|^2} \\
	&<p
\end{align*}
Since
\begin{align*}
	\E_{\theta} \frac{1}{\|X\|^2} &= \int_{\R^{p}} \frac{1}{\|X\|^2} \phi(x-\theta) dx \\
	&\ge c \int_{c_0 \le \|X\| \le c_1} \phi(x-\theta)dx \\
	&\ge c \P_{\theta} (\|X\|\in (c_0, c_1)) \\
	&> 0
\end{align*}
where here $\phi$ is our $N(\theta, I_p )$ pdf.
\end{proof}

\begin{remark}
	\enumerate[label=\roman*)]
\item
	While $\delta^{JS}$ strictly dominates $\delta_{MLE} = X$, the worst case (minimax) risk
	\[
		\sup_{\theta \in \R^{p}} R(\delta^{JS}, \theta) = \sup_{\theta \in \R^{p} R(X, \theta)} \text{ (see ex sheet)}
	\] 

\item
	The James-Stein estimator itself is also inadmissible, and for instance is dominated by
	\[
		\delta^{JS, +} =  \left(1-\frac{p-2}{\|X\|^2}\right)^{+}X\text{, where } (\cdot )^{+} = \max(\cdot , 0)
	,\] known as the 'positive part JS-estimator. This is also not admissible.

\item
	Other shrinkage factors are also permitted, but among the decision rules
	\[
	\delta^{c} := \left(1-c \frac{p-2}{\|X\|^2}\right) X, \, 0<c<2
	,\] the choice $c=1$ is optimal.

\item While $\delta^{JS}$ is attractive from a decision-theoretic perspective, its use for inference (confidence regions and tests) is less clear, as its distributional properties are more involved than those of $\delta_{MLE}(X) = X \sim N(\theta, I_p)$.
\end{remark}

\newpage

\subsection{Classification Problems}

For two pdf/pmfs $f_1, f_2$ defined on $\chi$, consider observing X drawn from $f_i$ with probability $q_1$, and from $f_2$ with probability $q_2 = 1-q_1$. Given an outcome $X=x$, we wish to classify it into the correct category $i$. This can be cast as a binary decision problem with $\Theta = \{1,2\} $ and prior  $\pi$ on $\{q_1, q_2\} $.

A \textit{classification rule} $\delta = \delta_{R}$ is given by a region $R\subseteq \chi (\subseteq \R^{d})$ such that
	\[
		\delta_R(X) = \begin{cases}
			1 \text{ if } X\in R \\
			2 \text{ if } X \in R^{c}
		\end{cases} 
	\]

	The classification errors are given by 
\[
	P\left( 2| 1, R) \right) = P_{f_1}(X\in R^{c} = \int_{R^{c}} f_1(x) dx
,\] and by
\[
	P\left( 1|2, R \right) = p_{f_2}(x\in R) = \int_{R} f_2(x) dx
\] 

The $\pi$-Bayes risk now becomes
\[
	R_{\pi}(\delta_{R}) = P\left(1|2, R \right) \pi_2) + P\left( 2|1,R \right) \pi(1)
,\] where $\pi$ is a fixed 'prior' sampling the probabilities $q_1, q_2$. The Bayes factors are given by
\[
	\frac{\pi(1|X=x)}{\pi(2|X=x)} = \frac{f_1(x) q_1}{f_2(x)q_2}
,\] and the $\pi$-Bayes classifier can be shown to choose $\{1\} $ whenever $\frac{f_1(x)}{f_2(x)} > \frac{q_2}{q_1}$. 

\begin{prop}
	Suppose that for all $i$, $P_{f_i} \left( \frac{f_1(x)}{f_2(x)} = \frac{1-q}{q} \right) = 0$. Then the unique $\pi$-Bayes classification rule for prior $\pi(\{1\})=q$arises from
	\[
		R = \left\{x\in \chi : \frac{f_1(x)}{f_2(x)} > \frac{1-q}{q}\right\} 
	,\]
	and in fact $\delta_R$ is also admissible.
\end{prop}

\begin{proof}
	Let $S \subset \chi$ be any other classification region with classification risk
	\begin{align*}
		q \int_{S^{c}} f_1(x) dx + (q-1) \int_{S}f_2(x) dx &= \int_{S^{c}} qf_1(x) - (1-q)f_2(x) dx + \int_{\chi} (1-q)f_2(x) \\
	\end{align*}

	The first term is minimal when $S^{c}$ includes precisely all $x \in \chi$ s.t. the integrand
	\[ 
		qf_1(x) - (1-q)f_2(x) < 0 \iff \frac{f_1(x)}{f_2(x)} < 1-\frac{q}{q}
	\]
	(Uniqueness since $P\left( \frac{f_1(x)}{f_2(x)} = \frac{1-q}{q} \right) = 0$). Thus $S = R$ and the first claim follows. Since unique Bayes rules are admissible, the result is proved. 
\end{proof}

Similarly, one can find minimax classifiers $\delta_{R}$ by choosing $R$ s.t. 
\[
	q P\left( 2|1,R \right) + (1-q)P\left( 1|2,R \right) = \text{ const in } q \in [0,1]
\] 
For the case where $f_i \sim N_p(\mu_i, \Sigma_i)$, those classifiers can be explicitly computed in dependence of the \textit{discriminant function} $D = X^{T} \Sigma (\mu_1 - \mu_2)$ (see ex sheet).

\section{Further Topics}

\subsection{Basic Multivariate Analysis}

Consider random vectors $X, Y$, here assumed to be from $N(\mu_x, \sigma_x), N(\mu_y, \sigma_y)$. [The assumption of normality is not required if we instead assume that we can work to just the order of the first two moments]. Then their correlation is $\rho_{x,y} = \frac{\cov(X,Y)}{\sqrt{VarX}\sqrt{VarY}}$. For $X_1, \ldots, X_n;Y_1,\ldots,Y_n$ jointly iid, this can be estimated by the empirical correlation
\[
	\hat{\rho}_{x,y} = \frac{\frac{1}{n}\sum_{i=1}^{n} (X_i - \overline{X}_n)(Y_i - \overline{Y}_n)}{\sqrt{S_{n,x}} \sqrt{S_{n,y}} }
,\] where $S_{n,x} = \frac{1}{n} \sum_{i=1}^{n}(X_i - \overline{X}_n)^2$ etc. 

The finite sample distribution of $\hat{\rho}_{x,y}$ can in principle be computed analytically; it is given by $f_{\hat{\rho}_{x,y}}(r) \propto (1-r^2)^{\frac{1}{2}(n-4)}, -1\le r\le 1 $.

Alternatively, we can regard $\rho_{x,y} = \Phi(\theta)$, with samples from $N\left(\begin{pmatrix} \mu_{x} \\ \mu_y \end{pmatrix}, \Sigma\right) $, where $\theta = \left( \begin{pmatrix} \mu_x \\ \mu_y \end{pmatrix}, \Sigma  \right) $. 

One shows that $\hat{\rho} = \Phi(\hat{\theta}_{MLE})$, and by the general theory developed earlier (plus Delta method for $\Phi $), we deduce $\sqrt{n} (\hat{\rho}_{x,y} - \rho) \underset{n\to \infty}{\to ^{d}} N(0, \var(\Phi, \theta))$ for some asymptotic variance $\var(\Phi, \theta)$

More generally, consider a partitioned random vector $X \in \R^{p}$ 
\[
	X = \begin{pmatrix} X^{(1)} \\ X^{(2)} \end{pmatrix}, X\sim N(0, \Sigma)
	,\] such that $X \in \R^{q} \times \R ^{p-q}$, with $\Sigma$ a $p\times p$ positive definite matrix. The conditional variance of $X^{(1)} | X^{(2)}$ is given by $\underbrace{\Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{12}}_{\equiv \Sigma_{11\cdot 2}}$, where $\Sigma = \begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{12} & \Sigma_{22} \end{pmatrix} $.

One further defines the partial correlations of $X_i, X_j, i\neq j, i,j \le q$, as
\[
	\rho_{ij\cdot 2} = \frac{\left(\Sigma_{11\cdot 2}\right)_{ij}}{\sqrt{\left(\Sigma_{11\cdot 2}\right)_{ii} \left( \Sigma_{11\cdot 2} \right)_{jj} } }
\] 
Here likewise, the plug-in MLE $\hat{\rho}_{ij\cdot 2}$ equals the 'empirical partial correlation coefficient' $\Phi(\hat{\Sigma}_{MLE})$, where $\hat{\Sigma}_{ij} = \frac{1}{n}\sum_{k=1}^{n} (X_k X_k^{T})_{ij}$, where $\Phi$ is again a smooth map on $\Theta = \{\Sigma \text{ positive definite}\} $

The previous theory again implies that $\sqrt{n}(\hat{\rho}_{ij\cdot 2} - \rho_{ij\cdot 2}) \underset{n\to \infty}{\to ^{d}} N(0,\cdot )$

Finally, consider $X \sim N(0, \Sigma)$, again with $\Sigma$ a $p\times p$ positive definite symmetric. Then there exists an orthonormal matrix $T$ s.t.
\[
	\Sigma = T^{T}\Lambda T \text{, } \Lambda = diag(\lambda_1, \ldots, \lambda_p), \lambda_1 >  \lambda_2 > \ldots > \lambda_p > 0.
\] 

In this case, we can define $U = TX \sim N(0, T\Sigma T^{T}) = N(0, \Lambda)$, and the entries of the random vector $U\in \R^{p}$ are called the principal components corresponding to the principle subspaces spanned by the column vectors of T (eigenvectors of  $\Sigma)$, arranged in decreasing order of 'explained variance'. Here again, if $\hat{\Sigma}_{ij} = \frac{1}{n} \sum_{k=1}^{n}(X_m X_m^{T})_{ij} $, then the plug-in MLE will give asymptotically efficient estimators $ \lambda_i, u_i$ provided that $\Sigma$ has no multiplicity in its eigenvalues. 

\subsection{Monte Carlo Methods}

We will discuss algorithms to generate random samples from given probability distributions, which are useful to construct numerical approximations of inference methods (e.g. non-conjugate posterior distributions).

One can generate, with a pseudorandom number generator (see Computer Science), random samples $U_1, \ldots, U_N \stackrel{iid}{\sim} U[0,1], N \in \N$. If $F$ is a cdf of some r.v. $X$, with quantile transform 
\[
	F^{-}(u) = \inf \{x_i : u \le F(x)\} 
,\]
then one shows that $X_i^{\ast} = F^{-} (U_i)$ are iid r.v's with distribution $F$

For normal r.v's, $F^{-}$ is not available in closed form, but one can still simulate $X \sim N_2 (0, I_2)$ starting from $U_1, U_2 \stackrel{iid}{\sim} U(0,1)$ by the Box-M{\"u}ller transform (see ex. sheet).

More elaborate MC-algorithms arise from \textit{importance sampling}, e.g. as in the following 'Accept-Reject' algorithm, where we want to sample from some pdf $f$ on $\chi$, and have another density $h$ that we can sample from, s.t. 
\[
	f(x) \le Mh(x) \text{  } \forall x \in \chi \text{, some } M>0
\] 

\begin{enumerate}
	\item Draw $X \sim h$, and independently $U \sim U[0,1]$
	\item Set $Y=X$ if $U \le \frac{f(X)}{M h(X)}$, otherwise return to step 1.
\end{enumerate}

One shows (ex sheet) that $Y \sim f$ on $\chi$.

In the preceding settings, if we can generate $\left(X_i^{\ast}\right)_{i = 1,\ldots,N}$ iid from a fixed distribution $P_F$, then we can numerically approximate integrals 
\[
	\E_{P_F} g(X) = \int_{\chi} g(x) dP_F(x) \text{, $g$ given}
\] 
by the MC-average
\[
	\frac{1}{N} \sum_{i=1}^{N} g(X_i^{\ast}) \underset{N\to \infty}{\overset{SLLN}{\to }} \E_{P_F} g(X)
\] 

\subsection{Markov Chain Monte Carlo (MCMC) Algorithms}

A discrete time Markov chain $(\vartheta_m : m\in \N)$ started at $\vartheta_0$ is a sequence of random variables whose 'transition probabilities' are of the form
\begin{align*}
P\left( \vartheta_m \in \mathcal{B} | \vartheta_{m-1}, \vartheta_{m-2}, \ldots \right) &= P\left( \vartheta_m \in \mathcal{B} | \vartheta_{m-1} = t \right) \\
&= P\left( \vartheta_1 \in \mathcal{B} | \vartheta_0 = t \right) \\
&= K(t,\mathcal{B})
\end{align*}
where $\mathcal{B} \subseteq \Theta$ (meas), and where $K$ is a \textit{Markov kernel} s.t. $K(t, \cdot )$ is a pdf on $\Theta$, the state space of $(\vartheta_m)$.

A pdf/pmf  $\mu$ on $\Theta$ is called invariant for $K$ if
\begin{align*}
	\int_{\Theta} K(t,B) \mu(t) dt &= \int_{\Theta} P\left( \vartheta_1 \in \mathcal{B} | \vartheta_0 = t \right) \mu(t) dt \\
	&= \mu(t)
\end{align*}

Under additional hypotheses (ergodicity of $(\vartheta_m)$), one shows that the distribution of  $\vartheta_m$ 'mixes towards' (converges to in some sense) its invariant measure $\mu$, and we can then use MC-averages $(\vartheta_m : m=1,\ldots,N)$ to approximate $\E_{\mu} g(X)$ by $\frac{1}{N} \sum_{i=1}^{N}g(\vartheta_m)$

An important MCMC method, known as the \textit{Metropolis-Hastings} algorithm, requires an auxiliary (conditional) pdf $q(\cdot , t), t\in \Theta$ that we can sample from, and proceeds as follows:
\begin{enumerate}
	\item Given $m\in \N, \vartheta_m \in \Theta$, generate $s_m \sim q(\cdot |\vartheta_m)$.
	\item Set $\vartheta_{m+1} = \begin{cases}
			s_m \text{ with probability } \rho(\vartheta_m, s_m) \\
			\vartheta_m \text{ with probability } 1 - \rho(\vartheta_m, s_m)
	\end{cases}$ 
\end{enumerate}
where
\[         
	\rho(t,s) = \min\left\{ 1,  \frac{\mu(s)}{\mu(t)} \frac{q(t|s)}{q(s|t)} \right\} 
,\]        
and $\mu$ is a prescribed pdf/pmf (in fact our invariant measure).

\begin{prop}
	For the above Markov chain, assuming $\mu, q(\cdot |t)$ are strictly positive on $\Theta$, the invariant measure of $(\vartheta_m : m \in \N)$ equals $\mu$.
\end{prop}

 \begin{proof}
	 (Uniqueness requires Probability and Measure).

	 The transition Markov kernel $K$ has 'density' k,
	 \[
		 k(t,s) = \rho(t,s)q(s|t) + (1-r(t))d\delta_{t}(s), s \in \Theta
	 ,\]
	 where $\delta_{\tau}$ is the Dirac point mass probability measure at $\tau$, and where $r(t) = \int_{\Theta} \rho(t, \tau)q(\tau|t) d\tau$.

	 We have that
	 \begin{align*}
		 \rho(t,s)q(s|t)\mu(t) &= \min\left( q(s|t)\mu(t), \frac{\mu(s)}{\mu(t)} \frac{q(t|s)}{q(s|t)} q(s|t)\mu(t) \right)  \\
		 &= \min\left( \mu(t)q(s|t), \mu(s) q(t|s) \right)  \\
		 &= \min\left( \frac{\mu(t)}{\mu(s)} \frac{q(s|t)}{q(t|s)} q(t|s)\mu(s), q(t|s)\mu(s) \right)  \\ 
		 &= \rho(s,t)q(t|s)\mu(s)
	 \end{align*}
	So our detailed balance conditions hold.

	Then
	\begin{align*}
		\int_{\Theta} P\left( \vartheta_1 \in \mathcal{B} | \vartheta_0 =t \right) \mu(t) dt &= \int_{\Theta}\int_{\mathcal{B}} \rho(t,s)q(s|t)\mu(t) dsdt + \int_{\Theta}\int_{\mathcal{B}} (1-r(t))d\delta_{t}(s)\mu(t)dt \\
		&= \int_{\mathcal{B}}\int_{\Theta} \rho(s,t)q(t|s)\mu(s) dt ds + \int_{\Theta} 1_{\{t\in \mathcal{B}\} } (1-r(t))\mu(t) dt \\
		&= \int_{\mathcal{B}} r(s) \mu(s) ds + \int_{\mathcal{B}} (1-r(t)) \mu(t) dt \\
		&= \int_{\mathcal{B}} \mu(s) ds \\
		&= \mu(\mathcal{B})
	\end{align*}
\end{proof}

The Metropolis-Hastings Markov chain can be used to approximately compute general posterior distributions arising from data $X_1, \ldots, X_n$ in a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}, \Theta = \R^{p}, p \in \N$, when the prior $\pi$ is $N(0, \Sigma_p)$ where $\Sigma_p$ is a $p\times p$ non-singular covariance matrix. In this case, the 'target' density $\mu$ should be
\[
	\pi(\theta|X_1,\ldots,X_n) \propto e^{l_n(\theta) - \frac{1}{2} \theta^{T}\Sigma_p^{-1} \theta}, \theta \in \R^{p}
\] 

If in the Metropolis-Hastings algorithm, we choose auxiliary densities $q(\cdot ,t) \sim N(\sqrt{1-2\delta}t, 2\delta \Sigma_p )$, which is possible by results from previous lectures, and then the corresponding steps are
\begin{enumerate}
	\item Given $\vartheta_m, m \in \N$, generate $\xi \sim N(0, \Sigma_p)$, define
		 \[
		S_m = \sqrt{1-2\delta} \vartheta_m + \sqrt{2\delta} \xi 
		\]
	\item Set $\vartheta_{m+1} = \begin{cases}
			S_m \text{ with probability } \rho(\vartheta_m, S_m) \\
			\vartheta_m \text{ with probability } 1 - \rho(\vartheta_m, S_m)
	\end{cases},$ 
	where $\rho(\vartheta_m, S_m) = \min\{1, e^{l(S_m) - l(\vartheta_m)}\}$
\end{enumerate}
This is sometimes called a pCN (preconditioned Crank-Nicolson) algorithm.

One shows (ex sheet) that an invariant (it is unique by P\&M) measure of $(\vartheta_m: m \in \N)$ is given by $\pi(\cdot |X_1, \ldots, X_n)$ from our target density $\mu$. This is valid for any $\delta > 0$, and we can think of $\vartheta_m$ as a Gaussian random walk, which moves forward calibrated by a sequence of corresponding likelihood ratio tests between $\vartheta_m$ and $S_m$. This way we can use Monte Carlo samples $(\vartheta_m : m=M_0, \ldots, M_0+M)$ where $M_0$ is some turn after initialisation at $\vartheta_0$, to approximate the posterior mean $\E [\theta |X_1,\ldots,X_n]$ by $\frac{1}{M}\sum_{m=M_0+1}^{M_0+M} \vartheta_m$, and likewise the posterior quantiles $R_n$ by empirical quantiles of the chain. In particular, we can approximately compute credible sets 
\[
	C_n = \{\theta : \|\theta - \E [\theta|X_1,\ldots,X_n]\| \le R_n\}, \pi(C_n|X_1,\ldots,X_n) = 1-\alpha
,\] which by the Bernstein-von Mises theorem are approximate $1-\alpha$ confidence sets, without requiring estimation of $I(\theta)^{-1}$.

\subsection{Gibbs Sampling}

In Bayesian statistics, often hierarchical prior specifications are of interest, e.g.
\[
	X|\theta \sim N(\theta,1), \theta \sim N(0, \sigma^2), \sigma^2 \sim \pi_{\sigma}, \pi_{\sigma} \text{ a \textit{hyperprior}}
.\]
If $X, Y$ are any r.v's with joint pdf/pmf $f_{X,Y}$ s.t. we can sample from the conditional distributions $f_{X|Y}, F_{Y|X}$, then the following 'Gibbs sampling' scheme can be used.

Initialise $X= x_0$, then draw $Y_1 \sim f_{Y|X}(\cdot |x_0)$, then $X_1 \sim f_{X|Y}(\cdot , y_1)$. Repeating, we have $Y_m \sim f_{Y|X}(\cdot |X_m-1), X_m \sim f_{X|Y}(\cdot |Y_m), m \in \N$.

One shows that $(X_m, Y_m), X_m, Y_m$ form Markov chains with invariant densities equal to $f_{X,Y}, f_{X|Y}, f_{Y|X}$ respectively.

\subsection{Bootstrap Methods}

There are non-Bayesian ways to bootstrap the 'asymptotic' quantiles of confidence sets, known often as 'bootstrap methods', the first of which (due to B. Efron) we will now study.

Given data $X_1, \ldots, X_n$, consider, conditional on the observed values, a new sample space $\chi_n^{b} = \{X_1, \ldots, X_n\} $, and draw bootstrap r.v's $X_{nj}^{b}, j=1,\ldots,n$ randomly from $\chi_n^{b}$ with replacement, i.e. precisely with law $\P_n = \P_n(\cdot |X_1,\ldots,X_n)$, $\P_n (X_{nj}^{b} = X_i) = \frac{1}{n} \forall i,j \text{ ($n$ fixed) }$ (This is easily done by sampling uniform variables and partitioning  $[0,1]$ into n equal parts).

The bootstrap sample mean 
\[
\overline{X_n^{b}} = \frac{1}{n} \sum_{j=1}^{n} X_{nj}^{b} 
\] has $\E$-expectation
\[
	\E X_{nj}^{b} = \sum_{i=1}^{n} X_i \P(X_{nj}^{b} = X_i) = \overline{X}_n
,\] 

and so by linearity $\E \overline{X_n^{b}} = \overline{X}_n$

The idea is to use the 'known' distribution of $\overline{X_{n}^{b}} - \overline{X}_n$ as a proxy for the unknown distribution of $\overline{X}_n - \mu, \mu = \E X_i$.

We can calculate \textit{roots} $R_n^{b} = R_n^{b}(X_1,\ldots,X_n)$ such that
\[
	\P_n\left(\omega \in \chi_n^{b} : | \overline{X_n^{b}}(\omega) - \overline{X}_n(\omega)| \le \frac{R_n}{\sqrt{n} }\right) = 1- \alpha, 0 < \alpha < 1
\] by evaluating the quantiles of the empirical approximation  of $\P_n(\cdot )$ obtained from repeated bootstrap sampling (via Monte Carlo). One then proposes a bootstrap confidence set
\[
\mathcal{C}_n^{b} = \left\{v \in \R : |\overline{X}_n - v| \le  \frac{R_n}{\sqrt{n} }\right\} 
\] 

This approach extends to general statistical models $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, by computing 'resampled' estimators $\hat{\theta}_n^{b} = \hat{\theta} (X_{n1}^{b}, \ldots, X_{nn}^{b})$, where  $\hat{\theta}(\cdot )$ is the MLE based on corresponding data points, and where $X_{nj}^{b}$ are iid $\P_n$, resampled from $\{X_1,\ldots,X_n\} $, where $X_i \stackrel{iid}{\sim} f(\cdot , \theta)$. One then uses the known distribution of $\hat{\theta}_n^{b} - \hat{\theta}_n$, where $\hat{\theta}_n = \hat{\theta}(X_1,\ldots,X_n)$, as a proxy/pivot distribution for the one of $\hat{\theta} - \theta_0$.

One then defines 
\[
\mathcal{C}_n^{b} = \left\{ \theta \in \Theta : \|\hat{\theta}_n^{b} - \hat{\theta_n}\| \le \frac{R_n}{\sqrt{n} } \right\}
,\] where $R_n$ is an appropriate root of
\[
	\P_n \left(\omega \in \chi_n^{b} : \|\hat{\theta}_n^{b}(\omega) - \hat{\theta}_n\| \le \frac{R_n}{\sqrt{n} }\right) = 1- \alpha, 0 < \alpha < 1
\] 

This approach is known as a non-parametric bootstrap, as it resamples from $\{X_1,\ldots,X_n\} $ without using distributional properties of the model. Alternatively, one may draw bootstrap samples from the distribution $P_{\hat{\theta}_n}$ corresponding to the MLE, known as the parametric bootstrap.  

We now prove 'consistency' of the bootstrap of the mean $\overline{X}_n$. 

\begin{thm}
	Let $X_1,\ldots,X_n \stackrel{iid}{\sim} P$ on $\R$ with mean $\mu$ and finite variance $\sigma^2$. Denote by $\Phi = \Phi_{\sigma}$ the cdf of $N(0, \sigma^2)$. Then
	\begin{align*}
		\sup_{t \in \R} \left|\P_n \left( \sqrt{n} \left( \overline{X_n^{b}} - \overline{X}_n \right) \le t | X_1,\ldots,X_n \right) - \Phi(t)\right| \underset{n\to \infty}{\to } 0 \text{ }P^{\N}-a.s. 
	\end{align*}
\end{thm}

\begin{remark}
	\begin{enumerate}
		\item A similar theorem can be proved for $\sqrt{n} (\hat{\theta}_n^{b} - \hat{\theta}_n)$, $\Phi_{I(\theta)^{-1}}$, by using asymptotic normality of $\hat{\theta}_n$ instead of the CLT in the proof (to follow).
		
		\item One shows (ex sheet) that the theorem implies validity of the bootstrap confidence interval, that is that if $P = P_{\mu}$ has mean $\mu$ then 
			\[
				P_{\mu} (\mu \in C_n) \underset{n\to \infty}{\to } 1- \alpha
			,\]
			using a similar proof to the asymptotic justification of Bayesian credible sets.
	\end{enumerate}
\end{remark}

\begin{proof} (Non-examinable)
	We start with a fact on convergence in distribution:
	
	Recall that $Z_n \to ^{d} Z$ if $P\left( Z_n \le x \right) \underset{n\to \infty}{\to } P\left( Z \le x \right)$ when the limit is continuous at x. 

	\begin{lemma}
		Suppose $F_n, F$ are cdf's on $\R$, and $F$ is continuous on $\R$. If further  $F_n(x) \to F(x)$ $\forall x \in \R$, then 
		\[
			\sup_{x \in \R} |F_n(x) - F(x)| \to 0 \text{ as } n\to \infty
		\]

		It is said that their Kolmogorov-distance goes to $0$.
	\end{lemma}

	\begin{proof}
		Since F is continuous, it takes $\R$ onto $[0,1]$. Thus 
		\[
			\forall k \in \N, \text{there exist points} -\infty = x_0 < x_1 < \ldots < x_n = \infty
		,\] s.t. $F(x_i) = \frac{i}{k}$, $i_0,\ldots,k$.

		Now let $\epsilon > 0$ and choose $k > \frac{2}{\epsilon}$, then for $x \in [x_i, x_{i+1}]$, we can write
		\begin{align*}
			F_n(x) - F(X) &\le  F(x_{i+1}) - F(x_i) \\
				      &= F_n(x_{i+1}) - F(x_{i+1}) + \frac{1}{k} \\
		\end{align*}
		
		Likewise,
		\begin{align*}
			F_n(x) - F(x) \ge F_n(x_i) - F(x_i) - \frac{1}{k}
		\end{align*}

		So that choosing $n = n(k) = n(\epsilon)$ large enough implies that
		\[
			|F_n(x) - F(x)| < \frac{2\epsilon}{2} = \epsilon
		,\]
		and so $\sup_x |F_n(x) = F(x)| \underset{n\to \infty}{\to } 0$

	\end{proof}

	By the previous lemma, it will suffice to show convergence in distribution of 
	 \[
		 \sqrt{n}\left( \frac{1}{n} \sum_{j=1}^{n} X_{nj}^{b} - \E X_{nj}^{b} \right) \to ^{d} N(0, \sigma^2)  
	\]
	on an event of $P^{\N}$-probability one. 

	\begin{prop} (CLT for triangular arrays)
		
		For each $n \in \N$, let $(Z_{n,i} : i = 1,\ldots,n)$ be iid r.v's with finite variance s.t.
		\begin{enumerate}
			\item $\forall \delta > 0 (\delta = \frac{1}{m}, m \in \N \text{ actually suffices})$, \[
					n P\left( |Z_{n1}| > \sqrt{n}\delta  \right) \underset{n\to \infty}{\to } 0
	\]

			\item $\var\left( Z_{n1} 1_{\{|Z_{n1}| \le \sqrt{n} \}}  \right) \underset{n\to \infty}{\to } \sigma^2$

			\item $\E Z_{n1} 1_{\{|Z_{n1} > \sqrt{n} \}} \underset{n\to \infty}{\to } 0$
		\end{enumerate}

		Then $\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^{n} Z_{ni} - \E Z_{ni} \right) \underset{n\to \infty}{\to ^{d}} N(0, \sigma^2)$
	\end{prop}

	We will apply this proposition to $Z_{ni} = X_{ni}^{b}$ under $\P_n = \P_{n}(\cdot |X_1,\ldots,X_n)$, and show that there exists a set $A \subseteq \R^{n}$ supporting $\{X_1,\ldots,X_n\} $ s.t. $\P^{\N}(A) = 1$, and on $A$, the conditions of the proposition are verified. Then on A, the previous lemma implies uniform convergence of the cdf's, and the theorem will be proved.

	To check our conditions, a key observation is the following: For all $c>0$, the SLLN for $X_i$ 's (under $\P^{\N}$ ) implies that for $1\le p\le 2$,
	\[
	\frac{1}{n}\sum_{i=1}^{n}|X_i|^{p} 1_{\{|X_i| > c\} } \underset{n\to \infty}{\to^{a.s} } \E |X_1|^{p} 1_{\{|X_1| > c\} }
	\]

	Since the limit is the tail $\int_{|x|>c} |x|^{p} dP(x)$ of a convergent integral, it tends to $0$ as $c\to \infty$. Therefore, given $\epsilon >0$, we can choose $c = c_{\epsilon}$ large enough s.t. 
	\[
	\E |X_1|^{p} 1_{\{|X_1| > c\} } < \frac{\epsilon}{2}
	\]

	Therefore, for any $\delta > 0$, we can choose $n = n_{\delta, c} = n_{\delta, \epsilon}$ large enough s.t $\sqrt{n} \delta > c$ s.t.
	\[
	0 \le \frac{1}{n} \sum_{i=1}^{n}|X_i|^{p} 1_{\{|X_i| > \sqrt{n}\delta \} } \le \frac{1}{n} \sum_{i=1}^{n} |X_i|^{p} 1_{\{|X_i| > c\} } - \E |X_1|^{p}1_{\{|X_1| > c\} } + \E |X_1|^{p}1_{\{|X_1| > c\} } \le  \frac{\epsilon}{2} + \frac{\epsilon}{2}
,\]
where our last inequality comes from our choice of $c$ and our observation about the SLLN. So overall, we have that

\[
	(\dagger) \frac{1}{n}\sum_{i=1}^{n}|X_i|^{p} 1_{\{|X_i| > \sqrt{n} \delta\} } \underset{n\to \infty}{\to } 0 \; \P^{\N}-a.s, 1\le p\le 2, \delta > 0
\]

Denote $A^{\dagger}$ the event on $P^{\N}$-probability one, on which the last limit holds. Then on $A^{\dagger}$,

\begin{enumerate}
	\item \begin{align*}
		n \P_n \left( |X_{n1}^{b}| > \sqrt{n} \delta \right) &= n \int_{|x|> \sqrt{n} \delta} \phi \P_n  \\
		&= n \frac{1}{n} \sum_{i=1}^{n} 1_{\{ |X_i| > \sqrt{n} \delta\} } \text{ on } 1\le \frac{\delta^{-1}}{\sqrt{n} } |X_i| \\
		&= \frac{\delta^{-2}}{n} \sum_{i=1}^{n} X_i^2 1_{\{|X_i| > \sqrt{n} \delta\} } \\
		&\to 0 \text{ on } A^{\dagger} \text{ as } n\to \infty
		\end{align*}

\item[3.] $\sqrt{n} |\E X_{n1}^{b} 1_{\{|_{n1}^{b} > \sqrt{n} \} }  \le  \sqrt{n} \frac{1}{n}\sum_{i=1}^{n} |X_i| 1_{\{|X_i| > \sqrt{n} \} } \le \frac{1}{n} \sum_{i=1}^{n} X_i^2 1_{|X_i| > \sqrt{n} } \underset{n\to \infty}{\to }0 \text{ on } A^{\dagger} \text{ as before.}$

\item[2.] \begin{align*}
		\var \left( X_{n1}^{b} 1_{\{|X_{n1}^{b}| \le \sqrt{n} \} } \right) &= \E \left[X_{n1}^{b} 1_{\{|X_{n1}^{b}| \le \sqrt{n} \} }  \right]^2 - \left[ \E X_{n1}^{b} 1_{\{|X_{n1}^{b}| \le  \sqrt{n} \} } \right]^2   \\
		&= \frac{1}{n} \sum_{i=1}^{n} X_i^{2} 1_{\{|X_i| \le \sqrt{n} \}} - \left( \frac{1}{n}\sum_{i=1}^{n} X_i 1_{\{|X_i| \le \sqrt{n} \} } \right)^2 \\
		&= \left[ \frac{1}{n} \sum_{i=1}^{n} X_i^2 - \left(\frac{1}{n}\sum_{i=1}^{n}\right)^2 \right] - \frac{1}{n} \sum_{i=1}^{n} X_i^2 1_{\{|X_i| > \sqrt{n} \} } \\
		&\text{  }- \left[ \left(\frac{1}{n} \sum_{i=1}^{n} X_i 1_{\{|X_i|\le \sqrt{n} \} }\right)^2 - \left( \frac{1}{n}\sum_{i=1}^{n} X_i \right)^2  \right] \\
	\end{align*}

	Now, our first term converges to $\sigma^2$ on an event $A$ of $\P^{\N}$-probability $1$ by the SLLN. Our second term converges to $0$ on $A^{\dagger}$, and writing our last term as $a_n^2 - b_n^2$, we have that  $a_n^2 - b_n^2 \to 0$ whenever $a_n - b_n \to 0$, as $a_n + b_n$ is stochastically bounded. Since $a_n - b_n \to 0$ on $A^{\dagger}$, we indeed have 
	\[
		\var \left( X_{n1}^{b} 1_{\{|X_{n1}^{b}| \le \sqrt{n} \} } \right) \to \sigma^2
	\]
\end{enumerate}
\end{proof}

\begin{remark}
	One can show that Efron's bootstrap works \textit{iff} the CLT holds for the $X_i$'s. 
\end{remark}

\subsection{Non-Parametric Methods}

For $X_1, \ldots, X_n$ iid from distribution $P$, the cdf $F$ is given by
\[
	F(t) = P(X\le t) = \int_{\R} 1_{(-\infty, t]}(x) dP(x), t\in \R
\] 

$F$ can be estimated unbiasedly by the empirical cdf
\[
	F_n(t) = \frac{1}{n} \sum_{i=1}^{n} 1_{(-\infty, t]}(X_i) = \frac{\#X_i's \le t}{n}
\]

\begin{thm} (Glivenko-Cantelli)
	
	For any $P$, as $n\to \infty$,
	\[
		\sup_{t \in \R} |F_n(t) - F(t)| \to 0 \; P^{\N}-a.s
	\] 
\end{thm}

\begin{proof}
	We can write 
	\begin{align*}
		\sup_{t \in \R} |F_n(t) - F(t)| &= \sup_{t \in \R} = \left| \frac{1}{n} \sum_{i=1}^{n}1_{(-\infty,t]} (X_i) - \E 1_{(-\infty,t]}(X) \right| \\
		&= \sup_{h\in \mathcal{H}} \left| \frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \right|
	\end{align*}
	Where $\mathcal{H} = \{1_{(-\infty,t]}, t\in \R\} $, and we will use the ULLN from earlier, and need to check the '$\epsilon$-bracketing' condition. Thus, given  $\epsilon = \frac{1}{m}, m \in \N$, we can partition $[0,1]$ into $m$ parts such that, for $F$ continuous, and a corresponding dissection $\{t_i : i=0,\ldots,N(\epsilon)\} $ of $\R$, s.t.  $F(t_{i+1}) - F(t_i) = \frac{1}{m} = \epsilon$. Including $t_0 = -\infty$, $t_{N(\epsilon)} = \infty$ by convention, the family of brackets
\[
	\underline{h}_i(x) = 1_{(-\infty, t_{i}]}, \overline{h}_i(x) = 1_{(-\infty, t_{i+1}]}
\] 
covers $\mathcal{H}$, and s.t. $\E |\overline{h}_i(x) - \underline{h}_i(x)| = |F(t_{i+1}) - F(t_i)| \le \epsilon$, so that the result follows for continuous F.

The general case follows similarly, noting that $F$ can only have at most $m$ jumps over $\frac{1}{m}$, and adding a bracket for any such jumps.
\end{proof}

A version of the CLT can be given as well. Let  $\mathcal{G}$ be a Brownian bridge on $[0,1]$, that is, a Brownian motion started at 0 and conditioned to equal  $0$ at $1$. A F-Brownian bridge is $\mathcal{G}_{F}(x) = \mathcal{G}(F(x)) $, $x\in \R$.

\begin{thm} (Donsley, Doob, Kolmogorov)
        
        $\left( \sqrt{n} (F_n - F)(t): t \in \R \right) \to ^{d} \left( \mathcal{G}_F(t) : t \in \R \right) \text{ as } n\to \infty$ uniformly.
\end{thm}


\begin{thm} (Kolmogorov-Smirnoff)
	For any continuous $F: \R \to [0,1]$, we have
	\[
		\sqrt{n} \sup_{t\in \R} |F_n(t)-F(t)| \to ^{d} \sup_{x\in[ 0,1]} \underbrace{|\mathcal{G}(x)|}_{\text{std B. bridge }} < \infty \text{ a.s}
	\] 
	i.e our worst case error is approximately the maximum of a standard Brownian bridge.
\end{thm}

The idea of the proof is as follows: One applies 'Donsley's' theorem and the continuous mapping theorem for $v \mapsto \sup_{t} |v(t)|$, and observe that for $\tilde{X}_i \stackrel{iid}{\sim} U(0,1)$, with cdf $\tilde{F}(t) = t$ to the effect that
\begin{align*}
	\sqrt{n}  \sup_{t\in [0,1]} \left| \frac{1}{n} \sum_{i=1}^{n} 1_{-\infty,t]}(\tilde{X}_i) - \tilde{F}(t) \right| \underset{n\to \infty}{\to ^{d}} \sup_{t\in [0,1]} |\mathcal{G}(t)|
\end{align*}

Since $F$ takes $\R$ onto $[0,1]$, we can write the supremum on the LHS as
 \begin{align*}
	 \sqrt{n} \sup_{x \in \R} \left| \frac{1}{n} \sum_{i=1}^{n} \underbrace{1_{\{\tilde{X}_i \le F(X)\} }}_{=^{d} 1_{\{X_i \le x\} }, X_i \stackrel{iid}{\sim} F} - F(x) \right| =^{d} \sqrt{n} \sup_{x\in \R} |F_n(x) - F(x)|
\end{align*}

Applications of KS-theorem:

\begin{enumerate}
	\item One constructs a universal confidence band $C_n(x) = \left[ F_n(x) - \frac{R_n}{\sqrt{n} }, F_n(x) + \frac{R_n}{\sqrt{n} } \right] $, s.t. \[\P_F (F(X) \in C_n(X) \forall X  \in \R) \underset{n\to \infty}{\to} 1 - \alpha .\]

	\item In a parametric model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ with MLE $\hat{\theta}$, one constructs a goodness of fit test/model check by referring (possibly after sample-splitting)
		\[
			\sqrt{n} \sup_{t} |F_n(t) - F_{\hat{\theta}_n}(t)|
		\]
		to the quantiles of $\sup_{x \in [0,1]} |\mathcal{G}(x)|$, where $F_{\hat{\theta}_n}$ is the cdf corresponding to $\hat{\theta}_n$.
\end{enumerate}

\end{document} 
