\documentclass[a4paper]{article}
\input{../../../preamble.tex}

\title{Principles of Statistics}
\date{}

\begin{document}
	
\maketitle

\section*{Lecture 1}

\section{Introduction}

Consider a random variable (r.v.) $X$ defined on some probability space: $ X: (\Omega, A, \mathbb{P}) \to \R$. ($\Omega$ is the set of outcomes, $A$ is the measurable events in $\Omega$, $\mathbb{P}$ is the probability measure on $A $), with distribution function $F\left( t \right) = \mathbb{P}\left( \omega \in  \Omega : X\left( \omega \right) \le  t\right), t \in \R$.

If $X$ is a discrete r.v. Then
\[
	F(t) = \sum_{x\le t} f(x) \textrm{, where f is the probability mass function (pmf)}
.\] 
and, if $X$ is a continuous r.v., then
\[
	F(t) = \int_{- \infty}^{t} f(x) dx \textrm{, where f is the probability density function (pdf)}
.\] 

We typically only write $F(t) = \mathbb{P}\left( X \le  t\right)$, where $P$ is the \textit{law} of  $X$ (i.e. the image measure  $P = \mathbb{P} \cdot X^{-1} $

\begin{defn}
	A statistical model for the law $P$ of $X$ is any collection $\{ f\left(\cdot, \theta \right): \theta \in \Theta \} $, or $\{ P_{\theta} : \theta \in \Theta\} $ of pdf/pmf's or probability distributions. The index set $\Theta$ is the parameter space.
\end{defn}

\begin{eg}
	1) $N\left( \theta, 1 \right), \theta \in \Theta = \R $, or $\Theta = \left[ -1, 1 \right] $

	2) $N\left( \mu, \sigma ^2 \right), \left( \mu, \sigma ^2 \right) = \theta \in  \Theta = \R \times \left( 0, \infty \right)  $
	
	3) $Exp\left( \theta \right) , \theta \in \Theta = \left( 0, \infty) \right)  $
\end{eg}

\begin{defn}
	A statistical model $\{ P_{\theta} : \theta \in \Theta \} $ is correctly specified (for the law $P$ of $X$) if $\exists \theta \in \Theta$ s.t. $P_{\theta} = P$. We often write $\theta_{0}$ for this specific 'true' value of $\theta$. We say that observations $X_{1}, \ldots, X_{n} \stackrel{iid}{\sim} $ arise from the model $\{ P_{0} : \theta \in \Theta\} $ in this case. We refer to n as the sample size.
\end{defn}

The tasks of statistical inference comprise at least:

1) Estimation: Construct an estimator $\hat{\theta}_{n} = \hat{\theta}\left( X_{1}, \ldots, X_{n} \right) \in  \Theta $ that is close with high probability to $\theta$ when $X_{1}, \ldots, X_{n} \stackrel{iid}{\sim} P_{\theta}, \forall \theta \in  \Theta$.

2) Hypothesis Testing: For $H_0 : \theta = \theta_{0} $ vs $H_1 : \theta \neq \theta_{0} $, we want a test (indicator) function $\psi_{n} = \psi \left( X_{1}, \ldots, X_{n} \right) $ s.t $\psi _{n} = 0$ with high probability when $H_{0}$ is true, and $\psi_{n} = 1$ otherwise.

3) Confidence Regions (Inference): Find regions (intervals) $C_{n} = C \left( X_{1}, \ldots, X_{n} \right) \subseteq   \Theta $ of confidence in that $P_{\theta} \left( \theta \in  C_{n} \right) \ge 1 - \alpha, \forall \theta \in  \Theta$. This quantifies the uncertainty in the inference on $\Theta$ by the size/diameter of $C_{n}$. Here, $0 < \alpha < 1$ is a pre-described significance level.

\section{Likelihood Principle}

\begin{eg}
        Consider a sample $X_{1}, \ldots, X_{n} \sim ^{iid} Poi\left( \theta \right) $ with (unknown) $\theta > 0$. If the actual observed values are  $X_{1} = x_{1}, \ldots, X_{n} = x_{n}$, then the probability of this particular occurrence of $x_{1}, \ldots, x_{n}$ as a function of $\theta$ is 
\begin{align*}
	f \left( x_{1}, \ldots, x_{n}, \theta \right) &= P_{\theta} \left( X_{1}=x_{1}, \ldots, X_{n} = x_{n} \right) \\
						    &= \prod_{i=1}^{n} P_{\theta} \left( X_{i} = x_{i} \right) \\
						    &= \prod_{i=1}^{n} e^{-\theta} \frac{\theta^{x_{i}}}{x_{i}!} \\
						    &= L_{n}\left( \theta \right), \textrm{ a random function of } \theta 
\end{align*} 
\end{eg}

\begin{remark}
	(C.F. Gauss, R. Fisher): 
	Maximise $L_{n}\left( \theta \right)$ over $\Theta$, and for continuous variables, replace pmf's by pdf's.
\end{remark}

In the example, we can equivalently maximise 
\[
	l_{n}\left( \theta \right) = \log\left( L_{n}\left( \theta \right)  \right) = -n\theta + \log\theta \sum_{i=1}^{n} X_{i}  - \sum_{i=1}^{n} \log\left( X_{i}! \right)
.\]
over $(0, \infty)$. Then  $l_{n}'\left( \theta \right) = -n + \frac{1}{\theta}\sum_{i=1}^{n} X_{i}$, so at maximum $\hat{\theta}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$

Also, $l_{n}''\left( \theta \right) = - \frac{1}{\theta ^2} \sum_{i=1}^{n} X_{i} < 0$ if not all $X_{i} = 0$ (in which case $\hat{\theta} = 0 = \frac{1}{n} \sum_{i=1}^{n} X_{i}$).

\newpage

\section*{Lecture 2}

\begin{defn}
	Given a statistical model $\{f\left( \cdot , \theta : \theta \in \Theta \right) \} $ of pdf/pmf's for the law $P$ of $X$, and given 'numerical' observations  $\left( x_{i}: i=1,\ldots,n \right) $ arising as iid copies $X_{i}\stackrel{iid}{\sim}P$, the \textit{likelihood function of the model} is defined as
	\begin{align*}
		L_{n} : \Theta \to \R \text{, } L_{n}\left( \theta \right) = \prod_{i=1}^{n} f\left( x_{i}, \theta \right)   
	.\end{align*}

Moreover, the \textit{log-likelihood} is
\begin{align*}
	l_{n}: \Theta \to \R \cup \{-\infty\} \text{, } l_{n}\left( \theta \right) = \sum_{i=1}^{n}\log f\left( x_{i}, \theta \right)
,\end{align*}
and the normalised log-likelihood is
\begin{align*}
	\overline{l}_{n}\left( \theta \right) = \frac{1}{n}\sum_{i=1}^{n}\log f\left( x_{i}, \theta \right) 
.\end{align*}
\end{defn}
We regard these functions as ('random' via the $X_{i}'s$) maps of $\theta$.

\begin{defn}
	A \textit{maximum likelihood estimator} (MLE) is any $\hat{\theta} = \hat{\theta}_{MLE} \left( X_{1}, \ldots, X_{n} \right) \in \Theta$ s.t. \[
		L_{n}( \hat{\theta}) = \max_{\theta \in \Theta} L_{n}\left( \theta \right) 
	.\] 
\end{defn}

\begin{eg}
	For $Poi(\theta), \theta \ge 0$, we have seen $\hat{\theta}_{MLE} = \frac{1}{n} \sum_{i=1}^{n} X_{i}$
\end{eg}

\begin{eg}
	$N\left( \mu, \sigma^2 \right) $, where $\theta = \left( \mu, \sigma^2 \right) \in \Theta = \R \times (0, \infty)$, one can show that the MLE
	\begin{align*}
		\hat{\theta}_{MLE} &= \begin{pmatrix} \hat{\mu}_{MLE} \\ \hat{\sigma}^2\end{pmatrix} \\
		&= \begin{pmatrix} \overline{X}_{n} \\ \frac{1}{n} \sum_{i=1}^{n}\left( X_{i} - \overline{X}_{n} \right)  \end{pmatrix} 
	,\end{align*}
	is obtained from solving  $\nabla \ln(\hat{\theta}_{MLE}) = 0$
\end{eg}

\begin{remark}
	Calculation of marginal MLEs that optimise only one variable is not sufficient. Typically the MLE for $\theta \in \Theta \subseteq \R^{p}$ is found by solving the \textit{score equations}
	\[
		S_{n}( \hat{\theta}) = 0, \text{ where } S_{n} : \Theta \to  \R^{p} \text{ is the score function } S_{n} (\theta) = \nabla \ln(\theta)
	.\]
	Here, we use the implicit notation $S_{n}(\hat{\theta}) = \nabla \ln(\theta) |_{\theta = \hat{\theta}}$
\end{remark}

\begin{remark}
	The likelihood principle 'works' as soon as a joint pdf/pmf (family $\{f\left( ., \theta \right) : \theta \in  \Theta\} $) of $X_{1}, \ldots, X_{n}$ can be specified, and does note rely on the iid assumption. For instance in the normal linear model, $N\left( \chi \beta, \sigma^2 I \right) $, where $\chi$ is an $n\times p$ matrix, $(\beta, \sigma^2) = \theta \in \R^{p} \times (0, \infty)$, the MLE coincides with the LS-estimator (not iid, but independent).
\end{remark}

\section{Information Geometry}

For a r.v. $X$ of law/distribution $P_{\theta}$ on $\chi \subseteq \R^{d}$, and let $g: \chi \to  \R$be given. We will write
\begin{align*}
	\E_{\theta} g(X) &= \E_{P_{\theta}} g(X) \\
	&= \int_{\chi} g(x) dP_{\theta}(x)
,\end{align*}
which in the continuous case equals $\int_{\chi} g(x) f(x, \theta) dx$, and in the discrete case is $\sum_{x \in \chi}g(x) f(x, \theta)$.

\begin{observation}[1]
	Consider a model $\{f(., \theta) : \theta \in \Theta\}$ for $X$ of law $P$ on $\chi$, and assume $\E_{P} |\log f(x, \theta)| < \infty$. Then $\overline{l}_{n}(\theta) = \frac{1}{n}\sum_{i=1}^{n}\log f(X_{i}, \theta)$ as a sample approximation of
	\begin{align*}
		l(\theta) = \E_{P} \log f(X, \theta), \theta \in \Theta
	.\end{align*}
	If the model is correctly specified with any true value $\theta_{0}$ s.t. $P=P_{\theta_{0}}$, then we can rewrite
	\begin{align*}
		l(\theta) &= \E_{P_{\theta_{0}}} \log f(X, \theta) \\
		&= \int_{\chi} \left(\log f(x, \theta) \right) f(x, \theta_{0}) dx
	.\end{align*}

	Next, we write 
	\begin{align*}
		l(\theta) - l(\theta_{0}) &= \E_{\theta_{0}}\left[ \log \frac{f(X, \theta)}{f(X, \theta_{0})} \right] \text{, which by Jensen's inequality applied to log} \\
		&\le \log \E_{\theta_{0}}\left[ \frac{f(X, \theta)}{f(X, \theta_{0})} \right] \\
		&= \log \int_{\chi} \frac{f(x, \theta)}{f(X, \theta_{0})} f(X, \theta_{0}) dx = 0,\text{ } \forall \theta \in  \Theta
	.\end{align*}

	Thus $l(\theta) \le l(\theta_{0}) \text{ }\forall \theta \in \Theta$, and approximately maximising $l(\theta)$ appears sensible.

	Note next that by the strict version of Jensen's inequality,  $l(\theta) = l(\theta_{0})$ can only occur when $\frac{f(X, \theta)}{f(X, \theta_{0})} = \text{const (in $X$) }$, which since $\int_{\chi} f(x, \theta) dx = 1$ can only happen when $f(\cdot , \theta) \stackrel{o.s}{=} f(\cdot , \theta_{0})$.
\end{observation}

The quantity 
\begin{align*}
	0 \le  -\left( l(\theta) - l(\theta_{0}) \right) &= \E_{\theta_{0}} \log \frac{f(X, \theta_{0})}{f(X, \theta)} \\
	&\equiv KL\left( P_{\theta_{0}}, P_{\theta} \right) 
\end{align*}
is called the Kullback-Leibler divergence (entropy-distance), which builds the basis of statistical information theory. In particular, the differential geometry of the map $\theta \to KL\left( P_{\theta_{0}}, P_{\theta} \right)$ determines what 'optimal' inference in a statistical model could be.

\section*{Lecture 3}

Let us say a statistical model $\{f(\cdot , \theta) : \theta \in \Theta\} $ is regular if $\frac{d}{d \theta} \frac{d^2}{d \theta d \theta ^{T}} ( = \nabla_{\theta}, \nabla_{\theta} \nabla_{\theta}^{T} )$ of $f(x, \theta)$ can be interchanged with $\int (\cdot ) dx$ integration.

\begin{observation}[2]
	In a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta\}$, we have $\forall \theta \in\intr(\Theta)$ (the interior in $\R^{p}$) that
	\begin{align*}
		0 &= \frac{d}{d \theta} 1 \\
		&= \frac{d}{d \theta} \int_{\chi} f(x, \theta) dx \\
		&= \int_{\chi} \frac{d}{d\theta} f(x,\theta) dx \\
		&= \int_{\chi} \frac{d}{d \theta} [\log f(x, \theta)] f(x, \theta) dx \\
		&= \E_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \right]
	\end{align*}
	In other words, the score vector will be $\E_{\theta}$ centred $\forall \theta \in\intr(\Theta)$
\end{observation}

\begin{defn}
	Let $\Theta \subseteq \R^{p}, \theta \in \intr(\Theta)$. Then the $p\times p$ matrix
	\[
		I(\theta) = \E_{\theta}\left[ \frac{d}{d \theta} \log f(X, \theta) \frac{d}{d \theta} \log f(x, \theta)^{T} \right] 
		\text{ , (if it exists) }
	\]
	is called the \textit{Fisher information (matrix)} of the model $\{f(\cdot , \theta) : \theta \in \Theta\} \text{ at } \theta$.
\end{defn}

\begin{prop}
	In a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta\}$, we have $\forall \theta \in\intr (\Theta), \Theta \subseteq \R^{p}, p\ge 1 $,
	\[
		I(\theta) = - \E_{\theta} \left[ \frac{d^2}{d \theta d \theta^{T}} \log f(X, \theta) \right] 
	\] 
\end{prop}

\begin{proof}
	As earlier, we write
	\begin{align*}
		0 &= \frac{d}{d \theta d \theta^{T}} 1 \\
		&= \frac{d^2}{d \theta d \theta^{T}} \int_{\chi} f(x, \theta) dx \\
		&= \int_{\chi} \frac{d^2}{d \theta d \theta^{T}} f(x, \theta) dx \tag{*}
	\end{align*}

	Moreover, using the chain/product rule, we have
	\begin{align*}
		\frac{d^2}{d\theta d\theta^{T}}\log f(X, \theta) &= \frac{d}{d \theta^{T}} \left[ \frac{1}{f(X, \theta)} \frac{d}{d\theta} f(X,\theta) \right] \\
		&= \frac{1}{f_(X,\theta)} \frac{d^2}{d\theta d\theta^{T}} f(X, \theta) - \frac{1}{f^2(X, \theta)} \frac{d}{d \theta} f(X, \theta) \frac{d}{d \theta} f(X, \theta)^{T}
	\end{align*}
	Then, taking $\E_{\theta}$ expectation, we see
	 \begin{align*}
		 \E_{\theta} \left[ \frac{d^2}{d\theta d\theta^{T}} \log f(X, \theta)\right] &= \int_{\chi} \frac{d^2}{d\theta d\theta^{T}} f(X,\theta) \frac{f(X, \theta)}{f(X,\theta)} dx - \E_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \frac{d}{d \theta} \log f(X, \theta)^{T}  \right] 
	\end{align*}
	Hence by $(*)$, (3.1) holds.
\end{proof}

\begin{remark}
\item[1)] When $p=1$, the above expressions simplify and we have
	\begin{align*}
		I(\theta) &= \E_{\theta} \left( \left[ \frac{d}{d \theta} \log f(X, \theta) \right]^2  \right)  \\
		&= Var_{\theta} \left[ \frac{d}{d \theta} \log f(X, \theta) \right]  \\
		&= -\E_{\theta \left[ \frac{d^2}{d \theta^2} \log f(X, \theta) \right] }
	\end{align*}

\item[2)] If $X = (X_1, \ldots, X_n)$ consists of iid copies of $X$ so that its pdf/pmf equals $f(x_1, \ldots, x_n, \theta) = \prod_{i=1}^{n} f(x_i, \theta)$, then the Fisher information \textit{tensorises}, that is 
	\begin{align*}
		I_n(\theta) &= \E_{\theta}\left[ \frac{d}{d \theta} \log f(X_1, \ldots, x_n, \theta) \frac{d}{d\theta} \log f(X_1, \ldots, X_n, \theta)^{T} \right] \\
		&= \sum_{i,j = 1}^{ n} \E_{\theta} \left[ \frac{d}{d \theta} f(X_{i}, \theta) \frac{d}{d \theta} \log f(X_j, \theta)^{T} \right]  \\
		&= \sum_{i=1}^{n} \E_{\theta} \left[ \frac{d}{d \theta} \log f(X_i, \theta) \frac{d}{d\theta} f(X_i, \theta)^{T} \right] + \sum_{i\neq j} \E_{\theta} \left[ \frac{d}{d\theta} \log f(X_i, \theta) \right] \E_{\theta}\left[ \frac{d}{d\theta} \log f(X_j, \theta) \right]  \\
		&= nI(\theta) + 0
	\end{align*}
	Where $I(\theta)$ is the Fisher information 'per observation', i.e the Fisher info for $\{f(x, \theta) : \theta \in \Theta, x \in \R\} $.
\end{remark}

\newpage

\begin{prop}
	(Cramer-Rao lower bound/inequality)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim}$ from a regular statistical model $\{f(\cdot , \theta) : \theta \in \Theta, \Theta \subseteq \R\}$ and suppose $\tilde{\theta} = \tilde{\theta}(X_1, \ldots, X_n)$ is any unbiased estimator of $\theta$ (i.e $\E_{\theta} \tilde{\theta} = \theta \text{ } \forall \theta \in \Theta$. Then $\forall \theta \in \intr(\Theta)$,
	\[
		Var_{\theta} \tilde{\theta} \ge \frac{1}{nI(\theta)} \text{ } \forall n \in  \N
	\] 
\end{prop}

\begin{proof}
	Assume wlog  that $Var_{\theta} \tilde{\theta} < \infty$, and consider first $n=1$. recall the Cauchy-Schwarz inequality to the affect that $Cov^2(Y, Z) \le Var Y\text{ } Var Z$. For $Y= \tilde{\theta}$ and for $Z = \frac{d}{d\theta} \log f(X, \theta)$. Then $\E_{\theta} Z = 0$ by observation 2, and by the preceding remark, $\E_{theta} Z = Var_{\theta} Z = I(\theta)$. Thus by Cauchy-Schwarz inequality,
	\begin{align*}
		Var(\tilde{\theta}) &\ge \frac{Cov^2(Y, Z)}{I(\theta)} \text{, since} \\
		Cov(Y,Z) &= \E_{\theta}(YZ) \text{, as } E_{\theta} Z = 0 \\
		&= \int_{\chi} \tilde{\theta} (x) \left( \frac{d}{d\theta} \log f(x,\theta) \right) f(X, \theta) dx \\
		&= \int_{\chi} \tilde{\theta}(x) \frac{d}{d\theta} f(x,\theta) dx \\
		&= \frac{d}{d\theta} \int_{\chi} \tilde{\theta}(x) f(x, \theta) dx \\
		&= \frac{d}{d\theta} \E_{\theta} \tilde{\theta} \\
		&= \frac{d}{d \theta} \theta \\
		&= 1
	\end{align*}

	For general n, replace $Z$ by $\frac{d}{d\theta} \log \prod_{i=1}^{n} f(X_{i}, \theta)$, and use that 
\[
	\E_{\theta} g(X_1, \ldots, X_n) = \int_{\chi} g(x_1, \ldots, x_n) \prod_{i=1}^{n} f(x_1, \ldots, x_n, \theta ) dx_{1} \ldots dx_{n}
,\]
and use that Fisher information tensorises. 
\end{proof}

Let us recall also

\begin{corol}
	If $\tilde{\theta}$ is not necessarily unbiased, the proof still gives
	\[
		Var_{\theta} (\tilde{\theta}) \ge \frac{(\frac{d}{d\theta} \E_{\theta} \tilde{\theta} )^2} {nI(\theta)} \text{ } \forall \theta \in \Theta, \Theta \subseteq \R
	\] - to be called: CR-inequality for biased estimators. 
\end{corol}

\section*{Lecture 4}

A multi-dimensional version of the CRLB can be obtained from considering estimation of general differentiable functions $\Phi : \Theta \to \R, \Theta \in \R^{p}$. Then one shows that for any unbiased estimator $\tilde{\Phi} = \tilde{\Phi}(X_1, \ldots, X_n)$, where $X_i \stackrel{iid}{\sim} \{f(\cdot , \theta) : \theta \in \Theta\} $, we have
\[
	Var_{\theta} \tilde{\Phi} \ge \frac{1}{n} \frac{\partial\Phi}{\partial\theta}^{T}(\theta)I(\theta)^{-1}\frac{\partial\Phi}{\partial\theta}(\theta) \text{,  } \forall \theta \in\intr(\Theta)
\] 
Indeed, for p=1, the proof is the same, but replacing $\frac{d}{d\theta} \E_{\theta} \tilde{\theta} = \frac{d}{d\theta} \theta = 1$ by $\frac{d}{d\theta} \E_{\theta} \tilde{\Phi} = \frac{d}{d\theta} \Phi(\theta)$, and for $p>1$ it only needs notational adjustment.

In particular, setting $\Phi(\theta) = \alpha ^{T} \theta$ for any $\alpha \in  \R^{p}$, we see that for any unbiased estimator $\tilde{\theta}$ of $\theta$ in $\R^{p}$, we also have
\[
	Var_{\theta} (\alpha ^{T}\tilde{\theta}) \ge \frac{1}{n} \alpha ^{T}I(\theta)^{-1}\alpha \text{     } \forall \alpha \in \R^{p},
\] 

so that $Cov_{\theta} \tilde{\theta} - \frac{1}{n} I(\theta)^{-1}$ is positive semi-definite, hence using the order structure on symmetric $p\times p$ matrices,
\[
	Cov_{\theta} \tilde{\theta} > \frac{1}{n} I(\theta)^{-1}, \forall \theta \in\intr(\Theta)
\] 

\begin{note}
	Here, > is in the sense of our order structure: for symmetric $n\times n$ matrices $A$ and $B$,  $A>B$ if $A-B$ is positive semi-definite.
\end{note}

\begin{eg}
	Consider $X \sim N(\theta, \Sigma)$, where $\theta = \begin{pmatrix} \theta_1 \\ \theta_2 \end{pmatrix} \in \R^{2}, \Sigma$ is positive definite $[n=1] $.

	Case 1: Suppose one wants to estimate $\theta_1$, and  $\theta_2$ is known. Then (see example sheet), one finds the Fisher information $I_{n}(\theta)$ of this one-dimensional statistical model $\{f(\cdot , \theta_1), \Theta_1 \in  \R\} $, with CRLB  $I_1(\theta)^{-1}$.

	Case 2: Now suppose $\theta_2$ is unknown. Then one can compute the $2\times 2$ information matrix $I_2(\theta)$, and the CRLB for estimating $\theta_1$ is, with $\Phi(\theta) = \theta_1$, 
	\[
		\frac{\partial\Phi}{\partial\theta}^{T} I(\theta)^{-1} \frac{\partial\Phi}{\partial\theta}
	\] 
	One can see $CRLB(1) < CRLB(2) $ unless  $\Sigma$ is diagonal
\end{eg}

\section{Asymptotic Theory for MLEs}

We will investigate the large sample performance of estimators $\tilde{\theta}(X_1, \ldots, X_n)$, specifically the MLE $\hat{\theta}_{MLE}$, as $n\to \infty$. The main goal will be to prove
\begin{align*}
	\hat{\theta}_{MLE} \underset{n\to\infty}{\approx} N\left(\theta, \frac{1}{n} I(\theta)^{-1}\right), \forall \theta \in  \Theta
,\end{align*} 
 in a sense to be made precise later.

\subsection*{Stochastic Convergence: Concepts and Facts}

\begin{defn}
	Let $(X_n: n \in \N), X$ be random vectors in $\R^{k}$, defined on some probability space $(\Omega, \mathcal{A}, \mathbb{P} )$

		1) We say $X_n\to X$ almost surely, $X_n \stackrel{a.s}{\to} X \text{ as } n\to \infty$, if
		\begin{align*}
			\mathbb{P}\left(\omega \in \Omega : \|X_n (\omega) - X(\omega)\| \to 0 \text{ as } n\to \infty \right) &= 1 \\
			\text{ i.e }\mathbb{P}\left(\|X_n - X\| \to  0 \text{ as } n\to \infty\right) &=1
		\end{align*}

		2) We say $X_n \to  X$ in probability, $X_n \to ^{P} X$ as $n\to \infty$, if $\forall \epsilon > 0$,
		\begin{align*}
			\mathbb{P}\left( \|X_n - X\| > \epsilon\right) \to 0 \text{ as } n \to \infty
		\end{align*}
		
\end{defn}

\begin{remark}
	The choice of norm on $\R^{k}$ is irrelevant. Also, one shows (see example sheet) that $X_n \stackrel{a.s}{\to}^{P} X$ as $n\to \infty$ is equivalent to $X_{n_{j}} \stackrel{a.s}{\to } ^{P} X_j $ as $n\to \infty \forall j = 1, \ldots, k$.
\end{remark}

\begin{defn}
	We say $X_n \to X$ in distribution (or in law), writing $X_n \to ^{d} X$ as $n \to \infty$, if
	\begin{align*}
		\mathbb{P}\left(X_n \le t\right) \to \mathbb{P}\left( X \le t \right) \forall t \in \R^{k}
	,\end{align*}
	for which $t \mapsto \mathbb{P}\left( X \le t \right) $ is continuous.
\end{defn}

Recall, $\mathbb{P}( Z\le t) := \mathbb{P}\left( Z_1 \le t_1, \ldots, Z_k \le t_k \right).$

The following facts on stochastic convergence will be frequently used, and can be proved with measure theory.

\begin{prop}
	1) $X_n \underset{n\to \infty}{\overset{a.s}{\to}} X \implies X_n \underset{n\to \infty}{\to^{P}} X \implies X_n \underset{n\to \infty}{\to^{d}} X$, but any converse is false in general.

	2) (Continuous Mapping Theorem) If $X_n, X$ take values in $\chi \subseteq \R^{k} $, and $g:\chi \to \R^{d}$ is continuous, then $X_n \underset{n\to \infty}{\to} X \text{ a.s/P/d } \implies g(X_n) \underset{n \to  \infty}{\to} g(X) \text{ a.s/P/d respectively} $

	3) (Slutsky's Lemma) Suppose $X_n \underset{n\to \infty}{\to^{d}} X, Y_n \underset{n\to \infty}{\to^{d}} c$,  $c$ constant (non-stochastic), then
	 \begin{align*}
		 Y_n &\to ^{P} c, n\to \infty  \\
		 X_n + Y_n &\to^{d} X + c, n\to \infty  \\
		 X_nY_n &\to^{d} cX \\
		 \frac{X_n}{Y_n} &\to ^{d} \frac{X}{c} \text{, provided $c \neq 0$, as $n\to \infty$ } \\
		 \text{If $(A_n)_{ij}$ are random matrices s.t. $(A_n)_{ij} \to^{P} A_{ij}$, then} \\
		 A_n X_n &\to^{d} AX \text{ as } n\to \infty
	 \end{align*}
	 4) If $X_n \to^{d} X \text{ as } n \to \infty$, then $X_n$ is stochastically bounded $\left[ = O_P(1) \right]$, that is $\forall \epsilon > 0, \exists M_{\epsilon}$ s.t. for all sufficiently large  $n, \mathbb{P}\left( \|X_n\| > M_{\epsilon} \right) < \epsilon$ 

\end{prop}

\section*{Lecture 5}

\section{Law of Large Numbers (LLN) and Central Limit Theorem (CLT)}

Consider $X_1,\ldots,X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$. This sequence can be realised as the coordinate projections of the infinite product probability space $\left( \Omega, \mathcal{A}, \P \right) = \left( \R^{\N}, \mathcal{B}^{\N}, P^{\N} \right)$, where $P^{\N} = \otimes_{i=1}^{\infty} \P$.

Under this product space, we can make some simultaneous statements about the stochastic behaviour of our $X_i$.

\begin{thm}
	(Weak Law of Large Numbers)

	If $var(X) < \infty$, we have that 
	\begin{align*}
		\mathbb{P}\left( \left| \frac{1}{n}\sum_{i=1}^{n} (X_{i} - \E(X)) \right| > \epsilon \right) &\le \frac{var\left( \overline{X} - \E(X) \right)}{n\epsilon^2} \text{ by Chebyshev} \\ 
		&= \frac{var(X)}{n\epsilon^2} \text{ as $X_i$ iid}\\
		&\to 0 \text{ as  } n \to \infty
	\end{align*}
\end{thm}	

\begin{thm}
	(Strong Law of Large Numbers)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$ such that $\E\left(\|X\|  \right)  < \infty$. Then $\overline{X} \overset{a.s}{\to}^{P} \E(X) \text{ as } n \to \infty$
\end{thm}

This is harder to prove than the Weak version, so we don't.

The stochastic fluctuations of $\overline{X}$ about $\E(X)$ are of order $\frac{1}{\sqrt{n} }$ and as long as $var(X) < \infty$, \textit{always} look normally distributed.

\begin{thm}
	(Univariate Central Limit Theorem)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R$, with $var(X) = \sigma^2 < \infty$. Then

	\[
		\sqrt{n} (\overline{X} - \E(X)) \underset{n \to  \infty}{\to^{d}} N(0, \sigma^2)
	\] 
\end{thm}

To give a multivariate version, we recall that $X \in \R^{k}$ is multivariate normal if $\forall t \in \R^{k}$, $t^{T}X$ is univariate normal, and write $X\sim N_{\mu}(\mu, \Sigma)$, where $\mu$ is our mean  $\E(X)$, and  $\Sigma $ is our covariance matrix $var(X)$.

In fact, X is uniquely characterised as the random variable on  $\R^{k}$ such that $t ^{T}X \sim N(t ^{T}\mu, t ^{T}\Sigma t)$ for all $t \in \R^{k}$.

If $\Sigma$ is invertible, then X has pdf
\[
	f(x) = \frac{1}{\sqrt{(2\pi)^{n}|\Delta \Sigma|} } \exp\left( - \frac{1}{2} (x-\mu)^{T} \Sigma^{-1} (x - \mu) \right) 
.\] 

If $A \in \R^{d\times k}$ and $b \in \R^{d}$, then
\[
	AX + b \sim N_{d}\left( A\mu + b, A\Sigma A^{T} \right) 
\] 

Furthermore, if $A_{n} \to^{P} A$ are random matrices, and $X_n \to ^{d} N_{k}\left( \mu, \Sigma \right) $, then $A_n X_n \to ^{d} N_{d}\left( A\mu, A\Sigma A^{T} \right) $.

\begin{thm}
	(Multivariate Central Limit Theorem)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R^{k}$ with $var(X) = \Sigma$ positive definite. Then

	 \[
		 \sqrt{n} \left( \overline{X} - \E(X) \right) \underset{n \to \infty}{\to ^{d}} N_{k}(0, \Sigma)
	\] 
\end{thm}

From this, and the SLLN, we can bound in probability the deviations.

\begin{defn}
	For a sequences $Y_1, \ldots, Y_n$ and $c_1, \ldots, c_n \in \R\setminus\{0\}$, we define 
	 \[
		 Y_n = O_{P}(c_n)
	\]
	if
	\[
		\forall \epsilon > 0, \exists M,N > 0 \text{ s.t } \mathbb{P}\left( \left| \frac{Y_n}{c_n} \right| > M \right) < \epsilon \text{  } \forall n > N
	\] (By Prokhorov's Theorem) 
\end{defn}

\begin{corol}
	\[
		\overline{X} - \E(X) = O_P \left(\frac{1}{\sqrt{n} }\right)
	\] 
\end{corol}

\begin{eg} (Confidence Intervals)

	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X \sim P$ on $\R$ with mean  $\mu_{0}$ and variance $\sigma^2$.

	Define  $\mathcal{C}_n = \{\mu \in \R : |\overline{X} - \mu| \le \frac{\sigma z_{\alpha}}{\sqrt{n} }\} $, where $\mathbb{P}\left( |Z| \le z_{\alpha} \right) = 1 - \alpha $ for $Z\sim N(0,1)$ as our 'confidence region'

	\begin{align*}
		\mathbb{P}\left( \mu \in \mathcal{C}_n \right) &= \mathbb{P}\left( |\overline{X} - \mu_0| \le \frac{\sigma z_{\alpha}}{\sqrt{n} }  \right) \\
		&= \mathbb{P}\left( |\overline{X} - \E(X)| < \frac{\sigma z_{\alpha}}{\sqrt{n} } \right) \\
		&= \mathbb{P}\left( \sqrt{n} \left| \frac{1}{n} \sum_{i=1}^{n} \frac{X_i - \E(X_i)}{\sigma} \right| \le z_{\alpha} \right) \\
		&\underset{n \to \infty}{\to } \mathbb{P}\left( |Z| \le z_{\alpha} \right) \text{ by CLT} \\
		&= 1- \alpha
	\end{align*}

	where we have used the continuous mapping theorem, and because $z_{\alpha}$ is a continuity point of the distribution of $Z$. Therefore $\mathcal{C}_n$ is an asymptotic confidence region with confidence level (or coverage) $1-\alpha$. (Alternatively of size/significance level  $\alpha$).

	When  $\sigma$ is unknown, we can replace it in $\mathcal{C}_n$ by $S_n$, where  $S_{n}^2 = \frac{1}{n-1} \sum_{i=1}^{n}(X_i - \overline{X})^2$, and the same conclusion follows, using the asymptotic distribution of the t-distribution,
	\[
		t_n := \frac{\sqrt{n} (\overline{X} - \E(X)}{S_n} \underset{n \to \infty}{\to ^{d}} N(0,1)
	\] 
\end{eg}

\section*{Lecture 6}

\section{Consistency of MLEs}

\begin{defn}
	Let $X_1, \ldots, X_n \stackrel{iid}{\sim} X$ form a statistical model $\{P_{\theta} : \theta \in \Theta\, \Theta \subseteq \R^{p}\}$. Then we say that an estimator $\tilde{\theta}_{n} = \tilde{\theta}(x_1, \ldots, x_n)$ is consistent (for that model) if $\tilde{\theta}_n \underset{n\to \infty}{\to} \theta$ in $(P_\theta^{\N})$-probability, $\forall \theta \in \theta$.
\end{defn}

\begin{assumption}
	Suppose a statistical model $\{f\left( \cdot , \theta : \theta \in \Theta \right) \}, \Theta \in \R^{p}$ of pdf/pmf on $\chi \subseteq \R^{d}$ satisfies the following conditions:

\begin{enumerate}
	\item $f(x,0) > 0 \forall (x, \theta) \in  (\chi, \Theta)$
	\item $\int_{\chi} f(x,\theta) dx = 1 \forall \theta \in \Theta$
	\item The map $\theta \mapsto f(x, \theta)$ is continuous $\forall x \in  \chi$
	\item  $\Theta \in \R^{p}$ is compact
	\item $\theta = \theta' \iff f(\cdot , \theta) = f(\cdot , \theta'), \forall \theta, \theta' \in \theta $
	\item $\E_{\theta} sup_{\theta \in \Theta} |\log f(x, \theta)| < \infty$
\end{enumerate}

\end{assumption}

\begin{remark}
	\begin{enumerate}
		\item The above conditions justify the application of Jensen's inequality in Observation (1) of section 3 (Information Geometry) - in particular the map $ \theta \mapsto l(\theta) = \E_{\theta_0} \log f(X, \theta)$ is maximised uniquely at $\theta_0 \in \Theta$.
		\item Using the dominated convergence theorem, one can integrate the limit
			\[
				lim_{\eta \to  0} |\log f(X, \theta + \eta) - \log f(X, \theta)| = 0
			\] wrt $P_\theta$, and conclude that also the map $\theta \mapsto f(\theta)$ is continuous under assumption A.
	\end{enumerate}
\end{remark}

\begin{thm}
	Suppose that the statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies assumption A. Then an MLE exists, and any MLE is consistent.
\end{thm}

\begin{proof}
	The map $\overline{l_{n}}(\theta) = \frac{1}{n} \sum_{i=1}^{n} \log f(X_i, \theta)$ is continuous on the compact set $\Theta \in \R^{p}$, so by the Heine-Borel theorem, $\overline{l_n}$ obtains a maximum on $\Theta$, hence a MLE  $\hat{\theta}_n $ exists.

	Now, let  $\hat{\theta}_n$ be any MLE, and for a true (arbitrary) value $\theta_0 \in \Theta$ we now prove that $\hat{\theta}_n \underset{n\to \infty}{\to ^{P}} \theta_0$ (in $P_{\theta_0}^{\N}$ probability). The idea is that maximisers $\hat{\theta}_n$ of $\overline{l_n}$ over $\Theta$ should converge to the unique maximiser  $\theta_0$ of $l$ over $\Theta$, since  $\overline{l_n}(\theta) \underset{n\to \infty}{\to ^{p}} l(\theta)$ by the LLN, for all $\theta \in  \Theta$ pointwise.

	This is generally false unless one has uniform convergence.
	\[
		(\ast) sup_{\theta \in \Theta} |\overline{l_n}(\theta) - l(\theta)| \underset{n\to \infty}{\to ^{P}} 0 \text{ [see Ex sheet for counterexample] }
	\] 

	We show later that $(\ast)$ indeed holds under the maintained hypothesis.

	Define for any $\epsilon > 0$ 
	\[
		\Theta_{\epsilon} = \{\theta \in  \Theta : \|\theta - \theta_0 \ge \epsilon\|\} 
	\] 
	which is again a compact subset of $\R^{p}$ intersection of closed and compact). Thus the function $l(\theta)$ attains its bounds on  $\Theta_{\epsilon}$, so
	\[
		c(\epsilon) = sup_{\theta \in \Theta_{\epsilon}} l(\theta) = l(\overline{\theta}_{\epsilon} \in \Theta_{\epsilon}) < l(\theta_0) \text{ since l is maximised uniquely at } \theta_0
	\] 
	Thus we can choose $\delta(\epsilon)$ small enough such that 
	 \[
	 c(\epsilon) + \delta(\epsilon) < l(\theta_0) - \delta(\epsilon) \tag{\dag}
	\]

	Now, $sup_{\theta \in  \Theta_{\epsilon}} \overline{l_n} = sup_{\theta \in \Theta_{\epsilon}} l(\theta) + (\overline{l_n}(\theta) - l(\theta) \le sup_{\theta \in \Theta_{\epsilon}} l(\theta) + sup_{\theta \in \Theta_{\epsilon}} |\overline{l_n} (\theta) - l(\theta)|$

	Next, define events (subsets of $\R^{\N}$ supporting $(X_1, X_2,\ldots)$) 
	\[
		A_n(\epsilon) = \{sup_{\theta \in \Theta_{\epsilon}} |\overline{l_n}(\theta) - l(\theta) | \le  \delta(\epsilon)\} 
	\] 

	On these events, we have 

	\[
		sup_{\theta \in \Theta_{\epsilon}} \overline{l_n} (\theta) \le  c(\epsilon) + \delta(\epsilon) < l(\theta_0) - \delta(\epsilon) \le \overline{l_n}(\theta_{0}) \text{, since on $A_n(\epsilon)$ we also have } |l(\theta_0) - \overline{l_n}(\theta)| < \delta(\epsilon)
	\] 

	Thus if we assume $\hat{\theta}_n \in \Theta_{\epsilon}$, then by what precedes

	\[
		\overline{l_n}(\hat{\theta_n} \le  sup_{\theta \in \Theta_{\epsilon}} \overline{l_n}(\theta) < l(\theta_0) \text{ on } A_n(\epsilon)
	\]

	This is a contradiction to $\hat{\theta}_n$ being a maximiser. Therefore on $A_n(\epsilon)$ we must have  $\hat{\theta}_n \in  \Theta_{\epsilon}^{c}$, or in other words

	\[
		A_n(\epsilon) \subseteq \{\| \hat{\theta}_n - \theta_0\| < \epsilon\} 
	.\] 
	Now, by $(\ast)$,  $\mathbb{P}\left( A_n(\epsilon) \right) \to 1$. We conclude that
	 \begin{align*}
		&\mathbb{P}\left( \| \hat{\theta}_n - \theta_0 \| < \epsilon  \right) \underset{n\to \infty}{\to } \text{ or } \\
		&\mathbb{P}\left( \| \hat{\theta}_n - \theta_0\| < \epsilon \right) \underset{n\to \infty}{\to }
	\end{align*}

	Since $\epsilon$ is non arbitrary,  $\hat{\theta}_n \underset{n\to \infty}{\to^{P} } \theta_0$, and the proof is complete modulo the verification of $(\ast)$.
\end{proof}

\begin{remark}
	The previous proof works as well if $(\Theta, d)$ is any compact metric space, and if continuity in assumption A is for the metric d.
\end{remark}

\section{Lecture 7}

To verify ($\ast$), we now make the following (non-examinable) digression:

For a (meas.) $\chi \in \R^{d}$ and (meas.) $h: \chi \to \R$, and let $X_1, \ldots, X_n \stackrel{iid}{\sim} X$ in $\chi$ with law $P$. Then the $h(X_i)$'s are also iid, and if $\E |h(X)| < \infty$ ($\E = \E_p$), then by the SLLN,
\[
	\frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \underset{n \to \infty}{\to^{a.s}} 0 
\] 

Next, let $h_1, \ldots, h_N$ be a finite collection of such functions. Then
\[
	\mathbb{P}\left( |\frac{1}{n} \sum_{i=1}^{n} h_j (X_i) - \E h_j (X)  | \underset{n \to \infty}{\to} 0 \right) \equiv \mathbb{P}\left( A_j \right) = 1.
\] 
Moreover,
\[
	\mathbb{P}\left( max_{j=1,\ldots,N} |\frac{1}{n} \sum_{i=1}^{n} h_j (X_i) - \E h_j (X)  | \underset{n \to \infty}{\to} 0 \right) = \mathbb{P}\left( \bigcap_{j=1}^{N} A_j  \right) = 1,
\] since
\[
	\mathbb{P}\left( (\bigcap_{j=1}^{N}A_j)^{c}  \right) = \mathbb{P}\left( \bigcup_{j=1}^{N} A_j^{c}  \right) \le \sum_{j=1}^{N}\mathbb{P}\left( A_j^{c} \right) = 0.
\] 

To transfer to an infinite collection of h's, let us say that a family of brackets $[\underline{h_j}, \overline{h_j}], \underline{h_j}, \overline{h_j} : \chi \to \R, j=1,\ldots,N$, covers a class $\mathcal{H}$ of maps on $\chi$ if  $\forall h \in  \mathcal{H}, \exists j$ s.t $\underline{h_j} \le h(x) \le \overline{h^{j}} \forall x \in \chi$.

\begin{prop}
	Suppose that $\forall \epsilon > 0$ there exist brackets $[\underline{h_j}, \overline{h_j}], j=1,\ldots,N(\epsilon)$ covering $\mathcal{H}$ and such that
	\begin{enumerate}
		\item $\E |\underline{h_j}(x) | < \infty, \E|\overline{h_j}(x)| < \infty$
		\item $\E |\overline{h_j}(x) - \underline{h_j}(x)| < \epsilon$
	\end{enumerate}
	Then
	\[
		sup_{h \in \mathcal{H}} \left| \frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \right| \underset{n \to \infty}{\to^{a.s} }
	\] 
\end{prop}

\begin{proof}[Non-examinable]
	Let $\epsilon = \frac{1}{m}$, where $m \in \N$ is arbitrary. Then take $N(\frac{\epsilon}{3})$-many brackets covering $\mathcal{H}$, and note that by the preceding argument, we have
	\[
		\mathbb{P}\left( max_{j=1,\ldots,N(\frac{\epsilon}{3})} \left| \frac{1}{n}\sum_{i=1}^{n} \underline{h_j} (X_i) - \E \underline{h_j} (X) \right| \le  \frac{\epsilon}{3}, \forall n \ge n_o (\epsilon) \right) = \mathbb{P}\left( A_{\epsilon} \right) = 1
	,\] and equivalently for $\overline{h_j}$

	Now, pick $h \in  \mathcal{H}$ arbitrary, and write for the respective bracket $\underline{h_j}, \overline{h_j} \ni h$ 
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^{n}h(X_i) - \E h(X) &\le \frac{1}{n}\sum_{i=1}^{n} \overline{h_j}(X_i) - \E \overline{h_j}(X) + \E \overline{h_j} (X) - \E h(X) \\
		&\le \frac{\epsilon}{3} + \E |\overline{h_j}(X) - \underline{h_j} (X) |  \\
		&\le \frac{2\epsilon}{3}
	\end{align*}
	Likewise, we get
	\begin{align*}
		\frac{1}{n} \sum_{i=1}^{n}h(X_i) - \E h(X) &\ge \frac{1}{n}\sum_{i=1}^{n} \underline{h_j}(X_i) - \E \underline{h_j}(X) + \E \underline{h_j} (X) - \E h(X) \\
		&\ge - \frac{2\epsilon}{3}
	\end{align*}

	Therefore on the event $A = \bigcap_{m} A_m $ we have
	\[
		\left| \frac{1}{n} \sum_{i=1}^{n} h(X_i) - \E h(X) \right| < \frac{2\epsilon}{3} < \epsilon
	,\] and since
	\[
	\mathbb{P}\left( A^{c} \right) \le \sum_{m=1}^{\infty} \mathbb{P}\left( A_m^{c} \right) = 0
	,\] the proof is complete.
\end{proof}

From this proposition, we deduce

\begin{prop}
	Let $\chi \subseteq \R^{d}, \Theta \in \R^{p}$ compact, and suppose $\theta \mapsto q(x, \theta)$ is continuous  $\forall x$ (and x measurable $\forall \theta$), and that  $\E sup_{\theta \in \Theta} |q(X, \theta)| < \infty$. If $X_1, \ldots, X_n \stackrel{iid}{\sim} X$, then
	\[
		sup_{\theta \in \Theta} \left| \frac{1}{n} \sum_{i=1}^{n} q(X_i, \theta) - \E q(X, \theta) \right| \underset{n \to  \infty}{\to ^{a.s}}
	\]
\end{prop}

\begin{remark}
	By choosing $q(X, \theta) = \log f(X,\theta)$, we verify $(\ast)$ in the proof of the last theorem.
	
\end{remark}

\begin{remark}
	The condition that the expectation of the supremum of $q$ over $\theta$ is finite can be seen to be necessary as $\E \|Z\|< \infty$ in the LLN for $Z_1, \ldots, Z_n$ iid in the space $\mathcal{C}(\Theta)$ of continuous functions on the compact space $\Theta$.
\end{remark}

\section{Asymptotic Distribution of MLEs}

\begin{defn}
	We say that an estimator $\tilde{\theta}_n$ is asymptotically efficient in a regular statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ if $lim_{n\to \infty} n Var_{\theta} (\tilde{\theta)} = I(\theta)^{-1}, \forall \theta \in\intr\Theta$.
\end{defn}

\newpage

\begin{assumption}
	Consider a model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}, \Theta \in \R^{p}$ of pdf/pmfs on $\chi \subseteq \R^{d}$ such that $f(x, \theta) > 0$ for all  $x \in \chi$ and all $\theta \in \Theta$, and such that $\int_{\chi} f(x, \theta) dx = 1$ for every $\theta \in  \Theta$.

	Let $\theta_0$ be a fixed ('true') value, and assume

	\begin{enumerate}
		\item $\theta_0 \in\intr\Theta$
		\item There exists an open set $U$ satisfying $\theta_0 \in U \subseteq \Theta$ such that $f(x, \theta)$ is, for every $x\in \chi$, twice continuously differentiable wrt $\theta$ in $U$
		\item the  $p\times p$ matrix $\E_{\theta_0} \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}}  $ is non singular, and 
			\[
				\E_{\theta_0} \left\|\frac{\partial \log f(X, \theta_0)}{\partial\theta} \right\|^2 < \infty
			\]
		\item There exists a compact ball $K \subset U$ (with non-empty interior) centered at $\theta_0$ such that
			\[
			\E_{\theta_0} \left\| \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}} \right\| < \infty
			,\]
			\[
				\int_{\chi} sup_{\theta \in K} \left\| \frac{\partial f(x, \theta)}{\partial\theta}\right\| dx < \infty \text{ and } \int_{\chi} sup_{\theta \in  K} \left\| \frac{\partial^2 \log f(X, \theta_0)}{\partial \theta \partial \theta^{T}} \right\| dx < \infty
			\]
			\item Suppose the MLE $\hat{\theta}_n$ in the model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ based on the sample $X_1, \ldots, X_n$ exists and is consistent, i.e. $\hat{\theta}_n \underset{n\to \infty}{\to ^{P_{\theta_{0}}}} \theta_0$.
	\end{enumerate}
\end{assumption}

\begin{thm}
	Suppose a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ is regular in the sense that it satisfies the conditions B on the handout. Then, if $\hat{\theta}_n$ is the MLE, based on $X_1, \ldots, X_n \stackrel{iid}{\sim}$ from the model, we have $\sqrt{n} (\hat{\theta}_n - \theta_0) \underset{n \to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$
\end{thm}

\begin{idea}[p=1]
	For $\hat{\theta}$, we must have that for $l_n(\theta) = \sum_{i=1}^{n} \log f(X_i, \theta)$
	\[
		0 = l_{n}' (\hat{\theta}) = l_{n}' (\theta_0) + l _{n}''(\overline{\theta}_n (\hat{\theta}_n - \theta_0) \text{ (MVT) }
	,\]
	so
	\[
		\sqrt{n} (\hat{\theta} - \theta_0) = \frac{\sqrt{n} l_{n}' (\theta_0)}{- \frac{1}{n} l_{n}''(\overline{\theta}_n)} = \frac{\sqrt{n} \sum_{i=1}^{n} \frac{d}{d\theta} \log f(X_i, \theta) - \E(\cdot , 1) }{- \frac{1}{n} \sum_{i=1}^{n} \frac{d^2}{d\theta ^2} \log f(X_i, \overline{\theta}_n)} \underset{n\to \infty}{\to^{d}}  \frac{N(0, I(\theta_0)}{I(\theta_0)} = N(0, I(\theta_0)^{-1})
	\] 
\end{idea}

\section*{Lecture 8}
		
\begin{lemma}
	Observations 2 and 3 from the Information Geometry section are valid.		
\end{lemma}
	
\begin{proof}
	Apply the dominated convergence theorem and Assumption B.
\end{proof}


\begin{proof}(8.2)
        Here, $\P \equiv P_{\theta_0}^{\N}, \E \equiv \E_{\theta_0}$.

	In proving convergence in distribution (say $Z_n \to ^{d} Z$), it suffices to restrict to any sequence $(E_n)$ of events (in $\R^{N}$ ) s.t. $\mathbb{P}\left( E_N \right) \to  1$. Indeed,
	\[
		|\mathbb{P}\left( Z_n \le  t \right) - \mathbb{P}\left( Z_n \le t  \mid  E_n \right) | \le \mathbb{P}\left( (E_n^{c} \right) \to 0
	.\]

	By consistency, $\hat{\theta}_n \to ^{P} \theta_0$, hence the events $E_n = \{\hat{\theta}_n \in K\} $ have probability tending to 1, and we restrict to this event in what follows. Therefore we must have
	\begin{align*}
		0 &= \begin{pmatrix} \frac{\partial}{\partial\theta_1} \overline{l}_n (\hat{\theta}_n) \\ \vdots \\ \frac{\partial}{\partial\theta_p} \overline{l}_n (\hat{\theta}_n) \end{pmatrix} 
	\end{align*} 
	Where we recall $\overline{l}_n(\theta) = \frac{1}{n} \sum_{i=1}^{n} \log f(X_i, \theta)$.

	For any map $h: U \to \R$, we can apply the mean value theorem along the line segment $\{t \hat{\theta}_n + (1-t) \hat{\theta}_0 : 0 < t < 1\} $ connecting $\hat{\theta}_n $ and $\theta_0$, and write
	\[
		h(\hat{\theta}_n) = h(\theta_0) + \frac{\partial h}{\partial \theta}^{T} \mid_{\theta = \overline{\theta}} (\hat{\theta}_n - \theta_0) 
	\]
	here $\overline{\theta} = \overline{\theta}(h)$ is some mean value on that line segment.

	Second derivatives of $\overline{l}_n(\theta)$ are differentials of the map $u \mapsto \frac{\partial}{\partial\theta} \ln(\theta)  \mid _{\theta = u}$, and hence applying what precedes p-times to the vector entries $\frac{\partial}{\partial\theta_j} \overline{l}_n(\hat{\theta}_k)$, we obtain

	\begin{align*}
		0 &= \begin{pmatrix} \frac{\partial}{\partial\theta_j} \\ \vdots \end{pmatrix} \\
		&= \begin{pmatrix} \frac{\partial}{\partial\theta_j} \overline{l}_n(\theta_0) \\ \vdots \end{pmatrix} + \begin{pmatrix} & \vdots & \\ \cdots & \frac{\partial^2}{\partial\theta_i \partial\theta_j} \overline{l}_n(\overline{\theta}_{ij}) & \cdots \\ & \vdots &\end{pmatrix} (\hat{\theta}_n - \theta_0) \\
		&:= \overline{A}_n (\hat{\theta}_n - \theta_0)
	\end{align*}
	where $\overline{\theta}_{ij}$ is the $p\times 1$ vector arising from the $j^{th}$ application of the MVT.

	We will show
	\[
		\overline{A}_n \underset{n\to \infty}{\to ^{P}} -I(\theta_0) \tag{\dagger}
	\]
	and in particular this implies convergence of $\|\overline{A}_n + I(\theta_0)\| \to ^{P} 0$ under the operator norm.

	Hence since $I(\theta_0)$ is non-singular, so is $\overline{A}_n$ on events of probability converging to $1$, and since we can rewrite
	\begin{align*}
		\sqrt{n}(\hat{\theta}_n - \theta_0) &= (-\overline{A}_n)^{-1} \sqrt{n} \frac{\partial}{\partial\theta} \overline{l}_n(\theta_0)   
	 \end{align*}
	 and the theorem follows from $(\dagger)$, Slutsky's Lemma, and since

	  \begin{align*}
		  \sqrt{n}\frac{\partial}{\partial\theta}\overline{l}_n(\theta) &= \frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left( \frac{\partial}{\partial\theta} \log f(X_i, \theta_0) - \E \frac{\partial}{\partial\theta} \log f(X_i,\theta_0) \right) \\
		  & \underset{CLT}{\to ^{d}} N(0, I_{\theta_0})) \text{ as $n\to \infty$, applying Observation 2 to the second term above. } 
	 \end{align*}

	 To verify $(\dagger)$, it suffices (see Ex. sheet) to check convergence in probability of $\overline{A}_{n_{jk}}\to (-I_(\theta_0))_{jk}$.

	 Now, we write
	 \begin{align*}
		 \overline{A}_{n_{jk}} = &\frac{1}{n} \sum_{i=1}^{n}\frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) - \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) \\
		 &+ \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \overline{\theta}_{(j)}) - \E \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(X_i, \theta_0) + (-I(\theta_0)_{jk}) \text{ ( by Obs. 3) } 
	 \end{align*}
	 We denote this by $(1) + (2) - I(\theta_0)_{jk}$
	 For $(1)$, notice that $\overline{\theta}_{(j)} \in K$, and hence with $q(x,\theta) = \frac{\partial^2}{\partial\theta_j \partial\theta_k} \log f(x, \theta)$,

	 \[
		 |(1)| = |\frac{1}{n} \sum q(X_i, \overline{\theta}_j) - \E q(X, \overline{\theta}_{(j)})| \le sup_{\theta \in K} |\frac{1}{n} \sum q(X_i, \overline{\theta}_j) - \E q(X, \overline{\theta}_{(j)})| \underset{n\to \infty}{\to ^{P}} 0 \text{ by the uniform LLN }
	 \] 

	 For $(2)$, we notice that  $\hat{\theta}_n \to ^{P} \theta_0 \implies \overline{\theta}_{(j)} \to ^{P} \theta_0 \text{ as } n\to \infty \forall j$, and since $\theta \mapsto \E q(X,\theta)$ is continuous, the continuous mapping theorem implies that 
	 \[
		 (2) = \E q(X, \overline{\theta}_j) - \E q(X, \theta_0) \underset{n\to \infty}{\to ^{P}} 0
	 ,\]
	 completing the proof of $(\dagger)$

\end{proof}

\section*{Lecture 9}

\begin{remark}
	\begin{enumerate}
		\item The assumption that $\theta \mapsto f(x,\theta)$ is $C^2$ can be relaxed to the existence of first derivatives (weak ones) by more involved proof methods (Le Cam Theory, see van der Vaart (1998)), including in particular the Laplace distribution (where one may show $I_n(\theta) = n)$. However, this cannot be weakened further, and for non-smooth parameterisation, the asymptotic theory for MLEs may be different as the example of $U(0, \theta), \theta \in [0,\infty)$ shows (see Ex. sheet).

		\item If the 'true' value $\theta_0$ lies at the boundary of $\Theta$, then the MLE is also not asymptotically normal (Ex. sheet $N(\theta, 1), \theta \in \Theta = [0, \infty) $)

		\item An asymptotic version of the Cramer-Rao lower bound can also be proved (see Le Cam theory), but it requires a restriction to 'regular' or 'uniformly consistent' (instead of unbiased) estimators to claim asymptotic efficiency. Some restriction on the class of estimators is indeed necessary, as the following example (due to Hodges) shows:
			Consider a statistical model $\{P_{\theta} : \theta \in \Theta \}, \Theta \subseteq \R, 0 \in  \Theta$, s.t $\sqrt{n}(\hat{\theta}_{MLE} - \theta) \underset{n \to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$ $ \forall \theta \in\intr\Theta$. [Recall that this implies that $\sqrt{n}(\hat{\theta} - \theta) $ is stochastically bounded, i.e, $\forall \epsilon > 0 \exists M_{\epsilon}$ st $\mathbb{P}\left( |\hat{\theta} - \theta| > \frac{M_{\epsilon}}{\sqrt{n} } \right) < \epsilon$, in particular $\hat{\theta} \underset{n\to \infty}{\to ^{P}} \theta$ ]

			Define  \begin{align*}
				\tilde{\theta}=\tilde{\theta_{Hodges}} =  \mid &\hat{\theta} \text{ if } |\hat{\theta}| > n^{- \frac{1}{4}} \G\
				&0 \text{ if } |\hat{\theta}| < n^{- \frac{1}{4}}
		\end{align*} 

		Now for $\theta \neq 0$ and under $P_\theta$,

		 \begin{align*}
			 \mathbb{P}\left( \tilde{\theta} \neq \hat{\theta} \right) &= \mathbb{P}\left( |\hat{\theta}| \le n^{- \frac{1}{4}} \right) \\
			 &= \mathbb{P}\left( \left( |\hat{\theta} - \theta + \theta| \le n^{- \frac{1}{4}} \right)  \right) \\
			 &\le \mathbb{P}\left( |\hat{\theta} - \theta| \ge \theta - n^{- \frac{1}{4}} \right) \\
			 &\le \mathbb{P}\left( |\hat{\theta} - \theta| > \frac{1}{2}|\theta| \right) \text{ for large enough n } \\
			 &\underset{n\to \infty}{\to } 0 \text{ since  } \hat{\theta} \to ^{P} \theta, |\theta|\neq 0
		 \end{align*}

		 So for such $\theta$ we thus have $\sqrt{n} (\tilde{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0, I(\theta)^{-1})$.

		 Next, if $\theta = 0$, we have under  $P_0$

		 \begin{align*}
		 	\mathbb{P}\left( \tilde{\theta} \neq 0 \right) &= \mathbb{P}\left( |\hat{\theta}| \ge n^{- \frac{1}{4}} \right) \\
			&= \mathbb{P}\left( |\hat{\theta} - \theta| > n^{- \frac{1}{4}} \right) \\
			&= \mathbb{P}\left( \sqrt{n}|\hat{\theta} - \theta| > n^{\frac{1}{4}}  \right)
		 \end{align*}

		 So for any $\epsilon$ > 0 and n s.t. $n^{\frac{1}{4}} > M_{\epsilon}$, we have by stochastic boundedness of $\sqrt{n}(\hat{\theta} - \theta) $ that the last probability is less that $\epsilon$. Hence we conclude that under  $P_0$, $\sqrt{n}(\tilde{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0,0)$, and so $\tilde{\theta}$ 'beats' the asymptotic efficiency bound $I(\theta)^{-1}$ at $\theta = 0$.
	\end{enumerate}
\end{remark}

\section{Plug-in MLEs and the Delta method}

Consider estimating a functional $\Phi : \Theta \to \R^{k}$, $\Theta \subseteq \R^{p}$ based on $X_i \stackrel{iid}{\sim} {f\left( \cdot , \theta \right) : \theta \in \Theta }$, where $\hat{\theta}$ is the MLE for $\theta$. One can show that \textit{a} MLE in the model  $\{f\left( \cdot , \phi \right) : \phi = \Phi(\theta) \text{ for some } \theta \in \Theta \}$

The asymptotic normality and efficiency of  $\hat{\theta}$ then implies the same for $\Phi(\hat{\theta})$ as long as $\Phi$ is differentiable.

\begin{thm}(Delta-method)
	
	Suppose $\Phi : \Theta \to \R$ is continuously differentiable at $\theta \in \Theta$ with gradient vector $\frac{\partial\Phi}{\partial\theta}(\theta)$. Suppose further $\hat{\theta}_n$ are random vectors in $\Theta$ s.t. $\sqrt{n}(\hat{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} Z $, where $Z$ is some random vector in  $\R^{p}$. Then $\sqrt{n}(\Phi(\hat{\theta}) - \Phi(\theta)) \underset{n\to \infty}{\to ^{d}} \frac{\partial\Phi}{\partial\theta}(\theta)^{T} Z$
\end{thm}

\begin{proof}
	By the mean value theorem applied to $\Phi$ on the line segment $\{t \hat{\theta} + (1-t)\theta : 0<t<1\} $ we can write for mean value $\overline{\theta}$
	\begin{align*}
		\sqrt{n}(\Phi(\hat{\theta}_n) - \Phi(\theta)) = \frac{\partial\Phi}{\partial\theta}(\overline{\theta}_n)^{T}(\hat{\theta}_n - \theta)
	\end{align*}

	Since $\sqrt{n}(\hat{\theta} - \theta) \to ^{d} Z $, we have in particular $\hat{\theta} \underset{n\to \infty}{\to ^{P}} \theta$ (by stochastic boundedness), so also $\overline{\theta} \underset{n\to \infty}{\to ^{P}} \theta$, and hence by the continuous mapping theorem, we also have $\frac{\partial\Phi}{\partial\theta}(\overline{\theta}_n) \underset{n\to \infty}{\to ^{P}} \frac{\partial\Phi}{\partial\theta} (\theta)$. Hence by Slutsky's Lemma, $\sqrt{n}(\Phi(\hat{\theta}) - \Phi(\theta)) \underset{n\to \infty}{\to ^{d}} \frac{\partial\Phi}{\partial\theta}(\theta)^{T} Z$ 
	
\end{proof}

\begin{remark}
	If $\sqrt{n}( \hat{\theta}_{MLE} - \theta) \to ^{d} N(0, I(\theta)^{-1})$, then what precedes implies that the plug-in MLE satisfies $\sqrt{n} (\Phi(\hat{\theta}_{MLE}) - \Phi(\theta)) \to ^{d} N\left(0, \frac{\partial\Phi}{\partial\theta}(\theta)^{T} I(\theta)^{-1} \frac{\partial\Phi}{\partial\theta}(\theta)\right)$, in particular the asymptotic covariance attains the CRLB for estimating $\Phi(\theta)$
	
\end{remark}

\section{Asymptotic inference with the MLE}

Suppose we want to make inference on $\theta_i$, the $i^{th}$ component of $\theta \in \R^{p}$, from a regular statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$. Then $\theta_i = e_i ^{T} \theta$, and by the last theorem,

\begin{align*}
	\sqrt{n} (\hat{\theta}_i - \theta_i) = \sqrt{n} e_i ^{T} (\hat{\theta} - \theta) \underset{n\to \infty}{\to ^{d}} N(0, e_i ^{T} I_(\theta)^{-1} e_i) = N(0, I(\theta)^{-1}_{ii}  
\end{align*}

This suggests an asymptotic confidence interval (CI) $C_n = \{v \in \R : |\hat{\theta}_i - v| \le  \frac{(I(\theta)^{-1}_{ii})^{\frac{1}{2}}}{\sqrt{n} } z_{\alpha}\} $

\section*{Lecture 10}

Indeed by the continuous mapping theorem,

\begin{align*}
	\mathbb{P}\left( \theta_j \in C_n \right) &= \mathbb{P}\left( \sqrt{n} (I(\theta)^{-1})^{- \frac{1}{2}}_{jj}  |\hat{theta}_{n,j} - \theta_j| \le  z_{\alpha}\right) \\
	&\underset{n\to \infty}{\to } \mathbb{P}\left( |Z| \le z_{\alpha} \right) \\
	&= 1- \alpha
\end{align*}

So $C_n$ is a confidence interval of asymptotic level $1-\alpha$.

In practice,  $I(\theta)$ may still depend on  $\theta$ and hence needs to be replaced by a consistent estimate $\hat{i}_{n} \underset{n\to \infty}{\to ^{P}}  I(\theta)$ (in which case, by Slutsky's lemma, the new CI again has asymptotic coverage level $1-\alpha$).

 \begin{defn}
	The $p\times p$ matrix
	\[
		i_n(\theta) = \frac{1}{n}\sum_{i=1}^{n} \frac{\partial }{\partial \theta} \log f(X_i, \theta) \frac{\partial }{\partial \theta} \log f(X_i, \theta)^{T} 
	\] 
	is called the observed Fisher information (at $\theta$).
	We then defined  $\hat{i}_n = i_n(\hat{\theta}_{MLE}$, an estimator of $I(\theta_0)$.
\end{defn}

\begin{prop}
	Suppose the statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies Assumption B. Then $\hat{i}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0) $.
\end{prop}

\begin{proof}
	Just as when proving $\overline{A}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0)$ in the proof of asymptotic normality of $\hat{\theta}_{MLE}$,  replacing $\frac{\partial^2 }{\partial \theta \partial\theta^{T}}, \overline{\theta}_{ij} $ by $\frac{\partial }{\partial \theta} $ and $\hat{\theta}_{MLE}$ respectively.
\end{proof}

\begin{remark}
	The continuous mapping theorem and invertability of $I(\theta_0)$ then also imply that $\hat{i}_n^{-1} \underset{n\to \infty}{\to ^{P}} I(\theta_0)^{-1}$, since $A \mapsto A^{-1} $ is continuous on $\{A : det A \neq 0\} $.  
\end{remark}

Alternatively, one uses $j_n(\theta) = - \frac{1}{n} \sum_{i=1}^{n} \frac{\partial ^2}{\partial \theta\partial\theta^{T}} \log f(X_i, \theta) $, and estimates $I_n(\theta_0)$ by $\hat{j}_n = j_n(\hat{\theta}_{MLE}$, which as before (and by Observation 3 from information geometry) satisfies again $\hat{j}_n \underset{n\to \infty}{\to ^{P}} I(\theta_0)$).

To make inference on the entire parameter $\theta \in  \Theta \subseteq \R^{p}$, one can use the Wald-Statistic
\begin{align*}
	W_n(\theta) = n (\hat{\theta}_n - \theta)^{T} \hat{i}_n (\hat{\theta}_n - \theta), \theta \in \Theta
,\end{align*}
with $\hat{i}_n$ possibly replaced by $i_n(\theta)$. One shows (Ex sheet), that under $P_{\theta}$ then
 \[
	 W_n(\theta) \underset{n\to \infty}{\to ^{d}} \chi^2_{p} \text{ (Chi-squared with p degrees of freedom)}
,\] 
and this entails that the confidence ellipsoid $C_n = \{\theta \in \R^{p} : W_n(\theta) \le \xi_{\alpha}\} $ has asymptotic coverage $\lim_{n\to \infty} P_{\theta}(\theta \in C_n) = 1- \alpha$ if $\xi_{\alpha}$ are the $1- \alpha$ quantiles of the  $\chi^2_{p}$ distribution.

Consider next a hypothesis testing problem
\begin{align*}
	H_0 : \theta \in \Theta_0 \subset \Theta \text{ vs } H_1 : \theta \in \Theta\setminus\Theta_0
\end{align*}
We wish to construct a test $\Psi_n = \Psi(X_1, \ldots, X_n)$ which takes value $0$ to indicate $H_0$ is true, and takes value $1$ otherwise (to indicate $H_1$ is true). The type-I error of any such test is, for $\theta \in \Theta_0$,

\begin{align*}
	P_{\theta}(\text{reject } H_0) = E_{\theta} \Psi_n
,\end{align*}
and the type-II error, for $\theta \in \Theta\setminus\Theta_0$ is
\begin{align*}
	P_{\theta}(\text{accept } H_0) = E_{\theta}(1-\Psi_n)
\end{align*}

\begin{defn}
A general purpose test can be constructed from the Likelihood ratio test statistic
\begin{align*}
	\Lambda_n(\Theta,\Theta_0) &= 2\log \frac{\prod_{i_=1}^{n} f(X_i, \hat{\theta}_{MLE})}{\prod_{i=1}^{n}f(X_i, \hat{\theta}_{MLE})} \\
	&= 2\log \frac{\sup_{\theta \in \Theta} \prod_{i_=1}^{n} f(X_i, \hat{\theta})}{\sup_{\theta \in \Theta_0} \prod_{i_=1}^{n} f(X_i, \hat{\theta})}
\end{align*}
\end{defn}

\begin{thm}[Wilks']
	In a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfying Assumption B, and for $\Theta_0 = \{\theta_0\} $, for $\theta_0 \in \Theta$, we have under $P_{\theta_0}$,
	\[
		\Lambda_{n}(\Theta, \Theta_0) \underset{n\to \infty}{\to ^{d}} \chi^2_{p},p = \dim\Theta
	\] 
	
\end{thm}

\begin{remark}
	\begin{enumerate}
		\item One can show more generally that for $\dim(\Theta_0) = p_0 > 0$ but $p_0 < p$, we have
	\[
		\Lambda_{n}(\Theta, \Theta_0) \underset{n\to \infty}{\to ^{d}} \chi^2_{p-p_0} \text{ under  } P_{\theta}, \theta \in \Theta_0
	\]
\item We can construct a test $\Psi_{n} = 1_{\{ \Lambda_n (\Theta, \Theta_0) > \xi_{alpha}\} }$ for $H_0$, where type-I errors are controlled at asymptotic level $\alpha$ if $\xi_{\alpha}$ are the $\alpha$-quantiles of  $\chi^2_{p-p_0}$-distribution.
	\end{enumerate}
\end{remark}

\begin{proof}
	We restrict to events $\hat{\theta}_n \in\intr \Theta$ (of probability approaching $1$). Then since $\hat{\theta}_{n,0} - \theta_0$, we can write 
	\begin{align*}
		\Lambda_n(\Theta,\Theta_0) &= 2\log(\hat{\theta}_n) - 2\log(\theta_0) \\
		&= (-2\log(\theta_0)) -(-2\log(\hat{\theta}_n)) \\
		&= -2 \frac{\partial }{\partial \theta} \log(\hat{\theta}_n) - \frac{2}{2} (\theta_0 - \hat{\theta}_n)^{T} \frac{\partial ^2}{\partial \theta\partial\theta} \log(\overline{\theta}_n)(\theta_0 - \hat{\theta}_n) \text{ (Taylor expansion)} \\ 
		&= 0 - (\theta_0 - \hat{\theta}_n)^{T} \frac{\partial ^2}{\partial \theta\partial\theta} \log(\overline{\theta}_n)(\theta_0 - \hat{\theta}_n) \text{ as $\hat{\theta}_n \in\intr \Theta$, as the gradient at maximiser must vanish }
	,\end{align*}
	where $\overline{\theta}_n$ are mean values lying on the line segment connecting $\hat{\theta}_n, \theta_0$. The second order term can be written

\[
	\sqrt{n}(\hat{\theta}_n - \theta_0)(j_n(\theta) - I(\theta_0))\sqrt{n}(\hat{\theta}_n - \theta_0) + \sqrt{n}(\hat{\theta}_n - \theta_0)(I(\theta_0))\sqrt{n}(\hat{\theta}_n - \theta_0) \underset{n\to \infty}{\to ^{d}} Z^{T}I(\theta_0)Z
		\]
by liberal usage of Slutsky's lemma, the previous proposition, and the continuous mapping theorem for the map $x \mapsto x^{T}I(\theta_0)x$ from $\R^{p} \to \R$. Moreover, by standard linear algebra, $Z^{T}I_{\theta_0}Z = \sum_{i=1}^{p} W_{i}^2$, $W_i \stackrel{iid}{\sim} N$, so $\sim \chi^2_{p}$ 
\end{proof}

\section*{Lecture 11}

\section{Bayesian Inference}

For a given statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}, \Theta \subseteq \R^{p}$, we will now regard $\theta$ as drawn at random from some \textit{prior} distribution $\pi$ on $\Theta$. This may

\begin{enumerate}[label=\roman*)]
	\item Model an intrinsically random state of nature $\Theta$.
	\item Model subjective beliefs about the state of nature $\Theta$.
	\item Serve as a way to generate statistical decision rules/estimators in our inference problem.
\end{enumerate}

\begin{eg}
	Consider a countable set of (scientific) hypotheses $H_i, i \in \Theta$, about the state of nature, each of prior probability $\pi_i, \sum_{i \in \Theta} \pi = 1$, and such that
	\[
		\mathbb{P}\left( X=x \mid H_i \right) = f_i(x)
	,\]
	where $x$ is a random outcome that can be measured. Then by the Bayes rule for conditional probabilities,
	\[
		\mathbb{P}\left( H_i  \mid  X = x \right) = \frac{f_i(x) \pi_i}{\sum_{j\in \Theta} f_j(x) \pi_j}
	\]

	To check whether $H_i$ is more likely than $H_j$ given  $X=x$, we compare
	 \[
		 \frac{\mathbb{P}\left( H_i  \mid X=x \right)}{\mathbb{P}\left( H_j  \mid X=x \right)} = \frac{f_i(x)\pi_i}{f_j(x)\pi_j}
	\]

	If all $\pi_i$ agree ($\Theta$ is finite), then this just reduces to the likelihood ratio test. In a general setting, $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, we wish to model the observation $X \mid \theta \sim f(\cdot , \theta)$ and $\theta \sim \pi$ on $\Theta$, where $\pi$ is the prior distribution. The \textit{posterior distribution} is then the conditional distribution of $\theta  \mid X$. To make this rigorous, consider a sample space $\chi \subseteq \R^{d}$ supporting $\{f\left( \cdot , \theta \right) : \theta \in \Theta \subseteq \R^{p}\}$, and on the product space $\chi \times \Theta \left( \subseteq \R^{d} \times \R^{p} \right) $ consider a probability distribution $Q$ with pmf $f$
	 \[
		 dQ(x,\theta) = f(x,\theta)\pi(\theta)dxd\theta
	 .\]

	 By the usual rules for conditional densities, if $(X,\Theta) \sim Q$, then
	 \[
		 X | \Theta \sim \frac{f(x,\theta)\pi(\theta)}{\int_{\chi}f(x,\theta)\pi(\theta)dx} = f(x,\theta)
	 \]

	 Likewise,
	 \[
		 \theta  | X \sim \frac{f(x,\theta)\pi(\theta)}{\int_{\Theta} f(x,\theta) \pi(\theta) d\theta} = \pi(\theta \mid X)
	 \]

	 is the pdf/pmf of the posterior distribution. If $X_1, \ldots, X_n$ are i.i.d copies of $X \mid \theta$ then the same argument gives that the posterior distribution is given by  
	 \[
		 \theta|(X_1, \ldots, X_n) \sim \frac{\prod_{i=1}^{n}f(X_i, \theta) \pi(\theta)}{\int_{\Theta} \prod_{i=1}^{n} f(X_i, \theta) \pi(\theta) \pi(\theta) d\theta} = \pi(\theta|X_1, \ldots, X_n)
	 \] 
	 
\end{eg}

\begin{eg}
	Consider a $N(\theta, 1)$ model with prior $\pi\sim N(0,1)$ on $\Theta = \R $. Given $X_1, \ldots, X_n$ i.i.d copies of $X|\theta$, we see that
	\begin{align*}
		\pi(\theta |X_1, \ldots, X_n) &\overset{\text{in $\theta$}}{\propto} e^{\frac{1}{2}\sum_{i=1}^{n}(X_i - \theta)^2}e^{-\frac{\theta^2}{2}} \\
		&= e^{-\frac{1}{2} \sum_{i=1}^{n}X_i^2 + \sum_{i=1}^{n}X_i \theta - \frac{n\theta^2}{2} - \frac{\theta^2}{2}} \\
		&\propto e^{n\overline{X}\theta - \frac{n+1}{2}\theta^2} \\
		&\propto e^{-\frac{1}{2} \left( \frac{n}{\sqrt{n+1}} \overline{X} - \sqrt{n+1} \theta  \right)^2 } \\
		= e^{-\frac{n+1}{2}\left(\frac{n}{n+1}\overline{X} - \theta  \right)^2 }
	 \end{align*}
	 And so $\pi(\theta|X_1, \ldots, X_n) \sim N(\frac{1}{n+1}\sum_{i=1}^{n}X_i, \frac{1}{n+1})$.
\end{eg}

One shows more generally that for normal prior and normal 'sampling' models $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$, the posterior distribution is again a normal distribution. This is an example of a \textit{conjugate prior} where the posterior distribution after sampling belongs to the same family of probability distributions.

\begin{eg} We have some other examples of conjugate priors.

	\begin{enumerate}[label=\roman*)]
		\item Beta prior $+$ Binomial sampling $\to $ Beta posterior
		\item Gamma prior $+$ Poisson sampling $\to $ Gamma posterior
	\end{enumerate}
\end{eg}

Even when $\pi$ is not a proper probability distribution, the expression
\[
	\pi(\theta | X_1, \ldots, X_n) = \frac{\prod_{i=1}^{n}f(X_i, \theta) \pi(\theta)}{ \int_{\Theta} \prod_{i=1}^{n}f(X_i, \theta) \pi(\theta) d\theta}
\]
may still be well defined, in which case we speak of a posterior distribution arising from an 'improper' prior. Specifically, the family of 'Jeffrey's' priors which are such that $\pi(\theta) \propto \sqrt{\det(I(\theta))} $ often fall into this class. For instance, for the $N(\theta, \sigma^2)$ model (with $\sigma^2$ known), one sees that the Jeffrey's prior is proportionally constant, and one shows that the 'improper' posterior is equal to a $N(\overline{X}_n, \frac{\sigma^2}{n}) = N(\hat{\theta}_{MLE}, \frac{\sigma^2}{n})$ distribution (see ex. sheet). Note however (see ex. sheet) that uniform priors do not necessarily return 'Bayes estimators' that coincide with MLEs, as the $\text{Bin}(n,p)$ model with $p\sim U(0,1)$ prior shows.

\subsection{Statistical Inference with Posterior Distributions}

The posterior distribution $\pi(\cdot, X_1, \ldots, X_n)$ is a (random) probability distribution on $\Theta$, and hence can be used in principle to construct inference procedures for $\theta$.
\begin{enumerate}[label=\roman*)]
	\item Estimation - One may use the posterior mean $\E^{\pi} [\theta | X_1, \ldots, X_n]$ as an estimator $\overline{\theta}_n = \overline{\theta}(X_1, \ldots, X_n)$ for $\theta$, or alternatively (when appropriately  defined) the posterior mode or median.
	\item Uncertainty quantification - Any subset $C_n \subseteq \Theta$ for which $\pi(C_n|X_1, \ldots, X_n) = 1-\alpha$ is a level $1-\alpha$ \textit{credible} set (but it has, a fortiori, no interpretation in terms of coverage probabilities $P_{\theta} (\theta  \in C_n))$ 
	\item Hypothesis testing - Given $\Theta_0, \Theta_1 \subseteq \Theta$, we can compute Bayes-factors
		\[
			\frac{\pi(\Theta | X_1, \ldots, X_n)}{\pi(\Theta_1|X_1, \ldots, X_n)} = \frac{\int_{\Theta_0} \prod_{i=1}^{n} f(X_i, \theta)\pi(\theta)d\theta}{\int_{\Theta_1} \prod_{i=1}^{n}f(X_i, \theta) \pi(\theta) d\theta} = \frac{\mathbb{P}\left( X_1, \ldots, X_n | \Theta_0) \right)}{\mathbb{P}\left( X_1, \ldots, X_n| \Theta_1) \right)}
		\]
		So we may 'test' for (choose to prefer) $H_0$ if $\phi_n 1_{\{\text{Bayes factor} <1\}}$
\end{enumerate}

\subsection{Frequentist Analysis of Bayes Methods}

Bayesian inference procedures $\overline{\theta}(X_1, \ldots, X_n), C(X_1, \ldots, X_n), \Psi(X_1, \ldots, X_n)$ can be analysed as statistical algorithms in their own right under the \textit{frequentist} sampling assumption that $X_i \stackrel{iid}{\sim} f(\cdot, \theta_0), \theta_0 \in \Theta$.

\begin{eg}
	$X_1, \ldots, X_n \stackrel{iid}{\sim}$ copies of $X|\theta \sim N(\theta, 1)$ with $\theta \sim N(0, 1)$ prior. Then the posterior is
	 \[
		 \theta | X_1, \ldots, X_n \sim N\left( \frac{1}{n+1} \sum_{i=1}^{n}X_i, \frac{1}{n+1} \right) 
	\]
\end{eg}

One shows easily that $\overline{\theta}_n = \frac{1}{n+1} \sum_{i=1}^{n} X_i \to^{a.s} \theta_0 $ under $P_{\theta_0}^{\N}$, and also that $\sqrt{n}(\overline{\theta}_n - \theta \to ^{d} N(0, I(\theta_0)^{-1})$ under $P_{\theta_0}^{\N}$. To corroborate Bayesian credible sets, however, more is required, as these are based not on the 'limit distribution' $N(0, I(\theta_0)^{-1})$, but on $\pi(\cdot , X_1, \ldots, X_n)$.

\begin{thm}(Bernstein-von Mises)
	
	Suppose a statistical model $\{f\left( \cdot , \theta \right) : \theta \in \Theta \}$ satisfies assumption B, and let the prior have a continuous and positive density $\pi$ near $\theta_0$. Denote by $\pi_n = \pi(\cdot , X_1, \ldots, X_n)$, and let $\hat{\phi}_n$ be the pdf of a $N(\hat{\theta}_{MLE}, \frac{1}{n}I(\theta_0)^{-1})$. Then
	\[
		\|\pi_n - \hat{\phi}_n\|_{L^{1}} = \int_{\R} |\pi_n - \hat{\phi}_n(\theta)d\theta \underset{n\to \infty}{\to ^{a.s}} 0
	\] 
\end{thm}

\begin{proof}
	The general proof requires LeCam theory, so we only prove $X|\theta \sim N(\theta, 1)$ with  $\theta \sim N(0,1)$, in which case $I(\theta) = 1$ and $\hat{\theta}_{MLE} = \overline{X}_{n}$. Recall $\pi_n$ is the pdf of a $N(\overline{\theta}$ distribution where $ \overline{\theta} = \frac{n}{n+1}$, and so
	\[
		\sqrt{n} (\hat{\theta}_{MLE} - \theta) = - \sqrt{n} \frac{1}{n+1} (\overline{X}_n - \theta_0 + \theta_0) \underset{n\to \infty}{\to ^{P}} 0 \text{ by SLLN.}
	\] 
	Since $\int_{\R} \pi_n - \hat{\phi}_n(\theta) d\theta = 1-1 = 0$, we have 
	\begin{align*}
		\int_{\R} |\pi_n (\theta) - \hat{\phi}_n(\theta) d\theta &= 2\int_{\R} \left( \pi_n(\theta) - \hat{\phi}(\theta) \right)^{+} d\theta  \\
		&= 2\int_{\R} \left( 1 - \frac{\pi_n(\theta)}{\hat{\phi}(\theta)} \right) ^{+} \hat{\phi}(\theta) d\theta \\
		&= 2\int_{\R} \left( 1 - \frac{\sqrt{\frac{n+1}{2\pi}} \exp\left(-\frac{n+1}{2}(\theta - \hat{\theta} + \hat{\theta} - \hat{\theta})^2\right) }{\sqrt{\frac{n}{2\pi}} \exp(-\frac{n}{2}(\theta - \overline{\theta})^2) } \right)^{+} \sqrt{\frac{n}{2\pi}} \exp\left( -\frac{n}{2}(\theta - \hat{\theta})^2 \right) d\theta  \\
		&= 2\int_{\R} \left( 1 - \sqrt{\frac{n+1}{n}} \frac{\exp\left( - \frac{n+1}{2n} (v + \sqrt{n} (\hat{\theta} - \theta))^2) \right) }{\exp(-\frac{1}{2}v^2) } \right)^{+} \frac{1}{\sqrt{2\pi} } e^{-\frac{v^2}{2}} dv
	\end{align*}
	Where we substituted $v = \sqrt{n} (\theta - \hat{\theta})$. Fixing  $\omega \in \Omega_0 \subseteq \Omega$ such that $\mathbb{P}\left( \Omega_0 \right) = 1$, so $\sqrt{n} (\hat{\theta}(\omega) - \overline{\theta}(\omega) \underset{n\to \infty}{\to } 0 $ (as scalars), note that for $Z\ge 0$, $(1-Z)^{+} \in [0,1]$, so that by integrability of $e^{-\frac{v^2}{2}}$ on $\R$, an application of the dominated convergence theorem implies that the whole last integral tends to $0$ $\forall \omega \in \Omega_0$. Since $\mathbb{P}\left( \Omega_0 \right) = 1$, the limit holds almost surely.
\end{proof}

\end{document}
