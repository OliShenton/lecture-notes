\documentclass[a4paper]{article}
\input{../../../preamble.tex}

\title{Asymptotic Methods}
\author{}

\begin{document}
	
\maketitle

\section{Introduction}
	
	 \[
		 f(x) = x + 3 + \frac{1}{x^2}
	 \]
	 graph showing asymptotically like x+3

	 \[
		 f(x) - (x+3) \to \frac{1}{x^2}
	 \]
	 \[
		 g(x) = x^2 + x + 3 + \frac{1}{x^{3}}
	 \] 

	 The purpose of this course is to analyse how functions, integrals, and solutions to differential equations behave in some asymptotic limit.


\subsection{Important Definitions}

\begin{defn} Big $\mathcal{O}$ Notation

	For $f: (a, \infty) \to \C / \R$, $g: (a,\infty) \to \R$, with $g(x)>0$ for  $x\ge A > a$, we say
	\[
		f(x) = \mathcal{O}\left( g(x) \right)  \text{ as } x\to \infty
	\]
	if $\exists M > 0, B>0 $  st
	\[
		|f(x)| \le Mg(x), \; x\ge B>A
	\] 
	

	We say
	\[
		f(x) =  \mathcal{O}\left( g(x) \right) x\to x_0
	\]
	if $\exists M, \delta > 0$ s.t.
	\[
		|f(x)| \le M g(x), \; 0 < |x-x_0| < \delta
	\]

	or alternatively
	\[
		\lim \sup_{x\to x_0} \frac{|f(x)|}{g(x)} < \infty
	\] 
\end{defn}

\begin{observation}
	If $f(x) = \mathcal{O}\left( g(x)  \right), x\to x_0$, then $ cf(x) =  \mathcal{O}\left( g(x) \right), c \in \R $
\end{observation}

\begin{eg}
	$f(x) = \frac{1}{x^2}\sin(\frac{1}{x}), x\to 0$ is $\mathcal{O(\frac{1}{x^2}})$, as $\lim\sup_{x\to 0} |\sin(\frac{1}{x})| = 1$
\end{eg}


\begin{defn} Little-o Notation
	
	$f(x) = o\left( g(x) \right)$ as $x\to x_0$ if
	\[
		\lim_{x\to x_0} \frac{f(x)}{g(x)} = 0
	\] 

\end{defn}

\begin{eg}
	$f(x) = \frac{1}{x^2}\sin(\frac{1}{x})$, $g(x) = \frac{1}{x^3}$

	\[
		\lim_{x\to 0} \frac{f(x)}{g(x)} = \lim_{x\to 0} x \sin\left(\frac{1}{x}\right) = 0
	\]
	So as $x\to 0$, $f(x) = o\left( g(x) \right) $
\end{eg}

\begin{notation}

	$f(x) = o\left( g(x) \right) $ is sometimes written as $f(x) \ll g(x)$.
	
\end{notation}

\section{Asymptotic Series}

\begin{defn} Asymptotic Sequence
	
	$\phi_n : D \subset \C \to  \C$, $n = 0, 1, \ldots$ is called an asymptotic sequence as $z\to z_0 \in D$, if
	\[
		\forall n > m, \phi_n(z) = o\left( \phi_m(z) \right) \text{ as } z\to z_0
	\] 
\end{defn}

\begin{eg}
	\begin{enumerate}
		\item $\phi_n(x) = x^{n-3}$ defines an asymptotic sequence as $x\to 0$.

		\item $\phi_n(x) = (x-5)^{-n}$ defines an asymptotic sequence as $x\to \infty$

		\item $\phi_n(x) = \frac{1}{x^2}\cos(nx)$ is not asymptotic as $x\to 0$, as $\lim\sup_{x\to 0} \frac{\cos(nx)}{\cos(mx)} = 1$. 
	\end{enumerate}
\end{eg}

Using simple asymptotic sequences, our goal is to describe the behaviour of much more complicated functions.

\begin{defn} Asymptotic Expansion

	Let $\phi_n : D \subset \C \to \C$ be an asymptotic sequence about $z_0$.

	We say the sum $\sum_{n=0}^{\infty} a_n \phi_n(z)$ is an asymptotic expansion of f(z) as $z\to z_0$ if for all  $N \in \N$, as $z\to z_0$,
	\[
		f(z) - \sum_{n=0}^{N} a_n \phi_n(z) = o\left( \phi_N(z) \right) 
	\] 

	i.e. 
	\[
		\lim_{z\to z_0} \frac{f(z) - \sum_{n=0}^{N}a_n \phi_n(z)}{\phi_N(z)} = 0
	\]

	If this holds, we write
	\[
		f(z) \sim \sum_{n=0}^{\infty} a_n \phi_n(z)
	\] 
\end{defn}

\begin{remark}
	We do not require that $\sum_{n=0}^{\infty} a_n \phi_n(z)$ converges.
\end{remark}

\begin{prop}
	If $f(z) \sim \sum_{n=0}^{\infty} a_n \phi_n(z)$, then
	\[
		a_{N+1} = \lim_{z\to z_0} \frac{f(z) - \sum_{n=0}^{N} a_n \phi_n}{\phi_{N+1}}
	\] 
\end{prop}

\begin{remark}
	If $a_n =0 \forall n$, we have that $\lim_{z\to z_0} \frac{f(z)}{\phi_n(x)} = 0$, so our asymptotic sequence is subdominated and provides no information.
\end{remark}

\begin{eg} (Taylor)
	
	$f \in C^{\infty}: [a,b] \to \R$, $f(x) \sim \sum_{n=0}^{\infty} \frac{f^{(n)}(x_0)}{n!} (x-x_0)^{n}$

	We have that $f(x) - \sum_{n=0}^{N} \frac{f^{(n)}(x_0)}{n!} (x-x_0)^{n} = \int_{x_0}^{x} \frac{(x-t)^{N}}{N!} f^{(N+1)}(t) dt =: R_N(x)$.

	$|R_N(x)| \le \max_{a\le t\le b} |f^{(N+1)}(t)| \frac{1}{N!}\int_{x_0}^{x} |x-t|^{N} = \frac{|x-x_0|^{N+1}}{(N+1)!} \max |f^{(N+1)}(t)|$

	So $\left|\frac{R_N(x)}{(x-x_0)^{N}} \right| \le \frac{|x-x_0|}{(N+1)!} \max |f^{(N+1)}(t)| \to 0$ 
\end{eg}

\begin{prop}
	\begin{enumerate}
		\item If $f(z) \sim \sum_{n=0}^{\infty} a_n \phi_n(z)$, $g(z) \sim \sum_{n=0}^{\infty} b_n \phi_n(z)$, then
			\[
				\alpha f + \beta g \sim \sum_{n=0}^{\infty} (\alpha a_n+ \beta b_n) \phi_n(z)
			\]

		\item If $f(z) \sim \sum_{n=0}^{\infty} a_n (x-x_0)^{n}$, $g(z) \sim \sum_{n=0}^{\infty} b_n (z-z_0)^{n}$ then
			\[
				f\cdot g (z) \sim \sum_{n=0}^{\infty} c_n (z-z_0)^{n}
		\]
		with $c_n = \sum_{k=0}^{n} a_k b_{n-k}$


	\end{enumerate}
\end{prop}


\begin{prop}
	
	The asymptotic expansion is unique. That is to say, if $f(z) \sim \sum_{n=0}^{\infty} a_n \phi_n(z)$, $f(z) \sim  \sum_{n=0}^{\infty} b_n \phi_n(z)$, then $a_n = b_n$ everywhere.
\end{prop}

\begin{proof}
	\begin{align*}
		a_0 - b_0 = \frac{(a_0\phi_0 - f) + (f - b_0 \phi_0)}{\phi_0} \to 0
	\end{align*}

	The rest follows inductively.
\end{proof}

\begin{eg}
	$f(x) = \begin{cases}
	 e^{-x^2}, x\neq 0 \\
	 0, x=0
	\end{cases}$

	Then $f(x)$ subdominates all polynomials, so $f(x) + \sin(x)$ has the same asymptotic expansion as $\sin(x)$ about $0$. Hence we can see that while the asymptotic expansion of a function is unique, an asymptotic expansion does not uniquely define a function. 
\end{eg}

\begin{prop}

	If $f(x) \sim \sum_{n=0}^{\infty} a_n (x-x_0)^{n}$, then
	\[
		\int_{x_0}^{x} f(\xi) d\xi \sim \sum_{n=0}^{\infty} \frac{a_n}{n+1} (x-x_0)^{n+1}
	\] 
\end{prop}

\begin{proof}
	Let $\epsilon > 0$.  $\exists \delta(\epsilon) > 0$ s.t. if $0<|\xi - x_0| < \delta$, then
	\[
		\left| f(\xi) - \sum_{n=0}^{N} a_n (\xi - x_0)^{n} \right| \le \epsilon |\xi - x_0|^{N}
	\]
	Then integrating,
	\begin{align*}
		\left|\int_{x_0}^{x} f(\xi) - \sum_{n=0}^{N} a_n (\xi - x_0)^{n} d\xi \right| &\le \int_{x_0}^{x} \left| f(\xi) - \sum_{n=0}^{N} a_n (\xi - x_0)^{n}\right| d\xi \\
		&\le  \epsilon \int_{x_0}^{x} |\xi - x_0|^{N} d\xi \\
		&\le \epsilon \frac{|x-x_0|^{N+1}}{N+1}
	\end{align*}
	And hence
	\[
		\frac{\left| \int_{x_0}^{x} f(\xi)d\xi - \sum_{n=0}^{N} \frac{a_n (x-x_0)^{n+1}}{n+1}\right|}{|x-x_0|^{N+1}} \le \epsilon
	,\]
	i.e. \[
		\int_{x_0}^{x} f(\xi) d\xi - \sum_{n=0}^{N} a_n (x-x_0)^{n+1} = o\left( (x-x_0)^{N+1} \right) 
	\] 

\end{proof}

\begin{ex}
	For $x_0 \in  (a, b)$, and $f(x)$ continuous on $[a,b]$, let $\{\phi_n\}\rvert_{n=0}^{\infty} $ be an asymptotic sequence of functions on $[a,b]$ s.t. $\phi_n(x) \neq 0$ for $x\neq x_0$. Then,
	\begin{enumerate}
		\item $\int_{x_0}^{x} \phi_n(\xi) d\xi$ is an asymptotic sequence. \\
		\item $\int_{x_0}^{x} f(\xi)d\xi \sim \sum_{n=0}^{\infty} a_n \int_{x_0}^{x} \phi_n(\xi) d\xi$
	\end{enumerate}
\end{ex}

\begin{ex}
	$f(x) = \tan x, x=0$.

	$f(x) = x + \frac{x^3}{3} + \ldots = \sum_{n=1}^{\infty}  \frac{2^{n} (2^{n-1}}{(2n)!} |B_{2n}| x^{2n-1}$

	$\phi_n(x) = (\sin x)^{n} ~x^{n}$ 

	Then we have asymptotic expansion
	\[
		\tan x \sim \sum_{n=1}^{\infty} \frac{(2n)!}{(n!)^{2}} (\sin x)^{n}
	\] 
\end{ex}

\subsection{Asymptotic Integrals}

\subsection*{Gamma Function}

\begin{align*}
	\Gamma(z) &= \int_{0}^{\infty} t^{z-1} e^{-t} dt\ \quad Re(z) > 0 \\
	&= \frac{t^{z}}{z} e^{-t} \big\rvert_{0}^{\infty} \int_{0}^{\infty} \frac{t^{z}}{z} e^{-t} dt \\
	\implies \Gamma(z) &= \frac{1}{z} \Gamma(z+1)
\end{align*}
Noting that $\Gamma(1) = \int_{0}^{\infty} e^{-t} dt =1$, we have that $n! = \Gamma(n+1)$.


$\Gamma(n+1) = n! \sim (2\pi n)^{\frac{1}{2}} (\frac{n}{e})^{n}$ 

$\Gamma(n) \sim (\frac{2\pi}{n})^{\frac{1}{2}} (\frac{n}{e})^{n}[n+\frac{1}{12n} + \frac{1}{28 n^2} + \ldots] $

$\Gamma(\frac{1}{2}) = \int_{0}^{\infty} t^{-\frac{1}{2}} e^{-t} dt = \int_{0}^{\infty} 2 e^{-s^2} ds = \sqrt{\pi} $

\subsection*{Stilljes Integral}

$S(x) = \int_{0}^{\infty} \frac{\rho(t)}{1+xt} dt,\quad x\to 0^{+}$.
Here, $\rho(t)$ is very smooth, and decays very fast at $\infty$.

We have that $S(0) = \lim_{x\to 0} S(x) = \int_{0}^{\infty} \rho(t) dt$. 

Now, \begin{align*}\frac{1}{1+xt} &= \sum_{n=0}^{N} (-xt)^{n} + \frac{(-1)^{N+1} x^{N+1} t^{N+1}}{1+xt} \\
	\frac{1}{1+xt} - \sum_{n=0}^{N} (-xt)^{n} &= \frac{(-1)^{N+1} x^{N+1} t^{N+1}}{1+x
t} \\
\implies \int_{0}^{\infty} \frac{\rho(t)}{1+xt}dt - \sum_{n=0}^{N} \int_{0}^{\infty} (-xt)^{n}\rho(t)dt &= (-x)^{N+1} \int_{0}^{\infty} \frac{\rho(t) t^{N+1}}{1+xt} \\	
\end{align*}
Let $C_n = \int_{0}^{\infty} t^{n} \rho(t) dt$ be the moments of $\rho(t)$, and note that  $\frac{1}{1+xt} \le 1$ for $x,t > 0$.

Then  \[
	\left| \int_{0}^{\infty}\frac{\rho(t)}{1+xt}dt - \sum_{n=0}^{N}(-1)^{n} C_n x^{n} \right| \le  x^{N+1} C^{N+1} = o(x^{N}) \text{ as } x\to 0
\] 

Hence
\[
	\int_{0}^{\infty} \frac{\rho(t)}{1+xt} dt \sim \sum (-1)^{n} C_n x^{n}
\] 

\subsection{Approximation of Integrals}

\begin{thm}
	Let $f, g$ be continuously differentiable, with integrable derivatives on $(a,b)$. Then
	 \[
		 \int_{a}^{b} f'(t) g(t) dt = \left[ f(t) g(t) \right]_{a}^{b} - \int_{a}^{b} f(t) g'(t) dt 
	\]

	In general,
	\[
	\int_{a}^{b} f^{(n)}(t) g(t)dt = \sum_{k=1}^{n} (-1)^{k-1} f^{(n-k)}(t)g^{(k)}(t) \big\rvert_{a}^{b} + (-1)^{n} \int_{a}^{b} f(t) g^{(n)}(t)dt
	\] 
\end{thm}

 \begin{eg}
	 $\erf(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty} ^{x} e^{-\frac{t^2}{2}} dt$. We wish to consider its behaviour for $x \gg 1$.

	 Let $F(x) = \sqrt{2\pi} (1-\erf(x) )$. Then
	 \begin{align*}
		 F(x) &= \int_{x}^{\infty} e^{-\frac{t^2}{2}} \frac{t}{t} dt \\
		 &= -\frac{1}{t} e^{-\frac{t^2}{2}} \big\rvert_{x}^{\infty} - \int_{x}^{\infty}\frac{1}{t^2} e^{-\frac{t^2}{2}} dt \\
		 &= \frac{e^{-\frac{x^2}{2}}}{x} - \int_{x}^{\infty} \frac{1}{t^2} e^{-\frac{t^2}{2}}dt \\
		 &:= \frac{e^{-\frac{x^2}{2}}}{x} -R(x) \\
		 |R(x)| &= \int_{x}^{\infty} \frac{e^{-\frac{t^2}{2}}}{t^2} \frac{t}{t}dt \le \frac{1}{x^{3}} e^{-\frac{x^2}{2}} \text{ as } \frac{1}{t^3} \le \frac{1}{x^3} \\
	 \end{align*}
	 Hence
	 \[
	 \left| \frac{F(x) - \frac{e^{-\frac{x^2}{2}}}{x}}{\frac{e^{-\frac{x^2}{2}}}{x}} \right| \le  \frac{1}{x^2} \text{ as } x \to \infty
	 \] 

	 Repeating inductively, integrating by parts, we get that

	 \[
		 \int_{x}^{\infty} e^{-\frac{t^2}{2}} dt \sim \frac{e^{-\frac{x^2}{2}}}{x} \sum_{n=0}^{\infty} \frac{(-1)^{n} (2n-1)!!}{x^{2n}}
	 \] 
\end{eg}

\begin{eg}
	Let $f \in C^{\infty}([a,b])$, with $f(b) \neq 0$. Find the expansion of 
	\[
		I(x) = \int_{a}^{b} f(t) e^{xt} dt \text{ as } x \to \infty
	.\]

	Intuition: Near $t=b$, is the main contribution. So we expect
	 \[
		 \int_{a}^{b} f(t) e^{xt} dt \sim \int_{b-\epsilon}^{b} f(t)e^{xt}dt \sim  f(b) \int_{b-\epsilon}^{b} e^{xt}dt = f(b) \frac{e^{xb} - e^{x(b-\epsilon)}}{x} \sim \frac{f(b) e^{xt}}{x}
	\]

	Integrating by parts, it is easy to see that
	\begin{align*}
	I(x) &= \sum_{k=1}^{n} f^{(n-1)} \frac{e^{xt}}{x^{k}}(-1)^{k-1} \Big\rvert_{a}^{b} + (-1)^{n} \int_{a}^{b} f^{(n-1)}(t) \frac{e^{xt}}{x^{n}} dt \\
	&=  \sum_{k=1}^{n} f^{(n-1)}(b) \frac{e^{xb}}{x^{k}}(-1)^{k-1} + (-1)^{n} \int_{a}^{b} f^{(n-1)}(t) \frac{e^{xt}}{x^{n}} dt -\sum_{k=1}^{n} f^{(n-1)}(a) \frac{e^{xa}}{x^{k}}(-1)^{k-1} \\
	&= e^{xb}\left( \sum_{k=1}^{n} f^{(n-1)}(b) \frac{1}{x^{k}}(-1)^{k-1} \Big\rvert_{a}^{b} + (-1)^{n} \int_{a}^{b} f^{(n-1)}(t) \frac{e^{x(t-b)}}{x^{n}} dt -\sum_{k=1}^{n} f^{(n-1)}(a) \frac{e^{-x(b-a)}}{x^{k}}(-1)^{k-1} \right) \\
\end{align*} 

We can see that $e^{-x(b-a)} = o(x^{-m}, m = 1,2,\ldots$

\begin{align*}
	\left| (-1)^{n} \int_{a}^{b} f^{(n-1)}(t) \frac{e^{x(t-b)}}{x^{n}} \right| &\le \max_{a\le t\le b} |f^{(n-1)}(t)| \int_{a}^{b} \frac{e^{x(t-b)}}{x^{n}} dt \\
	&= \frac{1}{x^{n+1}}(e^{-x(b-a)} - 1) \underbrace{\max_{a\le t \le b} |f^{(n-1)}(t)|}_{:=C_n} \\
	&= o\left(\frac{1}{x^{n}}\right)
\end{align*}

We can then see that
\[
	I(x) \sim  e^{xb} \sum_{k=1}^{n}\frac{(-1)^{k-1} f^{(k-1)}(b)}{x^{k}} 
,\] so in fact
\[
	I(x) \sim \frac{f(b)}{x} e^{xb}
\] 
\end{eg}

\begin{thm} Watson's Lemma

	Let $0 < T \le \infty $. Suppose f(t) has an asymptotic expansion about $t=0$ ( $t>0$)
	 \[
		 f(t) \sim t^{\alpha} \sum_{n=0}^{\infty} a_n t^{\beta n} \text{ as } t \to 0^{+}
	 ,\] with $\alpha > -1$,  $\beta > 0$.

	 Assume further that $f(t)$ satisfies one of the following:

	 \begin{enumerate}
		\item $|f(t)| \le  K e^{bt}, \quad t\ge 0$ \\
		\item $\int_{0}^{T} |f(t)| dt < \infty$	
	 \end{enumerate}
	
	 Then
	 \[
		 F(x) = \int_{0}^{T} e^{-xt}f(t) dt \sim \sum_{n=0}^{\infty} \frac{a_n \Gamma(\alpha + \beta n + 1)}{x^{\alpha + \beta n + 1}} \text{ as } x\to \infty
	 \]

\end{thm}

\begin{proof}
	\[
		F(x) = \int_{o}^{T} e^{-xt}f(t) dt = \int_{0}^{\epsilon}e^{-xt} f(t) dt + \int_{\epsilon}^{T} e^{-xt}f(t) dt
	\]

	Recall that  \[
		\int_{0}^{\infty} e^{-xt} t^{\lambda} dt \overset{u=xt}{=} \int_{0}^{\infty} e^{-u} \frac{u^{\lambda}}{x^{\lambda}} \frac{du}{x} = \frac{\Gamma(\lambda + 1)}{x^{\lambda + 1}}
	\] 

	We need to show that 
	\[
		R_N(x) := F(x) - \sum_{n=0}^{N} \frac{a_n \Gamma(\alpha + \beta n + 1)}{x^{\alpha + \beta n + 1}} = o\left( \frac{1}{x^{\alpha + \beta N+ 1}} \right) \text{ as } x\to \infty
	\]

	By our above comment,
	\[
		R(x) = F(x) - \sum_{n=0}^{N} a_n \int_{0}^{\infty} e^{-xt} t^{\alpha + \beta n} dt
	\]

	We can rewrite this as
         \begin{align*}
		 R_N(x) &= \int_{0}^{\epsilon} f(t) e^{-xt}dt + \int_{\epsilon}^{T} f(t) e^{-xt} dt - \sum_{n-0}^{N}a_n \left( \int_{0}^{\epsilon} e^{-xt}t^{\alpha + \beta n +1} dt + \int_{\epsilon}^{T} e^{-xt} t^{\alpha + \beta n} \right) \\
		 &= \left( \int_{0}^{\epsilon} e^{-xt} \left(f(t) - \sum_{n=0}^{N} a_n t^{\alpha+\beta n}\right) dt \right) + \int_{\epsilon}^{T} e^{-xt}f(t)dt + \sum_{n=0}^{N} a_n \int_{\epsilon}^{T} e^{-xt}t^{\alpha + \beta n} dt \\
		 &:= R_{N_1} + R_{N_2} + R_{N_3}
         \end{align*}

	 Now,
	 \begin{align*}
		 |R_{N_3}| &\le \sum_{n=0}^{N} |a_n| \int_{\epsilon}^{T} e^{-xt} t^{\alpha + beta n}dt \\
		 &= e^{-\epsilon x} \sum_{n=0}^{N} \int_{\epsilon}^{T} e^{-x(t-\epsilon)} t^{\alpha + \beta n} \\
		 &\le  \frac{e^{-\epsilon x}}{x} \sum_{n=0}^{N} |a_n| \int_{0}^{\infty} e^{-u} \left(1+\frac{u}{\epsilon x}\right)^{\alpha + \beta n} du \; \epsilon^{\alpha + \beta n} \text{ using $u = xt$}\\
		 &\le \frac{e^{-\epsilon x}}{x} \sum_{n=0}^{N} |a_n| \int_{0}^{\infty} e^{-u} \left(1+\frac{u}{\epsilon x}\right)^{\alpha + \beta n} du \; \epsilon^{\alpha} \text{ as $\beta$ positive} \\
		 &\le \frac{e^{-\epsilon x} \epsilon^{\alpha}}{x} \sum_{n=0}^{N}|a_n| \int_{0}^{\infty} C(\alpha, \beta, N) \left( 1 + \left( \frac{u}{\epsilon x} \right)^{\alpha + \beta n + 1}  \right)du \\
		 &=o(x^{-m}) \; \forall m\ge 1
	 \end{align*}

	Next,
	\begin{align*}
		|R_{N_1}| &= \left|\int_{0}^{\epsilon}e^{-xt} \left( f(t) - \sum_{n=0}^{N} a_n t^{\alpha + \beta N} \right) dt \right| \\
		&\le \int_0^{\epsilon} e^{-xt} \eta t^{\alpha + \beta N} dt \\
		&\le \eta \int_0^{\infty} e^{-xt} t^{\alpha + \beta n}dt \text{ for some suitably chosen $\eta(\epsilon)$}\\
		&= \eta \frac{\Gamma(\alpha + \beta n + 1}{x^{\alpha + \beta n + 1}} \\
		&= o\left( x^{-(\alpha + \beta n + 1)} \right) 
	\end{align*} 

	In the case that $|f(t)|$ is integrable,

	\begin{align*}
		|R_{N_2}| &\le e^{-xt} |f(t)| dt \\
			&\le  e^{-\epsilon x} \int_{0}^{T}|f(t)| dt
			&= o(x^{-m}) \; \forall m \ge_1
	\end{align*}

	Finally, in the case that $|f(t)| \le  Ke^{bt}$ asymptotically,
	\begin{align*}
		|R_{N_2}| &\le \int_{\epsilon}^{T} e^{-xt} |f(t)| dt \\
		&\le K \int_{\epsilon}^{T} e^{-xt}e^{bt} dt \text{ by assumption} \\
		&\le K \frac{e^{-(x-b)\epsilon}}{x-b} \\
		&= o(x^{-m}) \; \forall m \gg 1
	\end{align*}
\end{proof}

If we instead consider complex values,
\begin{align*}
	F(z) &= \int_0^{T} e^{-zt} f(t) dt
\end{align*}
Now, \[
|e^{-zt}| = e^{-xt}
,\]
and $|x|\ge |z| - |y| \ge |z|(1-\sin \theta) \ge |z|(1 - \sin\delta)$, where we enforce that $z$ is at least an angle $\delta$ into the RHP, $0<\delta<\frac{\pi}{2}$.

Like this, we can reformulate Watson's lemma for complex values.

\begin{eg}
	$F(x) = \int_{0}^{\infty} e^{-xt} \sin t dt$.

	$f(t) = \sin t = \sum_{k=0}^{\infty} \frac{(-1)^{k}t^{2k+1}}{(2k+1)!}$, $|f(t)| \le 1$, so we can apply Watson's lemma.

	Then \[
		I(x) \sim  \sum_{k=0}^{\infty} \frac{(-1)^{k}}{(2k+1)!} \frac{\Gamma(2k+1 + 1)}{x^{2k+2}}
	\]
	or more neatly, as $k \in \N$,
	\[
		I(x) \sim \frac{1}{x^2} \sum_{k=0}^{\infty} \frac{(-1)^{k}}{x^{2k}}
	\] 
\end{eg}

\begin{eg}
	One of the solutions of the hypergeometric equation
	\[
		xy_{x x} + (b-x) y_x - ay = 0
	\]
	With $\Re b > \Re a > 0$, is given by
	 \[
		 M(a,b,x) = \frac{e^{x}}{\Gamma(a)\Gamma(b-a)} \int_{0}^{1}  e^{-xt} (1-t)^{1-a} t^{b-a-1} dt
	\]

	So, let $f(t) = (1-t)^{1-a}t^{b-a-1}, \quad 0\le t\le 1$.
	\begin{align*}
		f(t) = t^{b-a-1} \sum_{n=0}^{\infty} \frac{(-1)^{n}}{n!} \frac{\Gamma(a)}{\Gamma(a-n)} t^{n}
	\end{align*}
	and as such we can apply Watson's Lemma to $I(x)$, getting 
	\[
		M(a,b,t) = \sum_{n=0}^{\infty} \left( \frac{(-1)^{n} \Gamma(a)}{\Gamma(a-n) n!} \frac{\Gamma(b-a+n)}{z^{b+a-n}} \right) \frac{e^{x}}{\Gamma(a)\Gamma(b-a)}
-a)
\]

So to leading order, \[
	M(a,b,x) = \frac{e^{x}}{x^{b-a} \Gamma(a)}
.\] 
\end{eg}

\section{Asymptotics of Integrals}

\subsection{Laplace's Method}

Consider
\[
	F(x) = \int_a^{b} f(t) e^{x\phi(t)} dt
,\] as $x\to \infty$. 

Firstly, consider the case that $\phi$ has a \textbf{unique} global maximum at $c\in [a,b]$, such that $\phi'(c) = 0$, and  $\phi''(c) < 0$. Assume also that $f(c) \neq 0$

Then we have  
\begin{align*}
	F(x) &\sim \int_{c-\epsilon}^{c+\epsilon} f(t) e^{x\phi(t)}dt \\
	&\sim \int_{c-\epsilon}^{c+\epsilon} f(t) e^{x (\phi(c) + \frac{1}{2} \phi''(c) (t-c)^2)} dt \\
	&\sim f(c)e^{x\phi(c)} \int_{c-\epsilon}^{c+\epsilon}e^{\frac{1}{2}\phi''(c)(t-c)^2}dt \\
	&\sim \frac{f(c) e^{x\phi(c)}}{|x\phi''(c)|^{\frac{1}{2}}} \underbrace{\int_{-|x\phi''(c)|^{\frac{1}{2}}}^{|x\phi''(c)|^{\frac{1}{2}}} e^{-\frac{s^2}{2}}ds}_{s= \sqrt{-x\phi''(c)}(t-c)}\\
	&\sim \frac{\sqrt{2\pi} }{|x\phi''(c)|^{\frac{1}{2}}} e^{x\phi(c)}f(c)
\end{align*}

\begin{eg}
	\[
		I(x) = \int_{-\infty}^{\infty} e^{-xt^2} e^{at} dt
	\]

	So we have $f(t) = e^{at}$, $\phi(t) = -t^2$, and
	\begin{align*}
		I(x) &\sim \frac{e^{a\cdot 0}e^{0} \sqrt{2\pi} }{(2x)^{\frac{1}{2}}} \\
			&\sim \sqrt{\frac{\pi}{x}} \text{ as } x\to \infty 
	\end{align*}

	It is also quite simple to calculate this explicitly, where we find that this is as expected correct to leading order.
\end{eg}

Next, we consider the case of Laplace's method when the global maximum occurs at the endpoint $t=a$, with $\phi'(a), \, f(a) \neq 0$ (so $\phi'(a) < 0$).
 \[
	 F(x) \sim \int_{a}^{a+\epsilon} f(t) e^{x\phi(t)} dx
\]

We see $\phi(t) \sim \phi(a) + \phi'(a) (t-a)$, so that
\begin{align*}
	F(x) &\sim  \int_{a}^{a+\epsilon} f(t) e^{x\left(\phi(a) + \phi'(a)(t-a)  \right) } dt \\
	&\sim f(a)e^{x\phi(a)} \int_{a}^{a+\epsilon} e^{\phi'(a)(t-a)}dt\\
	&\sim f(a)e^{x\phi(a)} \int_0^{\epsilon x |\phi'(a)|} e^{-u} \frac{du}{x |\phi'(a)|} \\
	&\sim  \frac{f(a)e^{x\phi(a)}}{x|\phi'(a)|}\left( \underbrace{\int_{0}^{\infty} e^{-u} du}_{=1} - \underbrace{\int_{\epsilon x|\phi'(a)|}^{\infty} e^{-u} du}_{\text{exponentially decays in $x$}} \right) \\
	&\sim \frac{f(a)e^{x\phi(a)}}{x|\phi'(a)|}
\end{align*}

\begin{remark}
	If our global maximum is instead at $t=b$, with $\phi'(b) >0$, we see that
	 \[
		 F(x) \sim \frac{f(b)e^{x\phi(b)}}{x\phi'(b)}
	\] 
\end{remark}

Our third case is that we have a unique global maximum of $\phi(t)$ at $c \in (a,b)$, with $\phi'(c) = \phi''(c) =0$. Assume  $f(c) \neq 0$.

\[
	F(x) = \int_{a}^{b} f(t) e^{x\phi(t)} dt \sim \int_{c-\epsilon}^{c+\epsilon} f(t) e^{\phi(t)}dt
\]

As $c$ is a maximum, we must have that
\[
	\phi(t) \sim \phi(c) + \frac{\phi^{(p)}(c)}{p!}(t-c)^{p}
,\] where p is the first (even) power such that $\phi^{(p)}(c) \neq 0$. For a maxima, $\phi^{(p)}(c) < 0$

Then 
\begin{align*}
	F(x) &\sim \int_{c-\epsilon}^{c+\epsilon} f(c) e^{x\left( \phi(c) + \frac{\phi^{(p)}(c)}{p!}(t-c)^{p} \right) } dt  \\
	&\sim f(c)e^{x\phi(c)} \int_{c-\epsilon}^{c+\epsilon} e^{x\phi^{(p)}(c)}\frac{(t-c)^{p}}{p!} dt
\end{align*}

Let $s = \left( \frac{x |\phi^{(p)}(c)|}{p!} \right)$. So
\begin{align*}
	F(x) &\sim  f(c)e^{x\phi(c)} \int_{- \epsilon \left( \frac{x|\phi^{(p)}(c)|}{p!} \right)^{\frac{1}{p}} }^{\epsilon \left( \frac{x|\phi^{(p)}(c)|}{p!} \right)^{\frac{1}{p}} } e^{-s^{p}} \frac{1}{\left( \frac{x|\phi^{(p)}(c)|}{p!} \right)^{\frac{1}{p}} } ds  \\
	&\sim \frac{f(c)e^{x\phi(c)}}{\left( \frac{x|\phi^{(p)}(c)|}{p!} \right)^{\frac{1}{p}} } \int_{-\infty}^{\infty}e^{-s^{p}} ds \\
	&\sim \frac{2 f(c) e^{x\phi(c)} (p!)^{\frac{1}{p} \Gamma(1+p)}}{(x|\phi^{(p)}(c)|)^{\frac{1}{p}}}
\end{align*}
Where the last integral (for even $p $) is left as an exercise to the reader.

\subsection*{Laplace Method - Higher Order Expansion}

Motivation: What if $f(c) = 0$?

Again, we assume that  $\phi(t)$ has a unique global maximum at $t=c$, $c\in (a,b)$, with $\phi'(c) = 0$,  $\phi''(c) < 0$. 

So $f(c)\sim f(c) + f'(c)(t-c) + \frac{1}{2} f''(c) (t-c)^2 + \ldots$

And $\phi(c) \sim \phi(c) + \frac{1}{2} \phi''(c) (t-c)^2 + \frac{1}{6}\phi'''(c) (t-c)^3 + \frac{1}{24} \phi''''(c) (t-c)^{4} + \ldots$

Then as before,
\begin{align*}
	F(x) &\sim \int_{c-\epsilon}^{c+\epsilon} f(t)e^{x\phi(t)}dt \\
	&\sim  \int_{c-\epsilon}^{c+\epsilon} \left(f(c) + f'(c)(t-c) + \frac{1}{2} f''(c) (t-c)^2 \right)  e^{x\left(\phi(c) +\frac{1}{2} \phi''(c) (t-c)^2 + \frac{1}{6}\phi'''(c) (t-c)^3 + \frac{1}{24} \phi''''(c) (t-c)^{4} \right)} dt \\
	&\sim \int_{c-\epsilon}^{c+\epsilon} \left(f(c) + f'(c)(t-c) + \frac{1}{2} f''(c) (t-c)^2 \right) e^{x\phi(c)} e^{\frac{1}{2}\phi''(c) (t-c)^2} e^{x\left(\frac{1}{6}\phi'''(c) (t-c)^3 + \frac{1}{24} \phi''''(c) (t-c)^{4}\right)} dt \\
	&\sim \frac{e^{x\phi(c)}}{\left( |x\phi''(c)| \right)^{\frac{1}{2}} } \int_{-\epsilon|x\phi''(c)|^{\frac{1}{2}}}^{\epsilon |x\phi''(c)|^{\frac{1}{2}}} E_1(s) e^{-\frac{s^2}{2}} \exp\left( \frac{\phi'''(c) s^3}{6 x^{\frac{1}{2}} |\phi''(c)|^{\frac{3}{2}}} + \frac{\phi''''(c)}{24} \frac{s^{4}}{x|\phi''(c)|^2} \right) ds
\end{align*}

Where \[
	E_1(s) = f(c) + \frac{f'(c)}{\left( x|\phi''(c)| \right)^{\frac{1}{2}} }s + \frac{f''(c) s^2}{2x|\phi''(c)|} 
\]

Now, writing our second exponential as $e^{z}$ and expanding to second order,

\begin{align*}
	F(x) &\sim \frac{e^{x\phi(c)}}{\left( |x\phi''(c)| \right)^{\frac{1}{2}} } \int_{-\epsilon|x\phi''(c)|^{\frac{1}{2}}}^{\epsilon |x\phi''(c)|^{\frac{1}{2}}} E_1(s) E_2(s) ds 
\end{align*}

Where \[
	E_2(s) = 1 + \left[ \frac{\phi''(c) s^3}{6x^{\frac{1}{2}} |\phi''(c)|^{\frac{3}{2}}} + \frac{\phi''''(c) s^{4}}{24 z|\phi''(c)|^2 } \right]  + \frac{(\phi'''(c))^2s^{6}}{2\cdot 32 |\phi''(c)|^3}
\]

Note that if $c$ is at an endpoint, our odd terms contribute, but here, we need only consider even terms by symmetry.

So,
\begin{align*}
	F(x) &\sim \frac{e^{x\phi(c)}}{\left( |x\phi''(c)| \right)^{\frac{1}{2}} } \int_{-\epsilon|x\phi''(c)|^{\frac{1}{2}}}^{\epsilon |x\phi''(c)|^{\frac{1}{2}}} \left( f(c) + \frac{1}{x} \left[ \frac{f''(c) ^2}{2 |\phi''(c)|} + \frac{f'(c) \phi'''(c) s^{4}}{6|\phi''(c)|^2} + \frac{f(c) \phi''''(c) s^{4}}{24|\phi''(c)|^2} + \frac{f(c)(\phi'''(c))^2 s^{6}}{721 |\phi''(c)|^3}\right]  \right) ds \\
	&\sim \frac{e^{x\phi(c)}}{\left( x|\phi''(c)| \right)^{\frac{1}{2}} } \sqrt{2\pi}  \left( f(c) + \frac{1}{x} \left[ \frac{f''(c) ^2}{2 |\phi''(c)|} + \frac{f'(c) \phi'''(c) s^{4}}{2|\phi''(c)|^2} + \frac{f(c) \phi''''(c) s^{4}}{8|\phi''(c)|^2} + \frac{f(c)(\phi'''(c))^2 s^{6}}{24 |\phi''(c)|^3}\right]  \right) 
\end{align*}

Where we used the identity (exercise) that for $p=2n$ even,
\[
	\int_{-\infty}^{\infty} e^{-\frac{s^2}{2}} s^{p} ds = \sqrt{2\pi} s^{n}\Gamma(n+\frac{1}{2}) 
\] 

\section{Oscillatory Integrals}

We wish to consider integrals of the form
\[
	I(\omega) = \int_{a}^{b} f(t) e^{i\omega \phi(t)} dt
,\] as $|\omega| \to  \infty$, for $f, \phi \in \R$.

We often call $\phi(t)$ the phase function.

\begin{lemma} Riemann-Lebesgue Lemma
	
	Let $-\infty \le a \le b \le  \infty$, with $|f(t)|$ integrable.

	Then
	\[
		I(\omega) = \int_{a}^{b} f(t) e^{i\omega t} dt \to 0 \text{ as } \omega \to \infty 
	\] 
\end{lemma}

\begin{proof}
	Consider first the case where $f \in C^{1}[a,b]$ 

	\begin{align*}
		I(\omega) &= \int_a^{b} f(t)e^{i\omega t} dt \\
			  &= \frac{f(t)e^{i\omega t}}{i\omega} \Bigg\rvert_{a}^{b} - \frac{1}{i\omega} \int_{a}^{b} f'(t) e^{i\omega t}dt \\
			  |I(\omega)| &\le  \frac{|f(b)| + |f(a)|}{|\omega|} + \frac{1}{|\omega|}(b-a) \max_{a\le t\le b} |f'(t)| \\
			  &\to 0 \text{ as } |\omega| \to \infty
	\end{align*}

	In our general case, we go back to definitions of Riemann Integrability. 
	$\forall \epsilon > 0$ , there is a partition such that our lower and upper sums satisfying 
	\[ 
		m(t) \le f(t) \le M(t)
	\]
	with
	\begin{align*}
		\left|\int_{a}^{b} f(t) - m(t) dt \right| &\le \epsilon \\
		\left| \int_a^{b} M(t) - f(t)  dt \right| &\le \epsilon \\
		\left| \int_a^{b} M(t) - m(t) dt \right| &\le \epsilon
	\end{align*}

	Hence
	\begin{align*}
		|\int_a^{b} f(t)e^{i\omega t}dt| &\le  \left|\int_a^{b} \left(f(t) - m(t)\right) e^{i\omega t} dt\right| + \left| m(t) e^{i\omega t} dt \right| \\
						 &\le  \int_a^{b}f(t) - m(t) dt + \left|\int_a^{b} m(t) e^{i\omega t} dt \right| \\
						 &= \epsilon + \left|\int_a^{b} m(t)e^{i\omega t} dt \right|
	\end{align*}

	Taking $\lim\sup_{\omega\to \infty}$ of both sides, we get the result.

	Note that in our first case, if $f'$ is Riemann integrable, we get that $I(\omega) = \mathcal{O}(\frac{1}{\omega})$.
\end{proof}

\subsection{Method of Stationary Phase}

\begin{prop}
	Consider $f \in C^{\infty}[a,b]$ Then

	\begin{align*}
		I(\omega) \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(i\omega)^{n+1}} \left( e^{i\omega b} f^{(n)}(b) - e^{i\omega a} f^{(n)}(a) \right) 
	\end{align*}
\end{prop}

\begin{ex}
	For $f(t) = \frac{1}{1+t}$, $I(\omega) = \int_0^{1} \frac{e^{i\omega t}}{ 1+t} dt$.

	Then  \[
		f^{(n)} (t) = \frac{(-1)^{n}n!}{(1+t)^{n+1}}
	\]
	and
	\[
		I(\omega) \sim \sum_{n=0}^{\infty} \frac{n!}{(i\omega)^{n+1}} \left[ \frac{e^{i\omega}}{2^{n+1}} - 1 \right] 
	\] 
\end{ex}

\begin{prop}
	Suppose $\phi'(t) \neq 0$ on $[a,b]$, and $f'(t)$ integrable.

	Then
	\[
		I(\omega) \sim  \frac{1}{i\omega} \left[ \frac{f(b)}{\phi'(b)} e^{i\omega \phi(b)} - \frac{f(a)}{\phi'(a)} e^{i\omega \phi(a)} \right] 
	\] 
\end{prop}

\begin{proof}
	Let $u=\phi(t)$. Then we can define $t = \phi^{-1}(u)$ by monotonicity.

	So
	\begin{align*}
		I(\omega) = \int_{\phi(a)}^{\phi(b)} f(\phi^{-1}(u)) \frac{e^{i\omega u}}{\phi'(\phi^{-1}(u))} du
	\end{align*}
\end{proof}

We now consider the case where $\phi(t)$ has a unique local max/min at $t=c, 1-c$, where  $\phi'(c) = 0, \phi''(c) > 0$
In the areas $a<t<c-\epsilon$ and $c+\epsilon<t<b$, our contribution is $\mathcal{O}(\frac{1}{\omega})$ as before. So, we consider
\begin{align*}
	\int_{c-\epsilon}^{c+\epsilon} f(t)e^{i\omega \phi(t)} dt &= \int_{c-\epsilon}^{c+\epsilon} f(t) e^{i\omega \left( \phi(c) + \frac{1}{2}\phi''(c)(t-c)^2 \right) } dt \\
	&\sim f(c) \int_{c-\epsilon}^{c+\omega} e^{i\frac{\omega}{2} \phi''(c) (t-c)^2} dt  \\
\end{align*}

We first consider $\phi''(c), \omega > 0$. Let  $s = \sqrt{\frac{\omega \phi''(c)}{2}}(t-c)$. Then
\begin{align*}
	\int_{c-\epsilon}^{c+\epsilon} f(t)e^{i\omega \phi(t)} dt &\sim \frac{f(c) e^{i\omega\phi(c)}}{\sqrt{\frac{\omega \phi''(c)}{2}}} \int_{-\epsilon \sqrt{\frac{\omega \phi''(c)}{2}} }^{\epsilon \sqrt{\frac{\omega \phi''(c)}{2}}} e^{is^2} ds \\
	&\sim \frac{f(c) e^{i\omega\phi(c)}}{\sqrt{\frac{\omega \phi''(c)}{2}}} \int_{-\infty}^{\infty} e^{is^2} ds \\
	&= \frac{2 f(c) e^{i\omega\phi(c)}}{\sqrt{\frac{\omega \phi''(c)}{2}}} \int_{0}^{\infty} e^{is^2} ds  \\
\end{align*}

We have found ourselves a Fresnel integral, $I = \int_{0}^{\infty} e^{ix s^2} ds = \frac{1}{2 }\sqrt{\frac{\pi}{x}} e^{i\frac{\pi}{4}} $. In general we might like to (for higher order expansions) consider $I_n = \int_{0}^{\infty} e^{ixs^{2n}}$. This can be done by considering wedges of angle $\frac{\pi}{2n}$.

So \[
	\int_{c-\epsilon}^{c+\epsilon} f(t) e^{i\omega \phi(t)} dt \sim \frac{f(c) e^{i\omega\phi(c)}\sqrt{2\pi} }{\sqrt{\omega \phi'(c)} } e^{i\frac{\pi}{4}}
\]

and in general, we get\[
	I(\omega) \sim f(c) e^{i\omega\phi(c)}\sqrt{\frac{2\pi}{|\omega \phi''(c)|}} e^{i\frac{\pi}{4} \sgn(\omega \phi''(c))} \text{ as } |\omega| \to \infty 
\]

As opposed to the Laplace's method, every local maxima or minima contributes. If our maxima occurs at the endpoint of the integral, we get a prefactor of $\frac{1}{2}$.

\begin{remark}
	If $c$ is a max/minx, with $\phi^{(2n)}\neq 0$ our first non-zero derivative, then we get that $I(\omega) \sim \mathcal{O}\left(\frac{1}{|\omega|^{\frac{1}{2n}}}  \right) $ 
\end{remark}

\begin{eg} Bessel Function of order $0$.

	\begin{align*}
		J_0(r) = \frac{1}{2\pi} \int_{0}^{2\pi} e^{ir \sin t} dt, \quad r\ge 0, \; r\to \infty
	\end{align*}
	For $\phi(t) = \sin t$, we have a maximum at $t = \frac{\pi}{2}$, and a minima at $t=\frac{3\pi}{2}$.

	At $\frac{\pi}{2}$, our contribution is
	\[
		\frac{1}{2\pi} \sqrt{\frac{2\pi}{r|-\sin(\frac{\pi}{2})}} e^{ir \sin \frac{\pi}{2} (-1)} = \frac{1}{\sqrt{2\pi r} } e^{ir} e^{-i\frac{\pi}{4}}
	,\]  and at $\frac{3\pi}{2}$ our contribution is similarly $\frac{1}{\sqrt{2\pi r} } e^{-ir} e^{i\frac{\pi}{4}}$.

	Then $J_0(r) \sim \sqrt{\frac{2}{\pi r}} \cos\left( r - \frac{\pi}{4} \right)  $.
\end{eg}

\subsection{Method of Steepest Descent}

We now wish to consider 
\[
	I(x) := \int_{C} g(z, x) dz
\] about some contour $C \subset \C$. We might like to try deforming our contour within a region of analyticity to some more convenient integral which is equal by Cauchy's Theorem.

We now consider the class of functions given by
\[
	I(x) = \int_{C} f(z) e^{x\phi(z)} dz
\] where $f, \phi$ are analytic in some region $D \supset C$.

Writing $\phi(z) = u(p, q) + iv(p,q)$, we have that $u_p = v_q$, $u_q = -v_p$, and so $\nabla u \cdot  \nabla v = 0$. Also, $\nabla u \neq  0 \iff \nabla v \neq 0 \iff \phi'(z) \neq 0$. We say then that $\phi$ is conformal at $z$.

\begin{eg}
	Take $\phi(z) = z^2 = p^2-q^2 + 2ipq$. Then $|e^{x\phi(z)}| = |e^{xu}|$ for real $x$.
\end{eg}

\subsubsection*{Ideal Case}

Ideally, we can deform $C \to \tilde{C}$ in a manner such that, for $e^{x\phi(z)} = e^{xu(p,q)} e^{iv(p,q)}$, $\tilde{C}$ is a level curve of $v$, i.e. $v(p,q) = v(p_0, q_0) = \text{const}$. 

In this case, \[
	I(x) = e^{iv(p_0,q_0)}\int_{\tilde{C}} f(z) e^{iu(p,q)} dz
\]

We can then parameterize $\tilde{C}$ as $z(t), \alpha \le t \le \beta$, and we can use Laplace's Method on 
\[
	I(x) = e^{ixv(p_0, q_0)} \int_{\tilde{C}} f(z(t)) e^{xu(p(t), q(t))} (\dot{p} + i \dot{q})dt
\]

\subsubsection*{Added Value}

In addition we have by Cauchy-Riemann that $\phi'(z_0) \neq 0 \iff \nabla u(p_0, q_0) \neq 0 \iff \nabla v(p_0, q_0) \neq 0$. Then if $phi'(z_0)$, we have a unique orthogonal intersection of curves with $u=\text{const}$ and $v = \text{const}$. The curve $v = \text{const}$ is in fact the steepest descent curve of $u$, that is along the direction of $\nabla u$.

\subsubsection*{Fixing Things}

Suppose $v(p_0, q_0) = v(p_1, q_1)$, where $C$ connects $z_0$ and $z_1$. In this case, we cannot deform to a curve $\tilde{C}$ with $v(p,q) = \text{const}$. 

\begin{idea}

We hope that by continuing two paths of $v=\text{const}$ from $z_0$ and $z_1$ to infinity, we can make it such that the connecting contour goes to 0, and we magically get the right answer (see example).
\end{idea}

Consider instead what happens at a point with $\phi'(z_0)$. Then $u$ (and $v$) is stationary at $(p_0, q_0)$. Now, $u,v$ are harmonic functions, so $u_{pp} = -u_{qq}$, and all interior stationary points must be saddle points. Then we can find a direction of steepest descent. 

\begin{eg} Saddle point

	Let $\phi(z) = iz^2$. Then $\phi'(z) = 2iz = 0 \iff z=0$.

	Now, $u = -2pq$, and $v = p^2 - q^2$.
	$v(0,0) = 0$. Consider the curve $v(p,q) = 0$. These are given by $p=q$ and $p=-q$.

	On these two curves,  $u= -2p^2$ or $u = 2p^2$. The former has a maximum at the origin, the latter has a minimum. Then to use steepest descent, we go along the curve $p=q$, where we can use Laplace's method.
\end{eg}

\begin{eg} (Bender and Orszage)

Let \[
	I(x) = \int_0^{1} e^{ixt^2} dt, \quad x\to \infty
\] (Fresnel integral at finite interval)

Then we expect, as $t^2$ is minimal at the origin, that $I(x) \sim \mathcal{O}\left(\frac{1}{\sqrt{x} }\right)$, with our leading contribution being from $t=0$. 

So, $\phi(z) = iz^2$, $f(z) = 1$, $C = [0,1]$. We have  $u = -2pq$, $v = p^2 - q^2$. Note that $v(0,0)  = 0 \neq v(1,0) = 1$.

By before, at $z =0$, our curve of steepest descent is $p=q$, where we go in to the UHP. Now, $\phi'(1) = 2i$, so there is a single line of steepest descent here. To have $v(p,q) = 1$, we have $p^2 - q^2 = 1$, so $p = \pm \sqrt{q^2 + 1} $. On this curve, we have $u = \mp 2q\sqrt{q^2 + 1} $. We pick the negative curve for steepest descent.  

We consider truncating at $q=T$, connecting the two curves with a segment of $q=T$.  

So we let $\Gamma_2(T) = \{z + s + iT: T \le s \le \sqrt{T^2 +1} \} $.

\begin{align*}
	\left| \int_{\Gamma_2(T)} e^{ixz^2} dz \right| &= \left| \int_{T}^{\sqrt{T^2 + 1} } e^{ix \left( s^2 -T^2  + 2isT \right) } ds \right| \\
	&\le  \left| \int_T^{\sqrt{T^2 + 1} } e^{-2sT} ds  \right| \\
	&= \frac{1}{2xT} \left( e^{-2xT^2} - e^{-2xT\sqrt{T^2 + 1} } \right)  \\
	& \to 0 \text{ as } T \to \infty
\end{align*}

Let $\Gamma_1(T)$ be our curve of steepest descent,  $p=q$.

\begin{align*}
	\int_{\Gamma_1}  e^{ixz^2} dz &= \int_0^{\infty} e^{-2q^2 x} (1+i) dq  \\
	&= \frac{1}{2} \sqrt{\frac{\pi}{x}} e^{i\frac{\pi}{4}} \\
\end{align*}

Finally, we let $\Gamma_3$ be our right curve of steepest descent, $p = \sqrt{q^2 + 1} $.

\begin{align*}
	\int_{-\Gamma_3} e^{ixz^2} dz &= \int_{0}^{\infty} \left( \frac{q}{\sqrt{1+q^2} } + 1 \right) e^{ix}e^{-2q\sqrt{1+q^2} } dq\\
\end{align*}
Let $s = 2q \sqrt{1+q^2} $. Then $i z^2 = i - s, z^2 = 1 + is$.

So
\begin{align*}
	\int_{-\Gamma_3} e^{ixz^2} dz &= \int_{0}^{\infty} \frac{e^{(i-s)x} i}{2(1+is)^{\frac{1}{2}}} ds \\
	&= \frac{i}{2} e^{ix} \int_0^{\infty} \frac{e^{-sx}}{(1+is)^{\frac{1}{2}}} \\
\end{align*}

Now, 
\[
	\frac{1}{(1+is)^{\frac{1}{2}}} = \sum_{n=0}^{\infty} (-i)^{n} \frac{s^{n}}{n!} \frac{\Gamma\left(n+\frac{1}{2}\right)}{\Gamma \left( \frac{1}{2} \right)}
,\] 
And so by Watson's Lemma,
\begin{align*}
	\int_{-\Gamma_3} e^{ixz^2} dz &\sim \frac{i}{2} e^{ix} \sum_{n=0}^{\infty} (-i)^{n} \frac{\Gamma \left(n + \frac{1}{2} \right)}{\Gamma \left( \frac{1}{2} \right) n! } \frac{\Gamma \left( n+1 \right) }{x^{n+1}} \\
	&= \frac{e^{i\frac{\pi}{2}}}{2} \sum_{n=0}^{\infty} (-i)^{n} \frac{\Gamma \left( n+\frac{1}{2} \right) }{\Gamma\left( \frac{1}{2} \right) } \frac{1}{x^{n+1}}
\end{align*}

And \[
	I(x) \sim \int_{\Gamma_1} - \int_{-\Gamma_3}
\] 
\end{eg}

We go back now to our general form
\[
	\int_{C} f(z) e^{x(u+iv)} dz
\]

We seek to deform $C$ to reach a saddle point, as there we have multiple curves of steepest descent, giving then the major contribution to the integral from the saddle point.

Suppose $C$ in fact starts from a saddle point and is of steepest descent for u. Then $\phi'(z_0) = 0$. Assume that $\phi''(z_0) \neq 0$.

\begin{align*}
	\int_{C} f(z) e^{x\phi(z)} dz = e^{ixv_0} \int_{C} f(z) e^{xu(z)} dz
\end{align*}
As $C$ is a curve of steepest descent, we can happily consider just a small region about $z_0$ for our majority contribution.

So
 \begin{align*}
	 \int_{C} f(z) e^{x\phi(z)} dz \sim f(z_0) e^{x\phi(z_0)}\int_{C_\epsilon} e^{\frac{1}{2} \phi''(z_0) (z-z_0)^2} dz
\end{align*}

Let $z(t) = z_0 + r(t) e^{i\theta(t)}$. Then $\phi''(z_0) (z-z_0)^2 = \phi''(z_0) r^2 e^{2i\theta(t)}$, and $\phi''(z_0) = |\phi''(z_0)| e^{i\alpha}$. $\Im (\phi(z) - \phi(z_0)) =0 $ on $C_{\epsilon}$, as $v$ is constant. So, writing  $\phi''(z_0) (z-z_0)^2 = |\phi''(z_0)| r^2 e^{i(2\theta + \alpha)}$, we have that $\sin (2\theta + \alpha) = 0$. Also, $\Re (\phi(z) - \phi(z_0)) <0$ for $z \neq z_0$ on $C_{\epsilon}$. So $\cos(2\theta + \alpha) < 0$, and so  $2\theta + \alpha = (2k+1)\pi$, $k=0,1$.

Then
 \begin{align*}
	 I(x) &\sim f(z_0) e^{x\phi(z_0)} \int_{C_{\epsilon}} e^{\frac{x}{2} |\phi''(z_0)| r^2 e^{i(2\theta + \alpha)}} d(re^{i\theta}) \\
	 &\sim  f(z_0) e^{x\phi(z_0)} e^{i\theta}\int_{0}^{\epsilon} e^{\frac{x}{2} |\phi''(z_0)| r^2 e^{i(2\theta + \alpha)}} dr \\
\end{align*}
Let $s = -i \left( \frac{|\phi''(z_0)| x}{2} \right)^{\frac{1}{2}} re^{i(\theta + \frac{\alpha}{2})} $ 

Then 
\begin{align*}
	I(x) &\sim f(z_0) e^{i\theta} e^{x\phi(z_0)} i \left( \frac{2}{x |\phi''(z_0)|} \right)^{\frac{1}{2}} e^{-i\frac{\alpha}{2}} e^{-i\theta} \int_{0}^{\infty} e^{-s^2} ds \\
	&\sim  \frac{i}{2} \left( \frac{2\pi}{x\phi''(z_0)} \right)^{\frac{1}{2}} e^{x\phi(z_0)} f(z_0)
\end{align*} as $\phi''(z_0) = |\phi''(z_0)| e^{i\alpha}$.

\subsection*{Airy Equation and Function}

The Airy equation is given by \[
y'' = xy
,\] and we seek to understand its asymptotic behaviour.

We solve this by taking Fourier transforms, getting 
\[
\frac{d}{dk} \hat{y} = ik^3 \hat{y}
,\] and so inverting,
\[
	Ai(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{\frac{1}{3} it^3 +ixt} dt
\]

We let $s = \sqrt{x} t$ for $x > 0$.

Then
\[
	Ai(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \sqrt{x} e^{i x^{\frac{3}{2}} \left( \frac{1}{3} s^3 + s \right) } ds
\]

So we let $\phi(z) = \frac{1}{3} i z^3 + iz$, and seek to use the method of steepest descent. As such, we look for the saddle points of $\phi$.

$\phi'(z) = i(z^2 + 1)$, so our saddle points  are at $z + \pm i$. We will choose  $z= i$, leaving $z=-i$ as an exercise.

Now, $\phi(p+iq) = \frac{1}{3}q^3 - p^2q - q + i\left( \frac{1}{3} p^3 - pq^2 + p \right) $.
So we seek curves $\frac{1}{3} p^3 - pq^2 + p = 0$, as it takes value $0$ at $z=i$.

Hence we have two such curves:  $p=0$, and $q^2 = \frac{1}{3} p^2 + 1$. The former gives $u= \frac{1}{3} q^3 - q$, the latter gives $u = -\frac{1}{3} q\left(2 + \frac{8}{3} q^2  \right) $. Picking the curve of steepest descent through $z=i$, we pick $q = \sqrt{\frac{1}{3} p^2 + 1} $.

Now, $\phi(i+z) = -\frac{2}{3} + z^2 + \mathcal{O}\left( z^3 \right) $, so using our formula from before for contributions of a saddle point, noting that we get double the contribution than before as our integral does not start at the saddle point.

\[
	Ai(x) \sim x^{\frac{1}{2}} e^{ix^{\frac{3}{2}}\phi(i)} \sqrt{\frac{2\pi}{-x^{\frac{3}{2}}\phi''(i) }} =  \frac{1}{2\sqrt{\pi} } x^{-\frac{1}{4}} e^{-\frac{2}{3} x^{\frac{3}{2}}}
\] 

Now we do the same for $x<0$. We let $s = (-x)^{\frac{1}{2}} t$, so that $\phi(z) = i\left( \frac{1}{3} z^3 - z \right) $, and $\phi'(z) = z^2 - 1$.

Our saddle points in this case are then $z = \pm 1$.

One can use the stationary phase method to find

 \[
	 Ai(x) \sim \frac{1}{2\pi} (-x)^{\frac{1}{4}} \sqrt{\frac{2\pi}{(-x)^{\frac{3}{2}}}} \left( e^{-\frac{2i}{3} (-x)^{\frac{3}{2}}} \frac{1}{\sqrt{2} } e^{i\frac{\pi}{4}} + e^{\frac{2i}{3} (-x)^{\frac{3}{2}}} e^{i\frac{\pi}{4}} \frac{1}{\sqrt{-2} } \right) = \frac{1}{\sqrt{\pi} }(-x)^{-\frac{1}{4}} \cos\left( \frac{2}{3} (-x)^{\frac{3}{2}} - \frac{\pi}{4} \right) 
\] 

Of course, we should really check that when we truncate our curve at steepest descent, connecting it to our path of integration, the real axis, that the integral contributions vanish. But that shall be left as an exercise.


\section{Dispersive Equations and Dispersive Functions}

Let $g: \R \to \R$ be a smooth function, and $u(x,t) := g(x-ct)$ (c > 0). We can think of this as a wave propagating at a speed  $c$ to the right.

We consider the wave $e^{i(kx - \omega t)}$ where in general we allow $\omega = \omega(k)$. This propagates with speed $\frac{\omega}{k}$.

Consider the wave equation
\[
u_{t t} - c^2 u_{x x} = 0
\]
This has solution $u = e^{i(kx - \omega t)}$, with $w^2 = c^2 k ^2$, and so general solution
\[
	u = \int b(k) e^{ik(x - ct)} + a(k) e^{ik(x+ct)} dt
,\] which is non-dispersive.

\vspace{1em}

Next, we consider the 1-d Schr\"{o}dinger Equation. 
\[
i\psi_t + \psi_{x x} = 0
\]

We consider $\psi = e^{i(kx - \omega t)}$, and we see that here $\omega = k^2$, so the wave in fact travels at a speed $\frac{\omega}{k} = k$, so we have dispersion.

 \[
	 \psi(x,0) =  \int b(k) e^{ikx} dk
 ,\] where $b(k) \ge 0$. 
 \[
	 \psi(x,t) = \int b(k) e^{i(kx - k^2t)} dk
 \] 

We want to see what happens to our wavepacket as time progresses.

\vspace{1em}
Suppose we have a general wavepacket,
\[
	\psi(x,t) = \int e^{i(kx - \omega(k)t)} b(k) dk
\] 

We will investigate $\psi(x,t)$ and $\psi(ct, t)$ as $t\to \infty$.

\begin{align*}
	\psi(ct, t) = \int b(k) e^{i(kc - \omega(k))t} dk
\end{align*}

We let $\phi(k) = kc - \omega(k)$, and look for its stationary values given by  $\omega'(k) = c$. Suppose it has a solution at  $k = \overline{k}$. Assume that $\phi''(\overline{k}) \neq 0$. Then by the stationary phase method,

\begin{align*}
	\psi(ct, t) &\sim b(\overline{k}) e^{i(\overline{k}c  -\omega(\overline{k}))t} \sqrt{\frac{\pi}{t\omega''(\overline{k})}} e^{i\frac{\pi}{4} \sgn \omega''(\overline{k})} \\
	&\sim  \mathcal{O} \left( t^{-\frac{1}{2}} \right) 
\end{align*}

While $\int |\psi(x,t)|^2 = \int |b(k)|^2 dk = \text{ const}$, the wave disperses, and its amplitude decreases.

We call $\frac{\omega(k)}{k}$ the wave speed, and $\omega'(k)$ the group velocity.

In the case of Schr\"{o}dinger's equation, we get $\overline{k} = \frac{c}{2}$.

\begin{eg} Shallow Water Equation
	
	We model the motion of shallow water by
	\[
		u_t + u_{x x x} + u u_x = 0  
	\]

	We consider the linearised initial value problem,
	\begin{align*}
		u_t + u_{x x x } &= 0 \\
		u(x,0) = f(x)
	\end{align*}

	Taking Fourier transforms,
	 \[
		 \hat{u}_t + (ik^3) \hat{u} = 0
	\]

	And so
	\[
		\hat{u}(k, t) = \hat{u}(k, 0) e^{-ik^3t} = e^{-ik^3 t} \hat{f}(k)
	\]

	Giving that
	\begin{align*}
		u(x,t) &= \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{ikx} e^{-ik^3t} \hat{f}(k) dk \\
		&= \frac{1}{2\pi} e^{i(kx - k^3 t)} \hat{f}(k) dk\\
	\end{align*}
	And so we have a dispersive wave with $\omega = k^3$, and so speed  $k^2$.
	
	If we assume that $\hat{f}(k)$ is real, then $\hat{f}(-k) = \hat{f}(k)^{\ast}$.

	Now, our constant speed solution is given by
	\[
		u(ct, t) = \frac{1}{\pi} \int_{-\infty}^{\infty} e^{i(kc - k^3)t} \hat{f}(k) dk
	\]

	So, we define $\Phi(z) = i(cz - z^3)$, which is stationary at $z^2 = \frac{c}{3}$, which are saddle points.

	We define $K_{L / R} = \pm \sqrt{\frac{c}{3}} $. These are saddle points, but as for $c>0$  $\Phi$ is purely imaginary, we can just use the stationary method.

	We let $\phi(k) = -i\Phi(k) = ck - k^3$. So $\phi''(k) = -6k$, and it is stationary at  $K_L$ and $K_R$, with second derivatives  $-6\sqrt{\frac{c}{3}} \neq 0 $.

	For our approach to be valid, we assume that $K_ L$ and $K_R$ are within the support of $\hat{f}(k)$. 


	So,
	\begin{align*}
		u(x,t) &\sim \frac{1}{2\pi} \int_{K_L - \epsilon}^{K_L + \epsilon} \hat{f}(k) e^{it\phi(t)} dk + \frac{1}{2\pi} \int_{K_R - \epsilon}^{K_R + epsilon} \hat{f}(k) e^{it\phi(t)} dk + \mathcal{O}\left(\frac{1}{t}\right) \\
		&:= I_L + I_R + \mathcal{O}\left( \frac{1}{t} \right) \\
		I_L &\sim \frac{1}{2\pi} \hat{f}(K_L) \sqrt{\frac{2\pi}{it \phi''(K_L)}} e^{it\phi(K_L)} \\
		&\sim e^{-i\frac{\pi}{4}}\sqrt{\frac{1}{12 \pi t}} \hat{f}\left(-\sqrt{\frac{c}{3}}\right) \left(\frac{3}{c}\right)^{\frac{1}{4}} e^{-2i \left( \frac{c}{3} \right)^{\frac{3}{2}} t} + \mathcal{O}\left( \frac{1}{t} \right) \\
		I_R &\sim \frac{1}{2\pi} \hat{f}(K_R) \sqrt{\frac{2\pi}{it \phi''(K_R)}} e^{it\phi(K_R)} \\
		&\sim e^{i\frac{\pi}{4}} \sqrt{\frac{1}{12\pi t}} \hat{f}\left(\sqrt{\frac{c}{3}}\right) \left(\frac{3}{c}\right)^{\frac{1}{4}} e^{2i \left( \frac{c}{3} \right)^{\frac{3}{2}}t } + \mathcal{O}\left( \frac{1}{t} \right) 
	\end{align*}
	Now, writing $\hat{f}(K_R) = |\hat{f}(K_R)|e^{i\Theta (K_R)}$, we see that

	\[
		u(ct, t) \sim \frac{|\hat{f}(K_R)|}{\sqrt{3  \pi t}} \left( \frac{c}{3} \right)^{\frac{1}{4}} \cos\left( 2t\left( \frac{c}{3} \right)^{\frac{3}{2}}  - \frac{\pi}{4} + \Theta(K_R) \right)  
	\] 

	Now, if $c < 0$, we have that  $\Phi'(z) = 0$ at $z = \pm i \sqrt{\left|\frac{c}{3} \right|} $. As with the Airy function, working with the saddle point at $i \sqrt{\left|\frac{c}{3} \right|}$, we get exponential decay.
\end{eg}

\section{Asymptotic Behaviour of Solutions to Differential Equations}

We consider linear equations of the form
\[
	y'' + p(x)y' + q(x)y = 0, \qquad \text{ about } x = x_0 \text{ or } x = +=\infty
\] 
and
\[
	\epsilon y'' + p(x) y' + q(x) y = 0, \qquad \epsilon \ll 1 \text{ as } \epsilon \to 0
\] 

\subsubsection*{Type 1}

Consider 
\[
	y'' + p(x) y' + q(x) y = 0
\] about $x=x_0$.

If $x_0$ is a regular point, we can use the power series method as in $IA$. If $x_0$ is a regular-singular point, such that $(x-x_0)p(x)$ and $(x-x_0)^2 q(x)$ are analytic at $x_0$, we can use the Frobenius method as in $IA$, considering power series solutions of the form $y = (x-x_0)^{\alpha} \sum_{k=0}^{\infty} a_k (x-x_0)^{k}$, or with a logarithmic singularity instead of polynomial ($\alpha$).

If we have an irregular singular point, then we expect an essential singularity at  $x_0$. As such, we consider solutions of the type $y = e^{S(x)}$, where $S(x)$ isn't analytic at $x_0$ (motivated by $e^{\frac{1}{x}}$ ). This is the Liouville-Green method.

Then 
 \[
	 e^{S(x)} \left[ s'' + (s')^2 + p(x) s' + q(x) \right] = 0
\] 

This gives a non-linear equation to solve:
\[
	s'' + (s')^2 + p(x) s' + q(x) = 0
\]

What about $x = \pm \infty$? We consider $X= \frac{1}{x}$, and define $Y(X) = y(\frac{1}{X})$. Then $Y' = -\frac{1}{X^2} y'(\frac{1}{X})$, and $Y'' = \frac{2}{X^3} y'(\frac{1}{X}) + \frac{1}{X^{4}} y''(\frac{1}{X})$ 

So we now consider
\[
	Y''(X) + P(X) Y'(X) + Q(X)Y(X) = 0
\] as $X \to 0$, where

\begin{align*}
	P(X) &= \frac{2}{X}-\frac{1}{X^2} p(\frac{1}{X}) \\
	Q(X) &= \frac{1}{X^{4}} q(\frac{1}{X})
\end{align*}

\begin{prop}
	\begin{align*}
		& y'' + p(x) y' + q(x) y = 0 \\
		\iff & u''  + Q(x)u = 0
	\end{align*}
\end{prop}

\begin{proof}
	Let $y = uv$ for some  $u,v$.

	Then  \begin{align*}
		y'& = u'v + uv' \\
		y'' &= u''v + 2u'v' + uv''
	\end{align*}

	So 
	\[
		u''v + u' (2v' + pv) + u(qv + v'' + pv') = 0
	\]

	We are free to choose
	 \[
		 v = e^{\frac{1}{2}\int ^{x} p(t) dt} \neq 0
	\] 

	Then we can happily write
	\[
		u'' + u\left( \frac{qv + v'' + pv'}{v} \right) = 0
	\] 
\end{proof}

So, we can now wlog work with equations 
\[
	y'' + Q(x) y =0
,\] assuming that $x=x_0 < \infty$ is an irregular singular point.

So, using our ansatz solution $y = e^{S(x)}$,
\[
	S'' + (S')^2 + Q(x) = 0
\] 

We might expect a solution along the lines of $S = \frac{C}{(x-x_0)^{\alpha}}$ for $\alpha > 0$.
Then  $S' = \frac{-c \alpha}{(x-x_0)^{\alpha+ 1}}$, $S'' = \frac{c\alpha (\alpha + 1)}{(x-x_0)^{\alpha+2}}$. So $(S')^2$ is our dominant term asymptotically, and we initially consider only this term.

We then might expect to find an asymptotic series $S \sim  S_0 + S_1 + \ldots$, as we keep considering our next most dominant terms. As our series is for a irregular point, we will stop when we find $S_n = o(1)$.

With this in mind, we look for our first approximation.

Assume that $S_0 '' \ll (S_0)^{2}$. (To be checked in retrospect).

Then
\[
	S_0' = \pm \sqrt{-Q(x)} 
\] We let $\sigma = \pm 1$ for ease of notation.

So  \[
S_0 = \sigma \int \sqrt{-Q} dx 
\] 

We get that 
\begin{align*}
	(S_0')^2  &= -Q(x) \\
	S_0'' = \frac{-\sigma Q'(x)}{2\sqrt{-Q(x)} }
\end{align*} 
We must check that $S_0'' \ll (S_0')^2.$ Assuming it is, we carry on.

Consider $y \sim  e^{S_0 + S_1}$.

\begin{align*}
	(S_0 + S_1)'' + (S_0' + S_1')^2 + Q(x) &= 0 \\
	S_0'' + S_1'' + (S_0')^2 + (S_1')^2 + 2S_0'S_1' + Q(x) &= 0 \\
	\implies S_1'' + (S_1')^2 + 2S_0S_1 = \frac{\sigma}{2} \frac{Q'(x)}{\sqrt{-Q(x)} }
\end{align*}

We are already working with the idea that $S_1 \ll S_0$. We further assume that $S_1' \ll S_0'$, and $S_1'' \ll S_1'S_0'$. Again, this must be checked upon calculation.

So to leading order, our dominant balance is
\begin{align*}
	2 S_0' S_1' &= \frac{\sigma}{2} \frac{Q'}{\sqrt{-Q} } \\
	S_1 ' &= \frac{1}{4} \frac{Q'}{-Q} \\#
	\impleis S_1 &= -\frac{1}{4} \ln Q \\
\end{align*}

So our revised approximation is
\[
	y \sim  \frac{e^{\sigma \int\sqrt{-Q} dx}}{(Q)^{\frac{1}{4}}}
\] 

Then our general solution is (to this order),
\[
	y \sim  A \frac{e^{\int \sqrt{-Q} }}{(Q)^{\frac{1}{4}}} + B \frac{e^{-\int \sqrt{-Q} }}{(Q)^{\frac{1}{4}}}
\] 

\begin{eg}
	\[
	x^3u'' = u
	,\] as $x \to  0^{+}$.

	First we must classify the ODE at $x=0$. Here, it is easy to see that is an irregular singular point, as  $x^2 \times  \frac{1}{x^3} $ is not analytic at $0$.

	So, we proceed with the ansatz 
	 \[
	y = e^{S}
	,\] with $S \sim S_0 + S_1 + \ldots$

	We have
	\[
		S_0'' + (S_0')^2 = \frac{1}{x^3}
	\] 
	Assuming that $S_0'' \ll (S_0')^2 $, then $S_0 ' = \frac{\sigma}{x^{\frac{3}{2}}}$, $S_0 = -2 \frac{\sigma}{x^{\frac{1}{2}}}$.

	So, $S_0'' = -\frac{3}{2} \frac{\sigma}{x^{\frac{5}{2}}} \ll \frac{1}{x^3}$ as $x\to 0$, so our assumption is valid, and we can proceed.

	We now consider our second term $S_1 \ll S_0$.

	\[
		S_0'' + S_1'' + (S_0')^2 + (S_1')^2 + 2S_0'S_1' = \frac{1}{x^3}
	\] 
	Assuming $S_0'S_1'$ is our leading order term, we get that
	\[
	S_1' = \frac{3}{4} \frac{1}{x}
	\]

	Note that if our original forcing term was e.g.  $\frac{1}{x^3} + \frac{1}{x^2}$, we would neglect the $\frac{1}{x^2}$ term in our $S_0$ expansion, as it is dominated at $0$. We would then reintroduce it for the second iteration here.

	Continuing,
	\[
	S_1 = \frac{3}{4} \ln x
	\]
	Then $S_1 \ll S_0$, $S_1' \ll S_0'$, $S_1'' \ll S_1'S_0'$, so our assumptions are consistent.
	
	Continuing, we let $w = e^{S_0 + S_1 + S_2}$. So
	\begin{align*}
		(S_0 + S_1 + S_2)'' + (S_0' + S_1' + S_2')^2 &= \frac{1}{x^3} \\
		(S_0'' + 2S_0'S_1') + ((S_0')^2 - \frac{1}{x^3}) + S_1 '' + S_2'' + (S_1')^2 + (S_2')^2 + 2S_2'(S_0' + S_1') &= 0 \\
		\implies -\frac{3}{4}\frac{1}{x^2} + S_2'' + \left(\frac{3}{4}\frac{1}{x}\right)^2 + (S_2')^2 + 2S_2'(S_0' + S_1') &= 0\\
	\end{align*}
	So if we assume that $S_2'' \ll S_0' S_1'$, and $S_2' \ll S_1.$. We already know that $S_1' \ll S_0'$.

	Then 
	\begin{align*}
		2S_0'S_2' &= \frac{3}{16}\frac{1}{x^2} \\
		\implies S_2' &= \frac{3\sigma}{16} \frac{1}{\sqrt{x} } \\
		S_2 &= \frac{3\sigma}{8} \sqrt{x} = o(1) \text{ as } x\to 0  \\
	\end{align*}
	You can check that this does not contradict our assumptions, so we stop here.

	Hence
	\[
		y \sim e^{S_0 + S_1} = e^{2\sigma x^{-\frac{1}{2}} + \frac{3}{4} \ln x}
	\] 
	So we have general solution
	\[
		y \sim A e^{-2x^{-\frac{1}{2}}} x^{\frac{3}{4}} + B e^{2x^{-\frac{1}{2}}} x^{\frac{3}{4}
		\]
	We see that one of our solutions decays at 0, the other diverges.
\end{eg}

\begin{eg} (Airy Equation)
	
	\[
	y'' = xy, \quad x \to \infty
	\] We let $X =\frac{1}{x}$. Then
	\[
	Y'' + \frac{2}{X}Y' - \frac{1}{X^{5}}Y = 0
	\] 
	We can see that $X=0$ is an irregular singular point. As such, we use the Liouville-Green method.

	Consider $y \sim e^{S_0}$.
	Then \[
		S_0'' + (S_0')^2 = x
	\]
	Assume that $S_0'' \ll (S_0')^2 $ as $x->\infty$.

	Then 
	 \[
		 S_0' = \sigma \sqrt{x}, \quad S_0 = \frac{2}{3} \sigma x^{\frac{3}{2}} 
	\]
	This gives $S_0'' = \frac{\sigma}{2\sqrt{x} } \ll (S_0')^2$ as $x \to \infty$, so are assumptions hold.

	So consider  $y \sim  e^{S_0 + S_1}$.

	Then \begin{align*}
		S_0'' + S_1'' + (S_0')^2 + (S_1')^2 + 2S_0'S_1' &= x \\
		\implies S_1 '' + (S_1')^2 + 2S_0'S_1' = -\frac{\sigma}{2\sqrt{x}}
	\end{align*}

	We assume that $S_1'' \ll S_0'S_1'$.

	Then our dominant balance is
	\begin{align*}
	2S_0'S_1' = -\frac{\sigma}{2\sqrt{x} }
	\implies S_1' = -\frac{1}{4}x^{-1}, \quad S_1 = -\frac{1}{4} \ln x
\end{align*} 
Indeed $S_1'' \ll S_0'S_1'$, and so 
\[
y_1 \sim e^{\frac{2}{3} x^{\frac{3}{2}}} x^{-\frac{1}{4}}
\]
and
\[
y_2 \sim  e^{-\frac{2}{3} x^{\frac{3}{2}}} x^{-\frac{1}{4}}
\] 
\end{eg}

\subsection{Regular and Singular Pertubations}

We are interested in a family of problems
\[
	\Phi(x; \epsilon) = 0
,\] where $\epsilon$ is small. We are interested in the behaviour of solutions $x_{\epsilon}$ as $\epsilon \to 0$. 
Supposing that $\Phi(\overline{x}, 0) = 0$, we would certainly like if $X_{\epsilon} \to \overline{x}$, so that our problem is continuous in a sense. But what if $x_{epsilon}$ doesn't converge, or doesn't converge at all.

As such, we might ask if we can approximate by a linear perturbation
\[
	\Phi(x; \epsilon) = F(x) + \epsilon G(x)
,\] and ask if we can approximate by solutions of $F(x) = 0$. If we can, we call this a regular pertubation of $F(x) = 0$. If this is not the case, we call it a singular pertubation. 

\begin{eg}
	\[
	x^2 - 2\epsilon x -1 =0
	\]
	In this case, \[
	x_{\epsilon} = \frac{2\epsilon \pm \sqrt{4\epsilon^2 + 4} }{2} = \epsilon \pm \sqrt{1 + \epsilon^2} 
	\] 
	And so  $x_{\epsilon} \to \pm 1$. Our unperturbed equation is
	\[
	x^2 = 1
	,\] so our pertubation is regular.
\end{eg}

\begin{eg}
	\[
	\epsilon x^2 -2x - 1 = 0
	\] 
	At $\epsilon = 0$, $x = -\frac{1}{2}$.

	Our perturbed equation has solution
	\[
		x_{\epsilon} = \frac{1 \pm \sqrt{1+\epsilon} }{\epsilon} = \frac{1}{\epsilon} \left[ 1 \pm \left( 1 + \frac{\epsilon}{2} + \ldots \right)  \right] 
	\] 

	So
	\begin{align*}
		x_{+}(\epsilon) &= \frac{2}{\epsilon} + \mathcal{O}(1) \\
		x_{-}(\epsilon) &= -\frac{1}{2} + o(1)
	\end{align*}

	And so our pertubation is singular.
\end{eg}

\begin{eg}
	\[
	y'' + 2\epsilony' + y = 0
	\]
	Our characteristic equation for ansatz $y = e^{\lambda x}$ is given by
	\[
	\lambda^2 + 2\epsilon\lambda + 1= 0
	\]
	So
	\begin{align*}
		\lambda = -\epsilon \pm \sqrt{\epsilon^2 -1} \to += i \text{ as } \epsilon \to 0 
	\end{align*}
	So
	\begin{align*}
		y_1(x) &= e^{-\epsilon x} \cos\left( \sqrt{1-\epsilon^2}  \right) \to \cos x \\
		y_2(x) &= e^{-\epsilon x} \sin\left( \sqrt{1 - \epsilon}  \right) \to \sin x
	\end{align*}

	So we converge to our solutions to $y'' + y =0$
\end{eg}

\begin{ex}
	\[
	y'' + \epsilon y =0
	\] 
	This has solutions $y =  \cos \sqrt{\epsilon}x$ and $y = \sin \sqrt{\epsilon}x $. How do we recover the limiting solution $y = x$ from this methodology. 
\end{ex}

\begin{eg}
	\[
	\epsilon y'' + 2y' + y = 0
	,\] with ansatz $e^{\lambda x}$.

	Then
	\[
	\epsilon \lambda^2 + 2\lambda + 1 =0
	\]
	So
	\[
		\lambda_{\pm}(\epsilon) = \frac{-1 \pm \sqrt{1-\epsilon} }{\epsilon}
	\]
	And then
	\begin{align*}
		\lambda_{+}(\epsilon) &= -\frac{1}{2} + o(1) \\
		\lambda_{-}(\epsilon) &= -\frac{2}{\epsilon} + \mathcal{O}(1)
	\end{align*}
	So \[
	e^{\lambda_{+} x} \sim  e^{-\frac{1}{2}x}, \quad e^{\lambda_{-} x} \sim e^{-\frac{2}{\epsilon} x}
	,\] and so our second solution diverges.
\end{eg}

\subsection{WKBJ Method}

Consider equations of the form
\[
	\epsilon^2 y'' = q(x) y
\] 

Motivated by our previous example, we consider solutions of the form
\[
y = e^{\frac{S}{\delta}}
,\] where $\delta(\epsilon)$ is to be determined.

So,
\begin{align*}
	y' = \frac{S'}{\delta}e^{\frac{S}{\delta}}, \quad y'' = \left( \frac{S'^2}{\delta^2} + \frac{s''}{\delta} \right) e^{\frac{S}{\delta}}
\end{align*}
and so
\[
	\epsilon^2\left( \frac{S'^2}{\delta^2} + \frac{s''}{\delta} \right) = q(x)
\]

Here we can choose either $\epsilon^2 = \delta$, or $\epsilon^2 = \delta ^2$. We assume that $q \neq 0$.

In the first case, our dominant balance gives \[
\frac{S' ^2}{\delta} = 0 \quad \implies $S = \text{ const }$
,\] which we don't care about.

So,
\[
	S'^2 + \epsilon S'' = q(x)
\] 
Our dominant balance gives
\begin{align*}
	S'^2 &= q \\
	S' &= \sigma \sqrt{q} \\
	\implies y_{\pm} &= A_{\pm} \exp\left( \frac{1}{\delta} \int ^{x} \pm \sqrt{q} \right) 
\end{align*}

To next order, we take $S = S_0 + \delta S_1 = S_0 + \epsilon S_1$.

So
\begin{align*}
	\epsilon^2 \left( \frac{(S_0' + \epsilon S_1')^2}{\epsilon^2} + \frac{(S_0'' + \epsilon S_1'')}{\epsilon} \right) &= q(x) \\
	\implies \epsilon^2 S_1'^2 + 2\epsilon S_0' S_1' + \epsilon \frac{\sigma}{2} \frac{q'}{\sqrt{q} } + \epsilon^2 s_1'' = 0
\end{align*}

So to dominant balance,
\[
2S_0' S_1' = -\frac{\sigma}{2}\frac{q'}{\sqrt{q}  }
\] 
Hence $S_1 = -\frac{1}{4} \log q$

So \[
	y_{\pm} A_{\pm} q^{-\frac{1}{4}} \exp\left( \frac{\pm}{\epsilon} \int ^{x} \sqrt{q}  \right) 
\] 

In general, to higher order we get dominant balance of terms order  $\epsilon^{n}$
\[
2S_0' S_n ' + \sum_{k=1}^{n-1} S_k' S_{n-k}' + S_{n-1}'' = 0
,\] which can be solved iteratively.

Now begins the struggle to deal with cases where $q$ can be $0$.

\subsubsection{One Turning Point}

Suppose $q(a) = 0$, $q'(a) \neq  0$, and so wlog $q'(a) < 0$. We must consider the behaviour of our method locally around  $x=a$, as our dominant balance from earlier is then invalid.

So, we approximate 
\[
	q(x) = \mu (x-a), \quad \mu > 0
\] for $|x-a| < \eta \approx \epsilon^{\frac{1}{3}}$

Consider
\[
	y'' = \frac{\mu}{\epsilon^2} (x-a) y
\]

Let $z = \left( \frac{\mu}{\epsilon^2} \right)^{\frac{1}{3}} (x-a) $. Then $\frac{d^2y}{dz^2} = zy$.

But this just is the Airy equation, so we consider a decaying solution to it for $z \gg 0$, as we want it to decay for $x-a \gg \epsilon^{\frac{2}{3}}$, becoming $y_{-}$ from before.

As such, we aim to match up the asymptotics of the decaying Airy function with the asymptotics of $y_{-}$, and this becomes our solution.

We work similarly for $x - a \ll -\epsilon^{\frac{2}{3}}$, using our $y_{+}$ solution.

\vspace{1em}

For $x > a$,  $q(x) > 0$. 
 \[
	 y_{>} = A q^{-\frac{1}{4}} e^{-\frac{1}{\epsilon} \int_{a}^{x} \sqrt{q(t)} dt}
\]
And for $\epsilon^{\frac{2}{3}} \le  t-a \le \epsilon^{\frac{1}{3}}$, where $q(t) \approx \mu(t-a)$, we have
 \[
	 y_{>} \sim \frac{A}{\left( \mu (x-a) \right)^{\frac{1}{4}} } \exp\left( -\frac{\mu}{\epsilon} \frac{2}{3} (x-a)^{\frac{3}{2}} \right) 
\] 

Our decaying Airy function, with $Z \gg 1$, has asymptotics
\[
y_0 \sim \frac{1}{2\sqrt{\pi} } z^{-\frac{1}{4}} e^{-\frac{2}{3} z^{\frac{3}{2}}}
\] 
or
\[
y_0(x) \sim \frac{1}{2\sqrt{\pi} } \frac{1}{\left( \left\frac{\mu}{\epsilon^2}\right)^{\frac{1}{3}} (a-x) \right)^{\frac{1}{4}} } \exp\left( \frac{-2}{3} \frac{\sqrt{\mu} }{\epsilon} (x-a)^{\frac{3}{2}} \right) 
\] 

For $x < a$, we have real part solution
\[
	y_{<} \approx C (-q)^{-\frac{1}{4}} \cos\left( \frac{1}{\epsilon} \int_{x}^{a} \sqrt{-q(t)}  dt - \gamma \right)  
\] 

If we are $\epsilon^{\frac{2}{3}} \le a-t \le \epsilon^{\frac{1}{3}}$, we have $q(t) \approx \mu (t-a)$, and so
\[
	y_{<} \sim \frac{C}{\left( \mu (a-x) \right)^{\frac{1}{4}} } \cos \left( \frac{\mu}{2} \frac{2}{3} (a-x) ^{\frac{3}{2}} - \gamma \right) 
\] 

Our Airy asymptotics can be shown to be
\[
	y_0 \sim  \frac{1}{\sqrt{\pi} } \frac{1}{\left( (\mu \epsilon^{-2})^{\frac{1}{3}} (a-x) \right)^{\frac{1}{4}} } \cos\left( \frac{2}{3} \frac{\mu}{\epsilon} (a-x)^{\frac{3}{2}}  - \frac{\pi}{4}\right) \] 
So we must have $\gamma = \frac{\pi}{4}$.

We can see that both matching conditions work if we choose our constants appropriately:

\begin{align*}
	y = \begin{cases}
		K (q(x))^{-\frac{1}{4}} \exp\left( -\frac{1}{\epsilon} \int_{a}^{x} \sqrt{q(t)} dt \right) \qquad x - a  \gg \epsilon ^{\frac{2}{3}} \\
		2\sqrt{\pi} K (\mu \epsilon)^{-\frac{1}{6}} Ai\left( \epsilon^{\frac{2}{3}} \mu^{\frac{1}{2}} (a-x) \right) \qquad |x-a| \ll 1 \\
		\frac{2}{3} K (-q)^{\frac{-1}{4}} \cos\left( \frac{1}{\epsilon} \int_{x}^{a} \sqrt{-q(t)} dt - \frac{\pi}{4} \right) \qquad a-x \gg \epsilon^{\frac{2}{3}}
	\end{cases}
\end{align*}

Hence for linear approximation of $q$, we can simply use the Airy function to patch up our solution.

\vspace{1em}

Next we consider the case that $q$ has two turning points at $b, a$. We already know our behaviour far away from  $b, a$ (and so perhaps even in between), so we hope to match the $\cos$ solution from each root in the middle, i.e. to have
\[
	\frac{1}{\epsilon} \int_{x}^{a} \sqrt{-q(t)} dt - \frac{\pi}{4} = \frac{1}{\epsilon} \int_{b}^{x} \sqrt{q(t)}  dt - \frac{\pi}{4} \quad a < x < b
\]

As such, we focus on the interval $[b,a]$, as our solution is fine everyone else. In fact, let  $J = [b+\epsilon^{\frac{1}{3}}, a-\epsilon^{\frac{1}{3}}]$.

Then
\begin{align*}
	y &\sim \frac{2R}{|q|^{\frac{1}{4}}} \cos\left( \frac{1}{\epsilon} \int_x^{a} |q|^{\frac{1}{2}}dt - \frac{\pi}{4} \right) \qquad x < a - \epsilon^{\frac{1}{3}} \\
	y &\sim \frac{2L}{|q|^{\frac{1}{4}}} \cos\left( \frac{1}{\epsion}\int_b^{x} |q|^{\frac{1}{2}} - \frac{\pi}{4} \right) 
\end{align*}

So we seek (by parity of cosine)
\[
	\frac{1}{\epsilon} \int_{x}^{a}|q|^{\frac{1}{2}} dt - \frac{\pi}{4} = - \left(\int_{b}^{x} |q|^{\frac{1}{2}} dt i \frac{\pi}{4}\right) + n\pi \qquad x \in (b,a)
\] 
As such, if we can find an integer $n$ such that
\[
\frac{1}{\epsilon} \int_b^{a} |q|^{\frac{1}{2}} = \frac{\pi}{2} + n\pi
,\] then taking $L = (-1)^{n}R$ gives a solution.

This solution gives oscillations in $J$, peaking around $b$ and $a$ where $q = 0$ and our exponential solution takes hold.

\begin{eg} The Stationary Schr\"{o}dinger Equation
	
\[
	- \frac{\hbar}{2m} \psi_{x x } + V(x)\psi(x) = E\psi(x)
\] 

We wish to find $\psi$ bounded, decaying at $\pm \infty$, and the corresponding eigenvalues $E$. We consider $\hbar = \epsilon$.

Then
\[
	\psi_{x x} = \frac{2m}{\epsilon^2} (V-E)\psi
\] 

We let $a, b$ be such that $V(a) = V(b) = E$. We know that  $a, b$ change continuously with $E$, so this is acceptable. In nice examples such as a harmonic oscillator, we precisely have the scenario of the two turning point WKBJ method from before, with  $q(x) = 2m(V(x) - E)$

Hence we seek to find  $E$ such that
\[
	\frac{\sqrt{2m} }{\epsilon} \int_{b(E)}^{a(E)} \sqrt{E -V(x)} dx = \frac{\pi}{2}  + n\pi
\] 

By continuity, we find $E_0, E_1, \ldots, E_n$ with
\[
	\sqrt{2m}\int_{b(E_i)}^{a(E_i)} \sqrt{E_i - V(x)} dx = (i +\frac{1}{2})\pi \hbar
,\] with $\left(b(E_n), a(E_n)\right)$ our classically allowed region.
This is known as the Bohr-Sommerfeld quantisation. 
\end{eg}

\begin{eg}
	Our classical energy is given by
	 \[
	E = \frac{p^2}{2m} + v
	\] 
	So $|p| = |2m(E-V)|^{\frac{1}{2}}$. So
	\[
		\oint |p| dx = 2 \int_{b}^{a} |2m(E-V)|^{\frac{1}{2}} dx = 2\pi \hbar (n + \frac{1}{2})
	\] 

	Consider a local wavelength $\lambda$ of the wavefunction $\psi$. 

	\[
	2\pi = \frac{1}{\epsilon}\int_{x}^{x+\lambda} |q|^{\frac{1}{2}} dt \approx \frac{1}{\epsilon} \lambda |q|^{\frac{1}{2}} = \frac{\lambda |p|}{\hbar}
	\] 

	This gives us the de Broglie wavelength
	\[
	\lambda = \frac{2\pi \hbar}{p}
	\] 
\end{eg}

\subsection{Stokes Phenomenon}

Suppose $f(z) \sim g(z)$, say as $|z| \to  \infty$. We don't expect this asymptote to be valid for all  $z \in \C$, but only in some sectors of $\C$.

For example, $f(z) = e^{-z}$ only goes to 0 for $-\frac{\pi}{2} < \arg z > \frac{\pi}{2}$. We seek to understand more about when such transitions in asymptotic behaviour occur.

\vspace{1em}

We revisit the Liouville-Green method for second order differential equations, considering

\[
	y \sim  A e^{S_1(z)} + Be^{S_2(z)}
\]

Suppose $\Re S_2(z) \ll \Re S_1(z)$ on some sub-domain $D \subset \C$. Then in this region, we have  dominant and a recessive term.

On curves with  $\Re \left( S_1(z) - S_2(z) \right) =0$, we do not get such behaviour. We call such curves (in this context) Stokes curves. We might expect that crossing the curves causes a switch in which asymptote is dominant. Similarly, for $S_i$ with branch cuts, we would likely get a change in behaviour.

We call curves $\Im \left( S_1(z) - S_2(z) \right) =0$ anti-Stokes curves.

\begin{eg} Airy Function and Airy Function

	\[
	y'' = xy
	\] about $x=0$ gives a power series solution

	\[
		y(x) = C_1 \sum_{n=0}^{\infty} \frac{x^{3n}}{9^{n}n! \Gamma(n + \frac{2}{3}} + C_2\sum_{n=0}^{\infty} \frac{x^{3n+1}}{9^{n} n! \Gamma(n+\frac{4}{3})}
	\] which has radius of convergence $R = \infty$, and so we get an entire function. No Stokes curves. 

	Taking $C_1 = 3^{-\frac{2}{3}}$, $C_2 = -3^{-\frac{4}{3}}$, we get Airy's function of the first kind $Ai(x)$. For the Airy function of the second kind (Bairy function), we take $C_1 = 3^{-\frac{1}{6}}$, $C_2 = 3^{-\frac{1}{3}}$. These are linearly independent. 

	Asymptotically, as $x \to \pm \infty$,

	\begin{align*}
		Ai(x) &\sim  \frac{1}{2\sqrt{\pi} } x^{-\frac{1}{4}} e^{-\frac{2}{3}x^{\frac{3}{2}}} \text{ as } x\to \infty \\
		Ai(x) &\sim  \frac{1}{\sqrt{\pi} } |x^{-\frac{1}{4}} \cos\left( \frac{2}{3} |x|^{\frac{3}{2}} - \frac{\pi}{4} \right) \text{ as } x \to -\infty
	\end{align*} and

	\begin{align*}
		Bi(x) &\sim \frac{1}{\sqrt{\pi} } x^{-\frac{1}{4}} e^{\frac{2}{3}x^{\frac{3}{2}}} \text{ as } x \to \infty \\
		Bi(x) &\sim \frac{1}{\sqrt{\pi} } \sin\left( \frac{2}{3} |x|^{\frac{3}{2}} - \frac{\pi}{4} \right) \text{ as } x \to  -\infty
	\end{align*}

	We now consider the asymptotics of the Airy equation in the complex plane. So as from the Liouville-Green method,

	\[
		y(z) \sim  Az^{-\frac{1}{4}} e^{-\frac{2}{3}z^{\frac{3}{2}}} + B z^{-\frac{1}{4}}e^{\frac{2}{3} z^{\frac{3}{2}}}
	\] 

	Our goal is to find an expression in all of $\C$ for the asymptotics of the Airy function. As such, we compare $e^{\frac{2}{3}z^{\frac{3}{2}}}$ and $e^{-\frac{2}{3}z^{\frac{3}{2}}}$ to find our Stokes curves.

	\begin{align*}
		\Re \left( \frac{4}{3}z^{\frac{3}{2}} \right) &= 0 \\
		\implies \frac{3}{2} \arg z &= \frac{\pi}{2} + k\pi, \quad k \in \Z \\
		\arg z &\in \{\frac{\pi}{3}, \pi, \frac{5\pi}{3}\} 
	\end{align*}

	Hence for  $|\arg z| < \frac{\pi}{3}$, where $z^{\frac{3}{2}} > 0$, then
	\[
		Ai(z) \sim \frac{1}{2\sqrt{\pi} } z^{-\frac{1}{4}} e^{-\frac{2}{3}z^{\frac{3}{2}}}
	\] 

	This is in fact a good approximation for $|\arg z| < \pi$.

	We need to find a better approximation for $|\arg z| > \frac{\pi}{3}$, in particular in the LHP.

\begin{prop}
	Let $\omega = e^{-\frac{2\pi i }{3}}$, a cubic root of unity.

	Let $y(z)$ be a solution of Airy's equation. Then $y(\omega z), y(\omega^2 z)$ are also solutions.
\end{prop}
\begin{proof}
	Put it in Airy's equation.
\end{proof}

So $Ai(\omega z), Ai(\omega^2 z), Bi(\omega z), Bi(\omega^2 z)$ are also solutions. But of course only two are linearly independent, as this is a second order ODE.

\begin{prop}
	\begin{align*}
		Ai(z) &\equiv -\omega Ai(\omega z) - \omega^2 Ai(\omega ^2 z) \\
		Bi(z) &\equiv i\omega Ai(\omega z) - i\omega^2 Ai(\omega^2 z)
	\end{align*}
\end{prop}

\begin{proof}
	Take the Taylor expansion of all terms. As the expansions are entire, and zeroes of an analytic function are isolated, the functions must in fact be equal everywhere.
\end{proof}

\begin{prop}
	\[
		Ai(z) \sim A z^{-\frac{1}{4}} e^{-\frac{2}{3} z^{\frac{3}{2}}} + B z^{-\frac{1}{4}} e^{\frac{2}{3}z^{\frac{3}{2}}}
	\] 

	\begin{enumerate}
		\item $A = \frac{1}{2\sqrt{\pi} }, B = 0$ for $|\arg z| < \pi$ \\
		\item $A = \frac{1}{2\sqrt{\pi} }, B = \frac{i}{2\sqrt{\pi} }$ for $\frac{\pi}{3} < \arg z < \frac{5\pi}{3}$
	\end{enumerate}
\end{prop}

\begin{proof}
	Let $\frac{\pi}{3} < \arg z < \frac{5\pi}{3}$. Then $-\frac{\pi}{3 } < \omega z < \pi$ and $-\pi < \arg z < \frac{\pi}{3}$ 

	Then we know that 
	\begin{align*}
		Ai(\omega z) &\sim \frac{1}{2\sqrt{\pi} } (\omega z)^{-\frac{1}{4}} e^{-\frac{2}{3} (\omega z)^{\frac{3}{2}}} \\
		Ai(\omega^2 z) &\sim \frac{1}{2\sqrt{\pi} } (\omega ^2 z)^{-\frac{1}{4}} e^{-\frac{2}{3}} (\omega^2 z)^{\frac{3}{2}}
	\end{align*}

	Putting this in our identity for $Ai(z)$, we get the claimed result.
\end{proof}
\end{eg}


\end{document}
